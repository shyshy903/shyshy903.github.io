<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Linux</title>
      <link href="/2021/02/01/Linux/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E7%90%86%E8%A7%A3epoll/"/>
      <url>/2021/02/01/Linux/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E7%90%86%E8%A7%A3epoll/</url>
      
        <content type="html"><![CDATA[<h1 id="手动实现与理解epoll"><a href="#手动实现与理解epoll" class="headerlink" title="手动实现与理解epoll"></a>手动实现与理解epoll</h1><h2 id="进程阻塞为什么不占用CPU资源？"><a href="#进程阻塞为什么不占用CPU资源？" class="headerlink" title="进程阻塞为什么不占用CPU资源？"></a>进程阻塞为什么不占用CPU资源？</h2><p>程序的运行状态有等待和执行等几种状态，其中等待状态就是阻塞状态。</p><p>等待队列是一种重要的数据结构，当进程创建socket语句是，会创建一个socket对象，socket对象包含了发送缓冲区，接收缓冲区，等待队列等成员。</p><p>当程序执行到recv()语句时，操作系统就会将进程A移动到socket的等待队列中去，继续转而去执行进程B\C，因此此时进程A被阻塞，并没有占用CPU资源。</p><p>当socket接收到数据后，等待队列上的进程就会被唤醒，重新放回工作队列，进程变为运行状态。相当于socket的接收缓冲区已经有了数据。</p><h2 id="操作系统如何知道网络数据对应于哪个socket-如何同时监听多个socket数据？"><a href="#操作系统如何知道网络数据对应于哪个socket-如何同时监听多个socket数据？" class="headerlink" title="操作系统如何知道网络数据对应于哪个socket, 如何同时监听多个socket数据？"></a>操作系统如何知道网络数据对应于哪个socket, 如何同时监听多个socket数据？</h2><p>网络数据包包含了ip和端口号信息，一个socket对应着一个端口号。内核可以通过端口号，找到对应的socket。操作i系统会维护端口号到socket的索引的数据结构。</p><p>而epoll的要义就是高效的监听多个socket。</p><p>首先看看select<br>在如下的代码中，先准备一个数组（下面代码中的fds），让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个（也可以是多个）socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD_ISSET判断具体哪个socket收到数据，然后做出处理</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> s = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);  </span><br><span class="line">bind(s, ...)</span><br><span class="line">listen(s, ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> fds[] =  存放需要监听的socket</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">    <span class="keyword">int</span> n = select(..., fds, ...)</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; fds.count; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(FD_ISSET(fds[i], ...))&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//fds[i]的数据处理</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="select-有什么缺点："><a href="#select-有什么缺点：" class="headerlink" title="select 有什么缺点："></a>select 有什么缺点：</h2><p>其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历（遍历进程A关心的所有socket，需要注意的是添加从等待队列头部添加，删除通过回调直接实现，所以每个socket的等待队列不用遍历），而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。<br>其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次（这一次遍历是在应用层）。<br>那么，有没有减少遍历的方法？有没有保存就绪socket的方法？这两个问题便是epoll技术要解决的。<br>当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞</p><h2 id="epoll高效的原因"><a href="#epoll高效的原因" class="headerlink" title="epoll高效的原因"></a>epoll高效的原因</h2><ol><li>功能分离<br>select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用select都需要这两步操作，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程（解耦）。显而易见的，效率就能得到提升</li><li>就绪列表<br>select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据。</li></ol><p>不重复传递socket句柄给内核，通过内核中红黑树存储要监控的句柄，通过双链表存储准备就绪的事件，并结合回调机制，造就了epoll的高效</p><h2 id="epoll使用了哪些数据结构？"><a href="#epoll使用了哪些数据结构？" class="headerlink" title="epoll使用了哪些数据结构？"></a>epoll使用了哪些数据结构？</h2><p>epool_creat()返回一个句柄，会创建一个event_poll结构体<br>红黑树:存储需要监听的事件<br>就绪列表:当红黑树的事件发生，调用epoll_ctl_callback将改事件添加到rdlist中去。</p><p>eptim对应一个事件</p><p><img src="&quot;https://s1.51cto.com/images/20180325/1521963216966024.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&quot;" alt="avatar"></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LRU缓存算法</title>
      <link href="/2021/01/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"/>
      <url>/2021/01/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>LRU缓存算法是双向链表与散列表的合体结构</p><p>需要考虑的问题</p><ol><li><p>当新数据在缓存节点中，找到缓存节点的位置，删除该节点，插入到链表的头节点中</p></li><li><p>当新数据不在缓存列表中，直接插入头部</p></li><li><p>缓存列表满了，删除尾部节点，插入头部节点。</p></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *双链表结点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> val,key;</span><br><span class="line">    node *prev,*next;</span><br><span class="line">    node():prev(<span class="literal">NULL</span>),next(<span class="literal">NULL</span>)&#123;&#125;</span><br><span class="line">    node(<span class="keyword">int</span> k,<span class="keyword">int</span> v):key(k),val(v),prev(<span class="literal">NULL</span>),next(<span class="literal">NULL</span>)&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//重载 == 号</span></span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> node &amp;p) <span class="keyword">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> val==p.val&amp;&amp;key==p.key;</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *双链表</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">DoubleList</span>&#123;</span></span><br><span class="line">     </span><br><span class="line">     <span class="keyword">public</span>:</span><br><span class="line">        node *first;</span><br><span class="line">        node *end;</span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">        DoubleList();</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(node*)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(node*)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">removeLast</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line"> &#125;;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *构造函数，新建首尾节点，相连</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">DoubleList::DoubleList()&#123;</span><br><span class="line">    n=<span class="number">0</span>;</span><br><span class="line">    first = <span class="keyword">new</span> node();</span><br><span class="line">    end = <span class="keyword">new</span> node();</span><br><span class="line">    first-&gt;next = end;</span><br><span class="line">    end-&gt;prev = first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *在第一位添加一个节点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DoubleList::addFirst</span><span class="params">(node *nd)</span></span>&#123;</span><br><span class="line">    n++;</span><br><span class="line">    <span class="comment">//node *tmp = new node(nd-&gt;key,nd&gt;val);</span></span><br><span class="line">    node *t = first-&gt;next;</span><br><span class="line">    nd-&gt;next = t;</span><br><span class="line">    first-&gt;next = nd;</span><br><span class="line">    nd-&gt;prev = first;</span><br><span class="line">    t-&gt;prev = nd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *删除一个肯定存在的节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DoubleList::remove</span><span class="params">(node *nd)</span></span>&#123;</span><br><span class="line">    n--;</span><br><span class="line"></span><br><span class="line">    node *p = first;</span><br><span class="line">    <span class="keyword">while</span>(p-&gt;key!=nd-&gt;key)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    node *pt = p-&gt;prev;</span><br><span class="line">    node *nt = p-&gt;next;</span><br><span class="line">    pt-&gt;next = nt;</span><br><span class="line">    nt-&gt;prev = pt;</span><br><span class="line">    <span class="keyword">delete</span> p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *删除最后一个节点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DoubleList::removeLast</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        node *tmp = end-&gt;prev;</span><br><span class="line">        node *pt = tmp-&gt;prev;</span><br><span class="line">    </span><br><span class="line">        pt-&gt;next = end;</span><br><span class="line">        end-&gt;prev = pt;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> t = tmp-&gt;key;</span><br><span class="line">        <span class="keyword">delete</span> tmp;</span><br><span class="line">        n--;</span><br><span class="line">        <span class="keyword">return</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DoubleList::size</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LRUCache</span>&#123;</span></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,node&gt; <span class="built_in">map</span>;</span><br><span class="line">        DoubleList *lru;    </span><br><span class="line">        <span class="keyword">int</span> maxSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        LRUCache()&#123;&#125;;</span><br><span class="line">        LRUCache(<span class="keyword">int</span> ms);</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> key)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span> val)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line"> &#125;;</span><br><span class="line"></span><br><span class="line">LRUCache::LRUCache(<span class="keyword">int</span> ms)&#123;</span><br><span class="line">    maxSize = ms;</span><br><span class="line">    lru = <span class="keyword">new</span> DoubleList();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *查找该节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">LRUCache::get</span><span class="params">(<span class="keyword">int</span> key)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>.count(key)==<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val = <span class="built_in">map</span>.find(key)-&gt;second.val;</span><br><span class="line">        put(key,val);</span><br><span class="line">        <span class="keyword">return</span> val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*将节点提前</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LRUCache::put</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span> value)</span></span>&#123;</span><br><span class="line">    node *nd = <span class="keyword">new</span> node(key,value);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>.count(nd-&gt;key)==<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="comment">//移到前面</span></span><br><span class="line">        lru-&gt;remove(nd);</span><br><span class="line">        lru-&gt;addFirst(nd);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(lru-&gt;n==maxSize)&#123;</span><br><span class="line">            <span class="keyword">int</span> k = lru-&gt;removeLast();</span><br><span class="line">            <span class="built_in">map</span>.erase(k);</span><br><span class="line">            lru-&gt;addFirst(nd);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            lru-&gt;addFirst(nd);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">map</span>[key] = *nd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LRUCache::show</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(lru-&gt;n==<span class="number">0</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"empty task"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        node *p = lru-&gt;first-&gt;next;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"当前一共有"</span>&lt;&lt;lru-&gt;n&lt;&lt;<span class="string">"个任务: "</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;lru-&gt;n;i++)&#123;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"第"</span>&lt;&lt;i+<span class="number">1</span>&lt;&lt;<span class="string">"个任务: "</span>&lt;&lt;p-&gt;val&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">            p=p-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    LRUCache *l = <span class="keyword">new</span> LRUCache(<span class="number">3</span>);</span><br><span class="line">    l-&gt;put(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">    l-&gt;put(<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">    l-&gt;put(<span class="number">3</span>,<span class="number">4</span>);</span><br><span class="line">    l-&gt;put(<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line">    l-&gt;show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构与算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>传输层</title>
      <link href="/2020/12/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"/>
      <url>/2020/12/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h1><p><strong>传输层（英语：Transport Layer）</strong>在计算机网络中是互联网协议包与开放系统互连（OSI）网络堆栈中协议的分层结构中的方法的一个概念划分。该层的协议为应用进程提供端到端的通信服务。[1] 它提供面向连接的数据流支持、可靠性、流量控制、多路复用等服务。</p><p>从通信和信息处理的角度看，传输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层<br>互联网与一般性网络的开放系统互连OSI模型的基础，TCP/IP模型的传输层的具体实现和含义（RFC 1122）[2]是不同的。在OSI模型中传输层最常被称作第4层或L4，而TCP/IP中不常给网络层编号。<br>IP 协议能够将 IP 数据报由源主机交付至目的主机，那么，为什么还需要传输层呢？这是因为，从 IP 层来说，通信的两端是两个主机。IP 数据报的首部明确地标志了这两个主机的 IP 地址。但是 “两个主机之间的通信” 这种说法还不够清楚，这是因为，真正进行通信的实体是在主机中的进程，是这个主机的一个进程和另外一个主机中的一个进程在交换数据。因此，严格来讲，两个主机进行通信就是两个主机中的应用进程互相通信</p><p>IP 协议虽然将分组送到目的主机，但是这个分组还停留在主机的网络层而没有交付主机中的应用进程。从传输层的角度看，通信的真正端点并不是主机而是主机中的进程。也就是说，端到端的通信是应用进程之间的通信</p><p>传输层为相互通信的应用进程提供逻辑通信，即所谓的 “端“到” 端“通信。并负责对收到的报文进行差错检验，消除网络间不可靠性，提供从源端主机到目的端主机的可靠的、与实际使用的网络无关的信息传输</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FFhSC4.jpg#shadow"></img></div><p>最著名的TCP/IP传输协议是传输控制协议（TCP）, 它的名称借用自整个包的名称。它用于面向连接的传输，而无连接的用户数据报协议（UDP）用于简单消息传输。TCP是更复杂的协议，因为它的状态性设计结合了可靠传输和数据流服务。这个协议组中其他重要协议有数据拥塞控制协议（DCCP）与流控制传输协议（SCTP）。</p><h2 id="复用与分用"><a href="#复用与分用" class="headerlink" title="复用与分用"></a>复用与分用</h2><ul><li>复用是指发送方不同的应用进程都可以使用同一个传输层协议传送数据（需要加适当首部）</li><li>分用是指接收方的传输层在剥去报文的首部后，可以将这些数据正确交付目的应用进程</li></ul><h2 id="TCP与UDP"><a href="#TCP与UDP" class="headerlink" title="TCP与UDP"></a>TCP与UDP</h2><p>TCP/IP 的传输层有两个不同的协议：  </p><p>(1) 用户数据报协议 UDP (User Datagram Protocol)  </p><p>(2) 传输控制协议 TCP (Transmission Control Protocol)</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FF4OtU.jpg#shadow"></img></div><p>UDP 在传送数据之前不需要先建立连接。对方的运输层在收到 UDP 报文后，不需要给出任何确认。UDP 不提供可靠交付</p><p>TCP 则提供面向连接的服务。TCP 不提供广播或多播服务。由于 TCP 要提供可靠的、面向连接的运输服务，因此不可避免地增加了许多的开销。这不仅使协议数据单元的首部增大很多，也占用许多的处理机资源</p><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><p>传输控制协议（英语：Transmission Control Protocol，缩写：TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。在简化的计算机网络OSI模型中，它完成第四层传输层所指定的功能。用户数据报协议（UDP）是同一层内另一个重要的传输协议。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Tcp_state_diagram_fixed_new.svg/375px-Tcp_state_diagram_fixed_new.svg.png"></img></div><p>在因特网协议族（Internet protocol suite）中，TCP层是位于IP层之上，应用层之下的中间层。不同主机的应用层之间经常需要可靠的、像管道一样的连接，但是IP层不提供这样的流机制，而是提供不可靠的包交换。</p><p>应用层向TCP层发送用于网间传输的、用8位字节表示的数据流，然后TCP把数据流分割成适当长度的报文段（通常受该计算机连接的网络的数据链路层的最大传输单元（MTU）的限制）。之后TCP把结果包传给IP层，由它来透过网络将包传送给接收端实体的TCP层。TCP为了保证不发生丢包，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的包发回一个相应的确认信息（ACK）；如果发送端实体在合理的往返时延（RTT）内未收到确认，那么对应的数据包就被假设为已丢失并进行重传。TCP用一个校验和函数来检验数据是否有错误，在发送和接收时都要计算校验和。</p><h3 id="运作方式"><a href="#运作方式" class="headerlink" title="运作方式"></a>运作方式</h3><p>数据在TCP层称为流（Stream），数据分组称为分段（Segment）。作为比较，数据在IP层称为Datagram，数据分组称为分片（Fragment）。 UDP 中分组称为Message。</p><p>TCP协议的运行可划分为三个阶段：连接创建(connection establishment)、数据传送（data transfer）和连接终止（connection termination）。操作系统将TCP连接抽象为套接字表示的本地端点（local end-point），作为编程接口给程序使用。在TCP连接的生命期内，本地端点要经历一系列的状态改变。</p><h3 id="创建通路"><a href="#创建通路" class="headerlink" title="创建通路"></a>创建通路</h3><p>TCP用三次握手（或称三路握手，three-way handshake）过程创建一个连接。在连接创建过程中，很多参数要被初始化，例如序号被初始化以保证按序传输和连接的强壮性。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Connection_TCP.png/330px-Connection_TCP.png"></img></div>一对终端同时初始化一个它们之间的连接是可能的。但通常是由一端打开一个套接字（socket）然后监听来自另一方的连接，这就是通常所指的被动打开（passive open）。服务器端被被动打开以后，用户端就能开始创建主动打开（active open）。1. 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序号设定为随机数A。2. 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK的确认码应为A+1，SYN/ACK包本身又有一个随机产生的序号B。3. 最后，客户端再发送一个ACK。此时包的序号被设定为A+1，而ACK的确认码则为B+1。当服务端收到这个ACK的时候，就完成了三次握手，并进入了连接创建状态。如果服务器端接到了客户端发的SYN后回了SYN-ACK后客户端掉线了，服务器端没有收到客户端回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，服务器端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会断开这个连接。使用三个TCP参数来调整行为：tcp_synack_retries 减少重试次数；tcp_max_syn_backlog，增大SYN连接数；tcp_abort_on_overflow决定超出能力时的行为。### 资源使用主机收到一个TCP包时，用两端的IP地址与端口号来标识这个TCP包属于哪个session。使用一张表来存储所有的session，表中的每条称作Transmission Control Block（TCB），tcb结构的定义包括连接使用的源端口、目的端口、目的ip、序号、应答序号、对方窗口大小、己方窗口大小、tcp状态、tcp输入/输出队列、应用层输出队列、tcp的重传有关变量等。服务器端的连接数量是无限的，只受内存的限制。客户端的连接数量，过去由于在发送第一个SYN到服务器之前需要先分配一个随机空闲的端口，这限制了客户端IP地址的对外发出连接的数量上限。从Linux 4.2开始，有了socket选项IP_BIND_ADDRESS_NO_PORT，它通知Linux内核不保留usingbind使用端口号为0时内部使用的临时端口（ephemeral port），在connect时会自动选择端口以组成独一无二的四元组（同一个客户端端口可用于连接不同的服务器套接字；同一个服务器端口可用于接受不同客户端套接字的连接）。对于不能确认的包、接收但还没读取的数据，都会占用操作系统的资源### 数据传输在TCP的数据传送状态，很多重要的机制保证了TCP的可靠性和强壮性。它们包括：使用序号，对收到的TCP报文段进行排序以及检测重复的数据；使用校验和检测报文段的错误，即无错传输[3]；使用确认和计时器来检测和纠正丢包或延时；流控制（Flow control）；拥塞控制（Congestion control）；丢失包的重传。<div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Tcp_transport_example.gif/525px-Tcp_transport_example.gif"></img></div><ol><li>发送方首先发送第一个包含序列号为1（可变化）和1460字节数据的TCP报文段给接收方。接收方以一个没有数据的TCP报文段来回复（只含报头），用确认号1461来表示已完全收到并请求下一个报文段。</li><li>发送方然后发送第二个包含序列号为1461，长度为1460字节的数据的TCP报文段给接收方。正常情况下，接收方以一个没有数据的TCP报文段来回复，用确认号2921（1461+1460）来表示已完全收到并请求下一个报文段。发送接收这样继续下去。</li><li>然而当这些数据包都是相连的情况下，接收方没有必要每一次都回应。比如，他收到第1到5条TCP报文段，只需回应第五条就行了。在例子中第3条TCP报文段被丢失了，所以尽管他收到了第4和5条，然而他只能回应第2条。</li><li>发送方在发送了第三条以后，没能收到回应，因此当时钟（timer）过时（expire）时，他重发第三条。（每次发送者发送一条TCP报文段后，都会再次启动一次时钟：RTT）。</li><li>这次第三条被成功接收，接收方可以直接确认第5条，因为4，5两条已收到。</li></ol><h3 id="校验和"><a href="#校验和" class="headerlink" title="校验和"></a>校验和</h3><p>TCP的16位的校验和（checksum）的计算和检验过程如下：发送者将TCP报文段的头部和数据部分的和计算出来，再对其求反码（一的补码），就得到了校验和，然后将结果装入报文中传输。（这里用反码和的原因是这种方法的循环进位使校验和可以在16位、32位、64位等情况下的计算结果再叠加后相同）接收者在收到报文后再按相同的算法计算一次校验和。这里使用的反码使得接收者不用再将校验和字段保存起来后清零，而可以直接将报文段连同校验加总。如果计算结果是全部为一，那么就表示了报文的完整性和正确性。</p><p>注意：TCP校验和也包括了96位的伪头部，其中有源地址、目的地址、协议以及TCP的长度。这可以避免报文被错误地路由。</p><p>按现在的标准，TCP的校验和是一个比较脆弱的校验。出错概率高的数据链路层需要更高的能力来探测和纠正连接错误。TCP如果是在今天设计的，它很可能有一个32位的CRC校验来纠错，而不是使用校验和。但是通过在第二层使用通常的CRC校验或更完全一点的校验可以部分地弥补这种脆弱的校验。第二层是在TCP层和IP层之下的，比如PPP或以太网，它们使用了这些校验。但是这也并不意味着TCP的16位校验和是冗余的，对于因特网传输的观察，表明在受CRC校验保护的各跳之间，软件和硬件的错误通常也会在报文中引入错误，而端到端的TCP校验能够捕捉到大部分简单的错误。这就是应用中的端到端原则。</p><h3 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h3><p>流量控制用来避免主机分组发送得过快而使接收方来不及完全收下，一般由接收方通告给发送方进行调控。</p><p>TCP使用滑动窗口协议实现流量控制。接收方在“接收窗口”域指出还可接收的字节数量。发送方在没有新的确认包的情况下至多发送“接收窗口”允许的字节数量。接收方可修改“接收窗口”的值。</p><p>TCP包的序号与接收窗口的行为很像时钟。<br>当接收方宣布接收窗口的值为0，发送方停止进一步发送数据，开始了“保持定时器”（persist timer），以避免因随后的修改接收窗口的数据包丢失使连接的双侧进入死锁，发送方无法发出数据直至收到接收方修改窗口的指示。当“保持定时器”到期时，TCP发送方尝试恢复发送一个小的ZWP包（Zero Window Probe），期待接收方回复一个带着新的接收窗口大小的确认包。一般ZWP包会设置成3次，如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。</p><p>如果接收方以很小的增量来处理到来的数据，它会发布一系列小的接收窗口。这被称作愚蠢窗口综合症，因为它在TCP的数据包中发送很少的一些字节，相对于TCP包头是很大的开销。解决这个问题，就要避免对小的window size做出响应，直到有足够大的window size再响应：</p><ul><li>接收端使用David D Clark算法：如果收到的数据导致window size小于某个值，可以直接ack把window给关闭了，阻止了发送端再发数据。等到接收端处理了一些数据后windows size大于等于了MSS，或者接收端buffer有一半为空，就可以把window打开让发送端再发数据过来。</li><li>发送端使用Nagle算法来延时处理，条件一：Window Size&gt;=MSS 或是 Data Size &gt;=MSS；条件二：等待时间或是超时200ms，这两个条件有一个满足，才会发数据，否则就是在积累数据。Nagle算法默认是打开的，所以对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性程序，需要关闭这个算法。可以在Socket设置TCP_NODELAY选项来关闭这个算法。</li></ul><h3 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h3><p>拥塞控制是发送方根据网络的承载情况控制分组的发送量，以获取高性能又能避免拥塞崩溃（congestion collapse，网络性能下降几个数量级）。这在网络流之间产生近似最大最小公平分配。</p><p>发送方与接收方根据确认包或者包丢失的情况，以及定时器，估计网络拥塞情况，从而修改数据流的行为，这称为拥塞控制或网络拥塞避免。</p><p>TCP的现代实现包含四种相互影响的拥塞控制算法：慢开始、拥塞避免、快速重传、快速恢复。</p><p>此外，发送方采取“超时重传”（retransmission timeout，RTO），这是估计出来回通信延迟 (RTT) 以及RTT的方差。</p><h3 id="终结通路"><a href="#终结通路" class="headerlink" title="终结通路"></a>终结通路</h3><p>连接终止使用了四路握手过程（或称四次握手，four-way handshake），在这个过程中连接的每一侧都独立地被终止。当一个端点要停止它这一侧的连接，就向对侧发送FIN，对侧回复ACK表示确认。因此，拆掉一侧的连接过程需要一对FIN和ACK，分别由两侧端点发出。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Deconnection_TCP.png/330px-Deconnection_TCP.png"></img></div><div align='center'>TCP的终止连接</div><p>首先发出FIN的一侧，如果给对侧的FIN响应了ACK，那么就会超时等待2*MSL时间，然后关闭连接。在这段超时等待时间内，本地的端口不能被新连接使用；避免延时的包的到达与随后的新连接相混淆。RFC793定义了MSL为2分钟，Linux设置成了30s。参数tcp_max_tw_buckets控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的TIME_WAIT状态的连接给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow）</p><p>连接可以工作在TCP半开状态。即一侧关闭了连接，不再发送数据；但另一侧没有关闭连接，仍可以发送数据。已关闭的一侧仍然应接收数据，直至对侧也关闭了连接。</p><p>也可以通过测三次握手关闭连接。主机A发出FIN，主机B回复FIN &amp; ACK，然后主机A回复ACK.</p><p>一些主机（如Linux或HP-UX）的TCP栈能实现半双工关闭序列。这种主机如果主动关闭一个连接但还没有读完从这个连接已经收到的数据，该主机发送RST代替FIN。这使得一个TCP应用程序能确认远程应用程序已经读了所有已发送数据，并等待远程侧发出的FIN。但是远程的TCP栈不能区分Connection Aborting RST与Data Loss RST，两种原因都会导致远程的TCP栈失去所有的收到数据。</p><p>一些应用协议使用TCP open/close handshaking，因为应用协议的TCP open/close handshaking可以发现主动关闭的RST问题。例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = connect(remote);</span><br><span class="line">send(s, data);</span><br><span class="line">close(s);</span><br></pre></td></tr></table></figure></p><h3 id="状态编码"><a href="#状态编码" class="headerlink" title="状态编码"></a>状态编码</h3><p>下表为TCP状态码列表，以S指代服务器，C指代客户端，S&amp;C表示两者，S/C表示两者之一：[15]</p><ul><li>LISTEN S<br>服务器等待从任意远程TCP端口的连接请求。侦听状态。</li><li>SYN-SENT C<br>客户在发送连接请求后等待匹配的连接请求。通过connect()函数向服务器发出一个同步（SYNC）信号后进入此状态。</li><li>SYN-RECEIVED S<br>服务器已经收到并发送同步（SYNC）信号之后等待确认（ACK）请求。</li><li>ESTABLISHED S&amp;C<br>服务器与客户的连接已经打开，收到的数据可以发送给用户。数据传输步骤的正常情况。此时连接两端是平等的。这称作全连接。</li><li>FIN-WAIT-1 S&amp;C<br>（服务器或客户）主动关闭端调用close（）函数发出FIN请求包，表示本方的数据发送全部结束，等待TCP连接另一端的ACK确认包或FIN&amp;ACK请求包。</li><li>FIN-WAIT-2 S&amp;C<br>主动关闭端在FIN-WAIT-1状态下收到ACK确认包，进入等待远程TCP的连接终止请求的半关闭状态。这时可以接收数据，但不再发送数据。</li><li>CLOSE-WAIT S&amp;C<br>被动关闭端接到FIN后，就发出ACK以回应FIN请求，并进入等待本地用户的连接终止请求的半关闭状态。这时可以发送数据，但不再接收数据。</li><li>CLOSING S&amp;C<br>在发出FIN后，又收到对方发来的FIN后，进入等待对方对己方的连接终止（FIN）的确认（ACK）的状态。少见。</li><li>LAST-ACK S&amp;C<br>被动关闭端全部数据发送完成之后，向主动关闭端发送FIN，进入等待确认包的状态。</li><li>TIME-WAIT S/C<br>主动关闭端接收到FIN后，就发送ACK包，等待足够时间以确保被动关闭端收到了终止请求的确认包。【按照RFC 793，一个连接可以在TIME-WAIT保证最大四分钟，即最大分段寿命（maximum segment lifetime）的2倍】</li><li>CLOSED S&amp;C<br>完全没有连接。</li></ul><h3 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h3><p>TCP使用了通信端口（Port number）的概念来标识发送方和接收方的应用层。对每个TCP连接的一端都有一个相关的16位的无符号端口号分配给它们。端口被分为三类：众所周知的、注册的和动态/私有的。众所周知的端口号是由因特网赋号管理局（IANA）来分配的，并且通常被用于系统一级或根进程。众所周知的应用程序作为服务器程序来运行，并被动地侦听经常使用这些端口的连接。例如：FTP、TELNET、SMTP、HTTP等。注册的端口号通常被用来作为终端用户连接服务器时短暂地使用的源端口号，但它们也可以用来标识已被第三方注册了的、被命名的服务。动态/私有的端口号在任何特定的TCP连接外不具有任何意义。可能的、被正式承认的端口号有65535个。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>应用层</title>
      <link href="/2020/11/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/"/>
      <url>/2020/11/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h1><p>应用层（英语：Application layer）位于OSI模型的第七层。应用层直接和应用程序接口结合，并提供常见的网络应用服务。应用层也向第六层表示层发出请求。</p><h2 id="域名系统（DNS"><a href="#域名系统（DNS" class="headerlink" title="域名系统（DNS)"></a>域名系统（DNS)</h2><p>域名通俗来讲就是网络上每个地址的名称，全球唯一（难道有两个 www.baidu.com 吗）</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FF7HFP.png#shadow"></img></div><h3 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h3><p>先输入nslookup查看本机 DNS，在输入要解析的域名www.baidu.com，返回该域名的 IP 地址</p><h2 id="DHCP服务器"><a href="#DHCP服务器" class="headerlink" title="DHCP服务器"></a>DHCP服务器</h2><p>动态主机配置协议 DHCP，负责给互联网上的计算机提供动态的 IP 地址</p><p>IP 地址获取方式有两种，一种静态 IP，一种动态 IP。静态 IP 是人工自己指定的，一般公司自己组建的局域网、学校机房的固定的计算机、机房服务器、互联网上的大型服务器，凡是位置固定不动的，都用静态 IP 地址。动态 IP 地址是用 DHCP 服务器来分配的地址，适用于计算机位置不固定、家庭拨号上网等情况。可以避免产生 IP 地址冲突。</p><p>DHCP 客户端请求 IP 地址的过程（逆 arp 协议）</p><p>需要地址的客户机先在网上发广播包请求地址，DHCP 服务器收到广播包后在自己的地址池里选一个地址（包括配套的子网掩码和网关），租给该客户机，该客户机再给 DHCP 服务器一个确认。（DHCP 服务器本身必须是静态地址！！！）</p><h2 id="FTP服务器"><a href="#FTP服务器" class="headerlink" title="FTP服务器"></a>FTP服务器</h2><p>FTP 连接方式：</p><ul><li>控制连接：标准端口为 21，用于发送 FTP 命令信息</li><li>数据连接：标准端口为 20，用于上传、下载数据</li><li><p>客户端选择数据连接的建立类型</p></li><li><p>主动模式：FTP 客户端告诉 FTP 服务器使用什么端口，FTP 服务器就主动用自己的 20 端口和 FTP 客户端的这个端口建立连接</p></li><li><p>被动模式：服务端在指定范围内打开一个新的端口，被动等待客户端发起连接<br>FTP 传输模式：</p></li><li><p>文本模式：ASCII 模式，以文本序列传输数据</p></li><li>二进制模式：Binary 模式，以二进制序列传输数据</li></ul><h2 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h2><ul><li>www：万维网简称，分布式超媒体系统。URL 标记这个大系统中的资源位置，HTTP 进行资源传输，HTML 用以显示页面，搜索引擎提供寻找资源的方法</li><li>url 的语法：协议名:// 主机域名或 ip 地址: 端口 / 资源的路径? 参数 1 = 值 1# 参数 2 = 值 2</li><li>作为服务器，要时刻监听 80 端口，一旦有访问，就建立一条 tcp 链接，在此基础上传输 http 协议的数据。解析协议数据，获取访问的 html 文档。由于采用了 tcp 请求，就有一个请求和确认，收到确认的过程。http 协议中，同样有一个请求链接时间，一个传输报文的时间。我们会设置一个超时最大的值，超过即放弃请求。</li><li>代理服务器：即高速缓存，解决了路由器 ip 地址被自己的客户机使用崩溃的问题。</li><li>HTTP 请求报文中的请求方法：get读取信息；POST给服务器添加信息；option请求一些选项的信息；head请求 html 页面里的 headers 标签；put在指定的 url 下存一些文档；DELETE删除 url 对应的文件；connect用于代理服务器；trace环回测试的请求报文</li><li>Cookie 是个服务器跟踪客户的工具</li><li>动态文档：动态文档是指在客户端请求到服务器时，服务器首先安排一个应用程序处理客户端的请求数据，并把处理后的数据封装成 http 报文里面，传输给客户端。这样，主要是有一些数据是实时可变的，不可能将数据放在静态的 html 里面，那样讲无法同步更新。但是这种动态指的是每次刷新都获取当前值，而不刷新的话，数据不会自己变化呈现。</li><li>活动 web 文档：服务器推送：在这种情况下，服务器和客户端之间建立 http 长链接，服务器不断地把数据推送给客户端。动画呈现，时间更新等等。太占带宽，链接不断，占用服务器资源，端口，带宽，时间资源。 活动文档：服务器直接把一个活动的程序，脚本之类的返回给客户端，其实链接已经断开了，但是程序运行呈现在页面上，看起来是连续不断的数据</li><li>搜索技术</li><li>加密技术 https：http 明文在网上传播不安全，对于有安全需求的消息，服务器和客户端之间首先建立连接，之后，服务器给客户端提供一个加密算法，一把秘钥，客户端程序使用算法和秘钥对数据加密，传输给服务器，服务器再用另一把不同的秘钥将数据解密出来</li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络层</title>
      <link href="/2020/11/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
      <url>/2020/11/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h1><p>网络层（Network Layer）是OSI模型中的第三层（TCP/IP模型中的网际层），提供路由和寻址的功能，使两终端系统能够互连且决定最佳路径，并具有一定的拥塞控制和流量控制的能力。相当于发送邮件时需要地址一般重要。由于TCP/IP协议体系中的网络层功能由IP协议规定和实现，故又称IP层。</p><p>在网络层也能为主机之间提供无连接和有链接的服务。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/17/iz69CF.png#shadow"></img></div><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><h3 id="寻址"><a href="#寻址" class="headerlink" title="寻址"></a>寻址</h3><p>对网络层而言使用IP地址来唯一标识互联网上的设备，网络层依靠IP地址进行相互通信（类似于数据链路层的MAC地址），详细的编址方案参见IPv4和IPv6。</p><h3 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h3><p>在同一个网络中的内部通信并不需要网络层设备，仅仅靠数据链路层就可以完成相互通信，对于不同的网络之间相互通信则必须借助路由器等三层设备。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>在网络层中这些服务（无论是有链接还是无连接）都是提供主机到主机的服务，在传输层中提供的则是提供应用层进程之间的服务。</li><li>在至今为止的所有的主要计算机网络结构体系中（因特网、ATM、帧中继等），网络层提供了主机到主机无连接或者有连接服务，而不同时提供两种服务。仅提供无连接的的网络称为数据报网络(Datagram Network)，仅提供有连接的网络称为虚电路网络（Virtual-Circuit，VC）。</li></ul><h2 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h2><p>网际协议（英语：Internet Protocol，缩写：IP；也称互联网协议）是用于分组交换数据网络的一种协议。</p><p>IP是在TCP/IP协议族中网络层的主要协议，任务仅仅是根据源主机和目的主机的地址来传送数据。为此目的，IP定义了寻址方法和数据报的封装结构。第一个架构的主要版本为IPv4，目前仍然是广泛使用的互联网协议，尽管世界各地正在积极部署IPv6。</p><ul><li>IP （V4 V6）</li><li>IPX</li><li>X.25</li><li>RARP</li><li>ICMP（V4、V6）</li><li>IGMP</li><li>IPsec</li><li>RIP</li></ul><h2 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h2><ol><li>网络地址</li></ol><p>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p><ol><li><p>广播地址<br>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p></li><li><p>地址划分</p></li></ol><div align='center'><img src="https://s1.ax1x.com/2018/11/18/izbBMq.jpg#shadow"></img></div><ul><li>A 类地址以 0 开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255</li><li>B 类地址以 10 开头，前两个字节作为网络号，地址范围是：127.0.0.0~191.255.255.255</li><li>C 类地址以 110 开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255</li><li>D 类地址以 1110 开头，地址范围是 224.0.0.0~239.255.255.255，D 类地址作为组播地址（一对多的通信）</li><li>E 类地址以 1111 开头，地址范围是 240.0.0.0~255.255.255.255，E 类地址为保留地址，供以后使用</li></ul><p>注：只有 A、B、C 由网络号和主机号之分，D、E 没有划分网络号和主机号</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/izzIat.png#shadow"></img></div><h2 id="特殊地址"><a href="#特殊地址" class="headerlink" title="特殊地址"></a>特殊地址</h2><ul><li>255.255.255.255</li></ul><p>该 IP 地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，首先广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的的地址的分组；一般广播地址既可以在本地广播，也可跨网段广播。例如：主机 192.168.1.1/30 直接广播数据包后，另一个网段 192.168.1.15/30 也能直接收到该数据包；若发送受限广播数据包则不能收到</p><p>注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址则不能通过路由器</p><ul><li><p>0.0.0.0<br>常用于寻找自己的 IP 地址，例如在 RARP，BOOTP 和 DHCP 协议中，若某个位置 IP 地址的无盘机想要知道自己的 IP 地址，他就以 255.255.255.255 为目的地址，像本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送 IP 请求分组</p></li><li><p>回环地址<br>127.0.0.0/8 被用作回环地址，会换地址表示本机的地址，常用于对本机的测试，用得最多的是 127.0.0.1</p></li><li><p>A、B、C 类私有地址<br>私有地址（Private Address）也叫专用地址，他们不会在全球使用，只具有本地意义</p></li></ul><ul><li>A 类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255</li><li>B 类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255</li><li>C 类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255</li></ul><h2 id="子网掩码及网络划分"><a href="#子网掩码及网络划分" class="headerlink" title="子网掩码及网络划分"></a>子网掩码及网络划分</h2><p>子网掩码是标志两个 IP 地址是否属于一个子网的，也是 32 位二进制地址，其每一个 1 代表该位是网络位，0 代表主机位。它和 IP 地址一样也是使用点式十进制来表示的。如果两个 IP 地址在子网掩码的按位与计算所得结果相同，即表明它们共属于同一个子网中。</p><p>注：在计算子网掩码时，要注意 IP 地址中的保留地址，即 “0” 地址和广播地址，他们是指主机地址或网络全为 “0” 或 “1” 时的 IP 地址，他们代表着本网络地址和广播地址，一般是不能被计算在内的。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSCYw9.png#shadow"></img></div><div align='center'>利用子网掩码解析IP地址</div><ul><li>求子网掩码前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数</li></ul><p>（1）将子网数转化为二进制表示：如欲将 B 类 IP 地址 168.195.0.0 划分成 27 个子网：27=11011;</p><p>（2）取得该二进制的位数，为 N：该二进制为 5 位数，N=5</p><p>（3）取得该 IP 地址的类子网掩码，将其主机地址部分的前 N 位置 1 即得出该 IP 地址划分子网的子网掩码</p><p>该 B 类地址的子网掩码 255.255.0.0 的主机地址前 5 位置 1，得到 255.255.248.0.</p><ul><li>利用主机数来计算<br>如欲将 B 类 IP 地址 168.195.0.0 划分成若干子网，每个子网内由主机 700 台：</li></ul><p>（1）将主机数转化为二进制表示：700=1010111100;</p><p>（2）如果主机数小于或等于 254，则取得该主机的二进制位数，为 N。如果大于 254，也就是说主机地址将占据不止 8 位，该二进制为十位数，N=10</p><p>（3）使用 255.255.255.255 来将该类 IP 地址的主机地址位数全部置 1，然后从后向前将 N 位全部置 0，即为子网掩码值。将该 B 类地址的子网掩码 255.255.0.0 的主机地址全部置 1，得到 255.255.255.255，然后再从后向前将十位置 0，即为：11111111.11111111.11111100.00000000，即 255.255.252.0. 这就是将划分成主机为 700 台 B 类 IP 地址 168.195.0.0 的子网掩码。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSCdW6.png#shadow"></img></div>同一个网段的中的计算机子网掩码相同，计算机的网关就就是到其他网段的出口，也就是路由器接口地址。路由器接口使用的地址可以是本网段中任何一个地址，不过通常使用该网段的第一个可用的地址或最后一个可用的地址，这是为了尽可能避免和网络中的计算机地址冲突<div align='center'><img src="https://s1.ax1x.com/2018/11/18/izvjSg.jpg#shadow"></img></div><p>结合上面内容及 IP 地址特点，我们应该注意到：</p><ul><li>在同一个局域网上的主机或路由器的 IP 地址中的网络号必须是一样的。图中的网络号就是 IP 地址中的 网络号字段的值</li><li>路由器总是具有两个或两个以上的 IP 地址。路由器的每一个接口都有一个不同网络号的 IP 地址</li><li>两个路由器直接相连的接口处，可指明也可不指明 IP 地址。如指明 IP 地址，则这一段连线就构成了一种只包含一段线路的特殊 “网络”（如图中 N1、N2、N3）。之所以称之为网络，因为他们有 IP 地址、但为了节省 IP 地址，对于这种仅由一段连线构成的特殊 “网络”，现在常不指明 IP 地址。称为无编号网络或者无名网络</li><li>用网桥（它只在数据链路层工作）互连的网段仍然是一个局域网，只能有一个网络号</li></ul><h3 id="划分子网"><a href="#划分子网" class="headerlink" title="划分子网"></a>划分子网</h3><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSehpd.jpg#shadow"></img></div><p>现在把上图的网络划分为三个子网（下图）。这里假定子网号占用 8 位，因此在增加了子网号后，主机号只有 8 位。所划分的三个子网分别是：145.13.3.0、145.13.7.0 和 145.13.21.0。在划分完子网后，整个网络对外部仍然表现为一个网络，其网络地址仍为 145.13.0.0。网络 145.13.0.0 上的路由器 R1 在收到外来的数据报后，再根据数据报的目的地址把它转发到相应的子网</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSuhp4.jpg#shadow"></img></div><h3 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h3><ul><li>默认子网掩码<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSQv3F.jpg#shadow"></img></div><div align='center'>默认子网掩码</div></li><li>获取子网地址<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSKejs.jpg#shadow"></img></div><div align='center'>获取子网地址</div></li><li>判断弟子所属网段<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSNDc6.png#shadow"></img></div><div align='center'>判断子网地址网段</div></li></ul><h2 id="路由转发算法"><a href="#路由转发算法" class="headerlink" title="路由转发算法"></a>路由转发算法</h2><p>当划分子网后，路由表必须包含以下三项内容：目的网络地址、子网掩码和下一跳地址。在划分分组情况下，路由器转发分组的算法如下：</p><ol><li>从收到的分组的首部提取目的 IP 地址 D</li><li>先用各网络的子网掩码和 D 逐位相 “与”，看是否和相应的网络地址匹配。若匹配，则将分组直接交付。否则就是间接交付，执行 3</li><li>若路由表中有目的地址为 D 的特定主机路由，则将 分组传送给指明的下一跳路由器；否则，执行 4</li><li>对路由表中的每一行的子网掩码和 D 逐位相 “与”，若其结果与该行的目的网络地址匹配，则将分组传送 给该行指明的下一跳路由器；否则，执行 5</li><li>若路由表中有一个默认路由，则将分组传送给路由表中所指明的默认路由器；否则，执行 6</li><li>报告转发分组出错</li></ol><h2 id="网络层的功能设备"><a href="#网络层的功能设备" class="headerlink" title="网络层的功能设备"></a>网络层的功能设备</h2><ul><li>路由器</li><li>三层交换机</li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>物理层</title>
      <link href="/2020/10/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%A9%E7%90%86%E5%B1%82/"/>
      <url>/2020/10/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%A9%E7%90%86%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h1><p>物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体</p><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf15d2286ea.png#shadow"></img></div><h2 id="局域网通信模型"><a href="#局域网通信模型" class="headerlink" title="局域网通信模型"></a>局域网通信模型</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf149f057e5.png#shadow"></img></div><h2 id="广域网通信模型"><a href="#广域网通信模型" class="headerlink" title="广域网通信模型"></a>广域网通信模型</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf15fc5d58d.png#shadow"></img></div><h2 id="模拟信号与数字信号"><a href="#模拟信号与数字信号" class="headerlink" title="模拟信号与数字信号"></a>模拟信号与数字信号</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf167d7ac2c.png#shadow"></img></div>模拟信号与数字信号转换<div align='center'><img src="https://i.loli.net/2018/10/23/5bcf17ca2d7d7.png#shadow"></img></div><h2 id="信道"><a href="#信道" class="headerlink" title="信道"></a>信道</h2><p>信道（Channel）是信息传输的通道，即信息进行传输时所经过的一条通路，信道的一端是发送端，另一端是接收端。一条传输介质上可以有多条信道（多路复用）</p><ul><li>单向信道<br>又称为单工通信，即信号只能向一个方向传输，任何时候都不能改变信号的传送方向。无线电广播或有线电视广播就是单工通信，信号只能是广播电台发送，收音机接收。</li><li>双向交替信道<br>又称半双工通信，信号可以双向传送，但是必须是交替进行，一个时间只能向一个方向传。有些对讲机就是用半双工通信，A 端说话 B 端接听，B 端说话 A 端接听，不能同时说和听。</li><li>双向同时信道<br>又称全双工通信，即信号可以同时双向传送。比如我们手机打电话，听和说可以同时进行。</li></ul><h2 id="奈氏准则"><a href="#奈氏准则" class="headerlink" title="奈氏准则"></a>奈氏准则</h2><p>在任何信道中，码元传输的速率是有上限的，否则就会出现码间串扰的问题，使接收端对码元的判决（即识别）成为不可能。举个例子，假如把一个人说话的声音录下来，然后进行倍速播放，当倍速达到一定程度时就听不清楚了。</p><p>如果信道的频带越宽，也就是能够通过的信号高频分量越多，那么就可以使用更高速率传递码元而不出现码间串扰</p><p>理想低通信道的最高码元传输速率 = 2WBaud</p><p>W 是理想低通信道的带宽，单位为 HZ<br>Baud 是波特，是码元传输速率的单位<br>使用奈氏准则给出的公式，可以根据信道的带宽，计算出码元的最高传输速率</p><h2 id="信道复用技术"><a href="#信道复用技术" class="headerlink" title="信道复用技术"></a>信道复用技术</h2><h3 id="频分复用"><a href="#频分复用" class="headerlink" title="频分复用"></a>频分复用</h3><div align='center'><img src="https://i.loli.net/2018/10/24/5bcffbba16928.png#shadow"></img></div>### 时分复用时分复用采用同一物理连接的不同时段来传输不同的信号，将时间划分为一段段等长的时分复用帧（TDM 帧）<div align='center'><img src="https://i.loli.net/2018/10/24/5bcffc6f83469.png#shadow"></img></div><h3 id="波分复用"><a href="#波分复用" class="headerlink" title="波分复用"></a>波分复用</h3><p>提高光纤的传输信号的速率，也可以进行频分复用，由于光载波的频率很高，因此习惯上用波长而不用频率来表示所使用的光载波。这样就得出了波分复用这一名词</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据链路层</title>
      <link href="/2020/10/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
      <url>/2020/10/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<h1 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h1><p>数据链路层（Data Link Layer）是OSI参考模型第二层，位于物理层与网络层之间。在广播式多路访问链路中（局域网），由于可能存在介质争用，它还可以细分成介质访问控制（MAC）子层和逻辑链路控制（LLC）子层，介质访问控制（MAC）子层专职处理介质访问的争用与冲突问题。</p><p>主要功能为在两个网络实体之间提供数据链路连接的创建、维持和释放管理。构成数据链路数据单元（frame：数据帧或帧），并对帧定界、同步、收发顺序的控制。传输过程中的网络流量控制、差错检测和差错控制等方面。</p><p>局域网与广域网皆属第一、二层。</p><h2 id="数据链路与帧"><a href="#数据链路与帧" class="headerlink" title="数据链路与帧"></a>数据链路与帧</h2><p><strong>链路（Link)</strong>是指的从一个节点到相邻节点的一段物理线路（有线或无线），而中间没有任何其他的交换节点。</p><p><strong>数据链路（Data Link）</strong>则是另一个概念，这是因为当需要在一条线路上传送数据时，除了必须有一条物理线路外，还必须有一些必要的通信协议来控制这些数据的传输。</p><p>数据链路层把网络层交下来的数据封装成帧发送到链路上，以及把接收到的帧中的数据取出并上交给网络层。在因特网中，网络层协议数据单元就是 IP 数据报（或简称为数据报、分组或包）。数据链路层封装的帧，在物理层变成数字信号在链路上传输。</p><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66b985a2b3.png#shadow"></img></div><h2 id="封装成帧"><a href="#封装成帧" class="headerlink" title="封装成帧"></a>封装成帧</h2><p>封装成帧，将网络层的 IP 数据报的前后分别添加首部和尾部，就构成了一个帧。</p><p>每一种数据链路层协议都规定了所能够传送的帧的数据部分长度的上限 — 即最大传输单元 MTU（Maximum Transfer Unit），以太网的 MTU 为 1500 个字节。</p><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66fbf448ae.png#shadow"></img></div><h2 id="差错控制"><a href="#差错控制" class="headerlink" title="差错控制"></a>差错控制</h2><p>比特在传输过程中可能会产生差错：1 可能会变成 0，而 0 也可能变成 1，这就叫做比特差错。</p><p>为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施。目前在数据链路层广泛使用了循环冗余检验 CRC(Cyclic Redundancy Check）的差错检验技术。</p><h2 id="PPP协议"><a href="#PPP协议" class="headerlink" title="PPP协议"></a>PPP协议</h2><p>有3个组成部分</p><div align='center'><img src="https://s1.ax1x.com/2018/10/30/i2OjUA.png#shadow"></img></div><ul><li>异步传输使用字节填充<br>在异步传输的链路上，数据传输以字节为单位，PPP 帧的转义符定义为 0x7D，并使用字节填充</li></ul><p>把信息字段中出现的每一个 0x7E 字节转变成为 2 字节序列（0x7D，0x5E）</p><p>若信息字段中出现一个 0x7D 的字节（即出现了和转义字符一样的比特组合），则把 0x7D 转变成为 2 字节序列（0x7D，0x5D）</p><ul><li>同步使用零比特填充<br>在同步传输的链路上，数据传输以帧为单位，PPP 协议采用零比特填充方法来实现透明传输。如果把 PPP 协议帧界定符 0x7E 写成二进制 01111110，可以看到中间有连续的 6 个 1, 只要想办法在数据部分不要出现连续的 6 个 1, 就肯定不会出现这界定符。具体办法就是 “零比特填充法”。</li></ul><h2 id="以太网帧格式"><a href="#以太网帧格式" class="headerlink" title="以太网帧格式"></a>以太网帧格式</h2><p>常用的以太网 MAC 帧格式有两种标准，一种是 EthernetV2 标准（即以太网 V2 标准），另一种是 IEEE 的 802.3 标准。使用得最多的是以太网 V2 的 MAC 帧格式。</p><p>信道利用率</p><p>设帧长为 $L(bit)$，数据发送率为 $C(bit/s)$，所以帧的发送时间为$ L/C=T_0(s)$<br>利用率是指的发送数据的时间占整个时间的比例。如下图所示，平均发送一帧所需要的时间经历了 n 倍争用期 $2\tau，T_0 $为发送该帧所需时间，$\tau$ 为该帧传播时延。</p><p>有冲突时信道利用率为</p><script type="math/tex; mode=display">S=\frac{T_0}{n2\tau +T_0+\tau}</script><p>从公式可以看出，要想提高信道利用率最好是 n=0，这就意味着以太网上的各个计算机发送数据不会产生碰撞（这显然已经不是 CSMA/CD，而需要一种特殊的调度方法），并且能够非常有效的利用网络的传输资源，即总线一旦空闲就有一个站立即发送数据。这种情况算出来的信道利用率是极限信道利用率。</p><script type="math/tex; mode=display">S_{max}=\frac{T_0}{T_0+\tau}=\frac{1}{1+\frac{\tau}{T_0}}</script><p>要想提高极限信道利用率就要降低公式中的 $\frac{\tau}{T_0}$<br>降低上面的分式有两种办法，分子足够小或者分母足够大，首先说分子，$\tau $值和以太网连线的长度有关，这就意味着$ \tau $值要小，以太网网线的长度就不能太长。其次是分母，带宽一定的情况下 $T_0 $和帧的长度有关，这就意味着，以太网的帧不能太短。</p><h2 id="MAC地址"><a href="#MAC地址" class="headerlink" title="MAC地址"></a>MAC地址</h2><p>这种 6 字节的 MAC 地址已被固化在网卡的 ROM 中。因此，MAC 地址也叫作硬件地址（hardware address）或物理地址。当这块网卡插入（或嵌入）到某台计算机后，网卡上的 MAC 地址就成为这台计算机的 MAC 地址了。</p><ul><li>单播（unicast）帧（一对一），即收到的帧的 MAC 地址与本站的硬件地址相同</li><li>广播（broadcast）帧（一对全体），即发送给本局域网上所有站点的帧（全 1 地址）</li><li>多播（multicast）帧（一对多），即发送给本局域网上一部分站点的帧</li></ul><h2 id="数据链路层的设备"><a href="#数据链路层的设备" class="headerlink" title="数据链路层的设备"></a>数据链路层的设备</h2><ul><li>交换机是本层设备。而集线器是物理层设备，不是数据链路层设备。</li><li>桥接器</li></ul>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络基础</title>
      <link href="/2020/10/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/10/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="计算机网络基础"><a href="#计算机网络基础" class="headerlink" title="计算机网络基础"></a>计算机网络基础</h1><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66b001571e.png#shadow"></img></div><h2 id="网络层次划分"><a href="#网络层次划分" class="headerlink" title="网络层次划分"></a>网络层次划分</h2><p>为了使不同计算机厂家生产的计算机能相互通信，在更大范围内建立计算机网络，国际标准化组织（ISO）在 1978 年提出了 “开放系统互联参考模型”，即著名的 OSI/RM 模型（Open System Interconnection/Reference Model）。它将计算机网络体系结构的通信协议划分为七层，自下而上依次为：<strong>物理层（Physics Layer）、数据链路层（Data Link Layer）、网络层（Network Layer）、传输层（Transport Layer）、会话层（Session Layer）、表示层（Presentation Layer）、应用层（Application Layer）</strong>。</p><div align='center'><img src="https://i.loli.net/2018/10/01/5bb234b6321ea.jpg#shadow"></img></div><h2 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h2><ol><li>网络地址</li></ol><p>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p><ol><li><p>广播地址<br>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p></li><li><p>地址划分</p></li></ol><ul><li>A 类地址以 0 开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255</li><li>B 类地址以 10 开头，前两个字节作为网络号，地址范围是：127.0.0.0~191.255.255.255</li><li>C 类地址以 110 开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255</li><li>D 类地址以 1110 开头，地址范围是 224.0.0.0~239.255.255.255，D 类地址作为组播地址（一对多的通信）</li><li>E 类地址以 1111 开头，地址范围是 240.0.0.0~255.255.255.255，E 类地址为保留地址，供以后使用</li></ul><p>注：只有 A、B、C 由网络号和主机号之分，D、E 没有划分网络号和主机号</p><h2 id="特殊地址"><a href="#特殊地址" class="headerlink" title="特殊地址"></a>特殊地址</h2><ul><li>255.255.255.255</li></ul><p>该 IP 地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，首先广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的的地址的分组；一般广播地址既可以在本地广播，也可跨网段广播。例如：主机 192.168.1.1/30 直接广播数据包后，另一个网段 192.168.1.15/30 也能直接收到该数据包；若发送受限广播数据包则不能收到</p><p>注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址则不能通过路由器</p><ul><li><p>0.0.0.0<br>常用于寻找自己的 IP 地址，例如在 RARP，BOOTP 和 DHCP 协议中，若某个位置 IP 地址的无盘机想要知道自己的 IP 地址，他就以 255.255.255.255 为目的地址，像本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送 IP 请求分组</p></li><li><p>回环地址<br>127.0.0.0/8 被用作回环地址，会换地址表示本机的地址，常用于对本机的测试，用得最多的是 127.0.0.1</p></li><li><p>A、B、C 类私有地址<br>私有地址（Private Address）也叫专用地址，他们不会在全球使用，只具有本地意义</p></li></ul><ul><li>A 类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255</li><li>B 类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255</li><li>C 类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255</li></ul><h2 id="子网掩码及网络划分"><a href="#子网掩码及网络划分" class="headerlink" title="子网掩码及网络划分"></a>子网掩码及网络划分</h2><p>子网掩码是标志两个 IP 地址是否属于一个子网的，也是 32 位二进制地址，其每一个 1 代表该位是网络位，0 代表主机位。它和 IP 地址一样也是使用点式十进制来表示的。如果两个 IP 地址在子网掩码的按位与计算所得结果相同，即表明它们共属于同一个子网中。</p><p>注：在计算子网掩码时，要注意 IP 地址中的保留地址，即 “0” 地址和广播地址，他们是指主机地址或网络全为 “0” 或 “1” 时的 IP 地址，他们代表着本网络地址和广播地址，一般是不能被计算在内的。</p><ul><li>求子网掩码前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数</li></ul><p>（1）将子网数转化为二进制表示：如欲将 B 类 IP 地址 168.195.0.0 划分成 27 个子网：27=11011;</p><p>（2）取得该二进制的位数，为 N：该二进制为 5 位数，N=5</p><p>（3）取得该 IP 地址的类子网掩码，将其主机地址部分的前 N 位置 1 即得出该 IP 地址划分子网的子网掩码</p><p>该 B 类地址的子网掩码 255.255.0.0 的主机地址前 5 位置 1，得到 255.255.248.0.</p><ul><li>利用主机数来计算<br>如欲将 B 类 IP 地址 168.195.0.0 划分成若干子网，每个子网内由主机 700 台：</li></ul><p>（1）将主机数转化为二进制表示：700=1010111100;</p><p>（2）如果主机数小于或等于 254，则取得该主机的二进制位数，为 N。如果大于 254，也就是说主机地址将占据不止 8 位，该二进制为十位数，N=10</p><p>（3）使用 255.255.255.255 来将该类 IP 地址的主机地址位数全部置 1，然后从后向前将 N 位全部置 0，即为子网掩码值。将该 B 类地址的子网掩码 255.255.0.0 的主机地址全部置 1，得到 255.255.255.255，然后再从后向前将十位置 0，即为：11111111.11111111.11111100.00000000，即 255.255.252.0. 这就是将划分成主机为 700 台 B 类 IP 地址 168.195.0.0 的子网掩码。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python复习笔记</title>
      <link href="/2020/07/23/python/shypython/"/>
      <url>/2020/07/23/python/shypython/</url>
      
        <content type="html"><![CDATA[<h1 id="Shypython-learn-notes"><a href="#Shypython-learn-notes" class="headerlink" title="Shypython-learn-notes"></a>Shypython-learn-notes</h1><h2 id="1-python-数据类型"><a href="#1-python-数据类型" class="headerlink" title="1. python 数据类型"></a>1. python 数据类型</h2><h3 id="1-1-变量"><a href="#1-1-变量" class="headerlink" title="1.1 变量"></a>1.1 变量</h3><p><strong>1.1.1 算术运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 加减乘除+、-、*、/</span><br><span class="line">- 取余、取整、取绝对值 %、//、abs()</span><br><span class="line">- 最小、最大值 min()、max()</span><br><span class="line">- 复数 complex(re,im)</span><br><span class="line">- 取共轭 c.conjugate()</span><br><span class="line">- 返回商和余数 divmod(x,y)</span><br></pre></td></tr></table></figure></p><p><strong>1.1.2 布尔运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-  小于、大于 &lt; 、 &gt;</span><br><span class="line">- 等于、不等于 == 、 != </span><br><span class="line">- 与、或、非 <span class="keyword">and</span> 、<span class="keyword">or</span> 、<span class="keyword">not</span></span><br></pre></td></tr></table></figure><br><strong>1.1.3 赋值运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- a=a+b  <span class="keyword">is</span>  a+=b</span><br><span class="line">- a=a-b  <span class="keyword">is</span>  a-=b</span><br><span class="line">- a=a*/b  <span class="keyword">is</span>  a*/b</span><br><span class="line">- a=a**(//)b  <span class="keyword">is</span>  a**(//)=b</span><br></pre></td></tr></table></figure></p><p><strong>1.1.4 位运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 与或 &amp; 、 |  </span><br><span class="line">- 异或、取反 ^ 、~ </span><br><span class="line">- 左位移、右位移  &lt;&lt;   、 &gt;&gt;</span><br></pre></td></tr></table></figure></p><p><strong>1.1.5 转义符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">- 续行符 \</span><br><span class="line">- 反斜杠符号 \\</span><br><span class="line">- 引号 \<span class="string">'</span></span><br><span class="line"><span class="string">- 响铃 \a</span></span><br><span class="line"><span class="string">- 退格 \b</span></span><br><span class="line"><span class="string">- 转义 \e</span></span><br><span class="line"><span class="string">- 空 \000</span></span><br><span class="line"><span class="string">- 换行 \n</span></span><br><span class="line"><span class="string">- 纵向制表符 \v</span></span><br><span class="line"><span class="string">- 横向制表符 \t</span></span><br><span class="line"><span class="string">- 回车 \r</span></span><br><span class="line"><span class="string">- 换页 \f</span></span><br><span class="line"><span class="string">- 八进制 \oyy</span></span><br><span class="line"><span class="string">- 十六进制 \xyy</span></span><br></pre></td></tr></table></figure></p><h3 id="1-2-字符串-不可变类型"><a href="#1-2-字符串-不可变类型" class="headerlink" title="1.2 字符串(不可变类型)"></a>1.2 字符串(不可变类型)</h3><p><strong>1.2.1 切片操作</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当索引为正数时，从0开始，当索引为负数时，从-1开始（从右往左）</span></span><br><span class="line">- newstr = s[a:b:c]  从索引a开始到b ,每隔c取一个值，左开右闭</span><br><span class="line">- newstr = s[<span class="number">0</span>:]  </span><br><span class="line">- newstr = s[:]  和上面的式子等价</span><br><span class="line">- newstr = s[::<span class="number">-1</span>] 实现字符串的逆序</span><br></pre></td></tr></table></figure><br><strong>1.2.2 字符串运算及方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">'I LOVE PYTHON!!'</span></span><br><span class="line">- 标准化字符串 <span class="string">r'str'</span> 或者 repr(str)</span><br><span class="line">- x <span class="keyword">in</span> s 子字符串 </span><br><span class="line">- s1 + s2 字符串连接 </span><br><span class="line">- s*n 字符串副本的拼接 </span><br><span class="line">- s[i] 字符串索引 </span><br><span class="line">- str.index(<span class="string">'s'</span>) 获得字符串s字符的索引位置</span><br><span class="line">- len(s) 字符串长度 </span><br><span class="line">- ord(str) 字符串的编码 </span><br><span class="line">- chr(number) 返回某个编码得到的字符 </span><br><span class="line">- str.spilt(<span class="string">', '</span>) 字符串的分割 ,返回值是一个列表</span><br><span class="line">- chr.join(list)  字符串编码的连接 , <span class="string">" "</span>.join(list)</span><br><span class="line">str = <span class="string">"www.runoob.com"</span></span><br><span class="line">- print(str.upper())  把所有字符中的小写字母转换成大写字母</span><br><span class="line">- print(str.lower())  把所有字符中的大写字母转换成小写字母</span><br><span class="line">- print(str.capitalize()) 把第一个字母转化为大写字母，其余小写</span><br><span class="line">- print(str.title())  把每个单词的第一个字母转化为大写，其余小写</span><br></pre></td></tr></table></figure><br><strong>1.2.3深入研究字符串的方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str.find(x)  返回x的第一个字符出现的索引位置</span><br><span class="line">str.count(x)  返回x出现的次数</span><br><span class="line">str.replace(<span class="string">'top'</span>,<span class="string">'bot'</span>)  返回一个修改的副本</span><br><span class="line">str.spilt()</span><br><span class="line">tabel = str.maketrans(<span class="string">'xyz'</span>,<span class="string">'uvw'</span>) ; str.translate(table)  返回一个映射后的副本</span><br><span class="line">str.strip()  返回字符串的一个副本，并且消除前后空格</span><br></pre></td></tr></table></figure></p><h3 id="1-3-列表（可变类型）"><a href="#1-3-列表（可变类型）" class="headerlink" title="1.3 列表（可变类型）"></a>1.3 列表（可变类型）</h3><p><strong>1.3.1 list的内置方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">1</span>,<span class="number">3</span>,a,[<span class="number">4</span>,<span class="number">5</span>],<span class="number">6</span>]</span><br><span class="line">- list.append(x)  在尾部增加一个元素</span><br><span class="line">- list.insert(x,i)  在索引i处添加一个元素</span><br><span class="line">- list.index(x)  获得元素x的索引</span><br><span class="line">- list.remove(x)  删除列表的原色</span><br><span class="line">- list.pop(i)  弹出索引为i的元素并在列表中删除它</span><br><span class="line">- list.clear() 清楚列表</span><br><span class="line">- list.count(x) 返回列表x出现的次数</span><br><span class="line">- list.sort() 对列表直接排序， 区别于排序函数 sorted()</span><br><span class="line">- list.reverse() 对列表进行反转</span><br><span class="line">- len(list)</span><br><span class="line">- <span class="keyword">for</span> item <span class="keyword">in</span> list:   对列表的遍历</span><br></pre></td></tr></table></figure><br> <strong>1.3.2 列表和字符串的相互转化</strong><br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1.</span> str &gt;&gt;&gt;list </span><br><span class="line"></span><br><span class="line">str1 = <span class="string">"12345"</span></span><br><span class="line">list1 = list(str1)</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"> </span><br><span class="line">str2 = <span class="string">"123 sjhid dhi"</span></span><br><span class="line">list2 = str2.split() <span class="comment">#or list2 = str2.split(" ")</span></span><br><span class="line"><span class="keyword">print</span> list2</span><br><span class="line"> </span><br><span class="line">str3 = <span class="string">"www.google.com"</span></span><br><span class="line">list3 = str3.split(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">print</span> list3</span><br><span class="line"> </span><br><span class="line">输出为：</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>, <span class="string">'5'</span>]</span><br><span class="line">[<span class="string">'123'</span>, <span class="string">'sjhid'</span>, <span class="string">'dhi'</span>]</span><br><span class="line">[<span class="string">'www'</span>, <span class="string">'google'</span>, <span class="string">'com'</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> list &gt;&gt;&gt;str</span><br><span class="line">str4 = <span class="string">""</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str4</span><br><span class="line">str5 = <span class="string">"."</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str5</span><br><span class="line">str6 = <span class="string">" "</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str6</span><br><span class="line">输出为：</span><br><span class="line">wwwgooglecom</span><br><span class="line">www.google.com</span><br><span class="line">www google com</span><br></pre></td></tr></table></figure></p><h3 id="1-3-元组类型（不可变类型）"><a href="#1-3-元组类型（不可变类型）" class="headerlink" title="1.3 元组类型（不可变类型）"></a>1.3 元组类型（不可变类型）</h3><p><strong>1.3.1 元组的运算及操作</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 元组为不可修改的字符串</span></span><br><span class="line">tuple = (<span class="number">2019</span>,<span class="string">'a'</span>,(b,c),<span class="string">'science'</span>)</span><br></pre></td></tr></table></figure><br><strong>1.3.2 元组与列表的转换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple = tuple(list)</span><br><span class="line">list  = list(touple)</span><br></pre></td></tr></table></figure></p><h3 id="1-4-集合类型（消除关系重复元素）"><a href="#1-4-集合类型（消除关系重复元素）" class="headerlink" title="1.4 集合类型（消除关系重复元素）"></a>1.4 集合类型（消除关系重复元素）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">myset = [<span class="string">'nature'</span>,<span class="string">'science'</span>]</span><br><span class="line">set.add(x)</span><br><span class="line">set.remove(X)</span><br><span class="line">set.discard(X)</span><br><span class="line">set.clear()</span><br><span class="line">set.pop()</span><br><span class="line">len(set)</span><br><span class="line"><span class="keyword">in</span> / <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">set.issubset(set2)  判断set是否是set2的子集，返回bool类型</span><br><span class="line">set.isuperset(set2)  </span><br><span class="line">set.union(set2)  计算并集</span><br><span class="line">set.intersection(set2)  计算交集</span><br><span class="line">set.difference(set2)  计算差集</span><br><span class="line">set.symmetric_difference(set2)  计算对称差集</span><br></pre></td></tr></table></figure><h3 id="1-5-字典类型（键值对）"><a href="#1-5-字典类型（键值对）" class="headerlink" title="1.5 字典类型（键值对）"></a>1.5 字典类型（键值对）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">'a'</span>:<span class="number">1</span>,<span class="string">'b'</span>:<span class="number">2</span>,<span class="string">'c'</span>:<span class="number">3</span>&#125;</span><br><span class="line">len(dict)</span><br><span class="line">str(dict)</span><br><span class="line">dict(<span class="string">'a'</span>)  访问字典中键为a的值</span><br><span class="line">dict.clear()</span><br><span class="line">dict.items() 以列表形式返回可遍历的（键，值）元素数组</span><br><span class="line">dict.keys() 以列表形式返回一个字典中的所有键</span><br><span class="line">dict.values() 以字典形式返回一个字典中的所有值</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">## 2. 语句类型</span></span><br><span class="line"><span class="comment">### 2.1 if 语句</span></span><br><span class="line">```python </span><br><span class="line"><span class="keyword">if</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">elif</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="2-2-while语句"><a href="#2-2-while语句" class="headerlink" title="2.2 while语句"></a>2.2 while语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 死循环</span></span><br><span class="line"><span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="2-3-for-语句"><a href="#2-3-for-语句" class="headerlink" title="2.3 for 语句"></a>2.3 for 语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在for循环中使用内置函数 range()</span></span><br><span class="line">range(a,b,c)  返回一个数字区间的所有整数</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的冒泡排序算法S</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(n)<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(n)-i<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> n[j] &gt; n[i]:</span><br><span class="line">            n[j], n[j+<span class="number">1</span>] = n[j+<span class="number">1</span>], n[j]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在for循环中使用内置函数zip()</span></span><br><span class="line">zip(x,y)  将多序列生成一个新的序列，每个序列的元素以元组形式存储数据</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t1,t2 <span class="keyword">in</span> zip(x,y):</span><br><span class="line">    print(t1,t2)</span><br></pre></td></tr></table></figure><h3 id="2-4-控制语句"><a href="#2-4-控制语句" class="headerlink" title="2.4 控制语句"></a>2.4 控制语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">break</span>  跳出循环</span><br><span class="line"><span class="keyword">continue</span>  终止当前一次循环、继续进行下一次循环</span><br><span class="line"><span class="keyword">pass</span> 什么都不做，保持结构完整性</span><br></pre></td></tr></table></figure><h2 id="3-格式化输入与输出"><a href="#3-格式化输入与输出" class="headerlink" title="3. 格式化输入与输出"></a>3. 格式化输入与输出</h2><h3 id="3-1-格式化输入"><a href="#3-1-格式化输入" class="headerlink" title="3.1 格式化输入"></a>3.1 格式化输入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = input(&lt;info&gt;)</span><br><span class="line">a = input(repr(str))</span><br></pre></td></tr></table></figure><h3 id="3-2-格式化输出"><a href="#3-2-格式化输出" class="headerlink" title="3.2 格式化输出"></a>3.2 格式化输出</h3><p><strong>3.2.1 print语句</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(a, b)   同行以空格隔开输出</span><br><span class="line">print(a, b,s ep=<span class="string">','</span>)  以逗号隔开进行输出</span><br><span class="line">print(s, ewp=<span class="string">'\n'</span>)   以换行隔开进行输出</span><br><span class="line">print(name, end=<span class="string">'!'</span>)  每个输出都要添加!</span><br><span class="line">print(<span class="string">' i love %s '</span> % s)  <span class="keyword">print</span> 格式化输出</span><br><span class="line"><span class="comment"># 对字符串的每个元素进行换行输出</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list:</span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><br><strong>3.2.2 字符串方法format()</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'&#123;0&#125; : &#123;1&#125; : &#123;2&#125;'</span>.format(hour, minute, second))</span><br><span class="line"><span class="comment"># 按照对齐格式化进行排列</span></span><br><span class="line">print(<span class="string">'&#123;0:3&#125;,&#123;1:5&#125;'</span>.format(<span class="number">12</span>, <span class="number">534</span>)) :后面的内容为只等的格式，表示占位数，如果不够，在前面用空格补齐</span><br></pre></td></tr></table></figure><br><strong>3.2.3 数据输出的格式类型</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a表示占位，不够，按照原长度打印，多了左侧空格补齐，小数点后面为保留几位小数，f位数据类型</span></span><br><span class="line">print(<span class="string">'i love %a.bf'</span>, num)</span><br><span class="line">- b 以二进制形式输出</span><br><span class="line">- c 输出证书值对应的unicode字符</span><br><span class="line">- d 以十进制形式输出数值</span><br><span class="line">- e 以科学计数法形式输出</span><br><span class="line">- o 以八进制形式输出</span><br><span class="line">- x 以小写形式的十六进制输出</span><br><span class="line">- X 以大写形式的十六进制输出</span><br></pre></td></tr></table></figure></p><h2 id="4-函数"><a href="#4-函数" class="headerlink" title="4. 函数"></a>4. 函数</h2><h3 id="4-1-函数定义及调用函数"><a href="#4-1-函数定义及调用函数" class="headerlink" title="4.1 函数定义及调用函数"></a>4.1 函数定义及调用函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- <span class="function"><span class="keyword">def</span> <span class="title">funname</span><span class="params">(para1, para2,*，para3...)</span>:</span></span><br><span class="line">    函数体</span><br><span class="line"><span class="comment"># 在参数列表中使用（*），代表调用函数时，在（*）后面的参数都必须指定参数名称，如下</span></span><br><span class="line">funname(para1, para2，para3=<span class="number">2.</span>..)   调用函数</span><br><span class="line">- <span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(strname, age=<span class="number">32</span>)</span>：   后面的参数位默认形参，默认形参必须放在后面</span></span><br><span class="line"><span class="function">- 对元组和列表进行解包</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">fun</span><span class="params">(*person)</span>:</span></span><br><span class="line">fun(<span class="string">'shy'</span>,<span class="string">'21'</span>)</span><br><span class="line">mylist = [<span class="string">'shy'</span>,<span class="string">'21'</span>]</span><br><span class="line">fun(*mylist)</span><br><span class="line">- 对字典进行解包定义参数及调用</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(**person)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'姓名'</span>,person[<span class="string">'name'</span>],<span class="string">'年纪'</span>,person[<span class="string">'age'</span>])</span></span></span><br><span class="line"><span class="function"><span class="title">fun</span><span class="params">(<span class="string">'shy'</span>,<span class="string">'21'</span>)</span></span></span><br><span class="line">mydict = &#123;'name':'shy','age':21&#125;</span><br><span class="line">fun(**mydict)</span><br></pre></td></tr></table></figure><h3 id="4-2-函数类型"><a href="#4-2-函数类型" class="headerlink" title="4.2 函数类型"></a>4.2 函数类型</h3><p><strong>4.2.1 python内置函数</strong></p><p>下图python3.8官方文档给出的内置函数库：</p><div style="align: center"><img src="https://img.vim-cn.com/5f/bd7267b5d93389d33733905d3bb7216a43d7c1.png"/></div><p>官方中文文档的连接：<a href="https://docs.python.org/zh-cn/3/library/functions.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/functions.html</a></p><p><strong>4.2.2 匿名函数与可迭代函数</strong></p><ul><li><strong>匿名函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匿名函数</span></span><br><span class="line"><span class="keyword">lambda</span> para1, para2... : 表达式</span><br><span class="line">r = <span class="keyword">lambda</span> x,y:x*y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与reduce函数的组合应用</span></span><br><span class="line">reduce(fun, seq, initial)  <span class="comment">#用序列值依次调用fun</span></span><br><span class="line"><span class="keyword">from</span> funtools <span class="keyword">import</span> reduce</span><br><span class="line">a = reduce(<span class="keyword">lambda</span> x,y:x + y, range(<span class="number">1</span>,<span class="number">101</span>))  <span class="comment">#实现求1~100的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与map函数的组合应用</span></span><br><span class="line">map(fun, seq[,seq,])  <span class="comment">#将seq内部的元素作为参数依次调用</span></span><br><span class="line">t = map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])  <span class="comment">#返回一个map对象</span></span><br><span class="line">print(list(t))   <span class="comment">#打印值为[1,4,9,16,25]</span></span><br><span class="line">y =map(<span class="keyword">lambda</span> x,y:x+y,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(list(t))   <span class="comment"># 打印值为[5,7,9]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与filter函数的组合应用</span></span><br><span class="line">filter(fun <span class="keyword">or</span> none, seq)  <span class="comment">#将序列对象依次放到fun中，如果返回true就留下</span></span><br><span class="line">t = filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(list(t))   <span class="comment"># 打印值为[2,4,6]</span></span><br></pre></td></tr></table></figure></li><li><strong>可迭代函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个生成器对象会有一个__next__()方法</span></span><br><span class="line">t = filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(t.__next__())  <span class="comment"># 打印2</span></span><br><span class="line">print(t.__next__())  <span class="comment"># 打印4</span></span><br></pre></td></tr></table></figure><strong>4.2.3 生成器函数与工厂函数</strong></li><li><strong>生成器函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成器与迭代器不同，迭代器的内容存在内存里，用next函数遍历，生成器用完立即销毁</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Reverse</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(data)<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">yield</span> data[idx]   <span class="comment"># 生成器函数用yield返回</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> Reverse(<span class="string">'Python'</span>):</span><br><span class="line">    print(c, end = <span class="string">' '</span>)  <span class="comment"># 打印 n o h t y P</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器表达式</span></span><br><span class="line">mylist = [x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]  <span class="comment"># 使用生成器表达式返回一个对象</span></span><br></pre></td></tr></table></figure></li><li><strong>工厂函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 闭合函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">fun = wrapperfun('shy')</span><br><span class="line">fun(<span class="number">37</span>)    <span class="comment"># 打印 shy 37</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 装饰器属性：在原有的函数包一个函数，不改变原代码的基础上，添加新功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span>    <span class="comment"># 装饰器函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(strname)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(strname.(str))：</span><br><span class="line">            <span class="keyword">return</span> fn(strname)  <span class="comment"># 判断字符串类型</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'error'</span>)  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line">wrapperfun2 = checkParams(wrapperfun)</span><br><span class="line">fun = wrapperfun(<span class="string">'shy'</span>)</span><br><span class="line">fun(<span class="number">37</span>)  <span class="comment"># 打印 shy 37</span></span><br><span class="line">fun = wrapperfun2(<span class="number">37</span>)  <span class="comment"># 输入不合法</span></span><br></pre></td></tr></table></figure></li><li><strong>@修饰符</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span>    <span class="comment"># 装饰器函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(strname)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(strname.(str))：</span><br><span class="line">            <span class="keyword">return</span> fn(strname)  <span class="comment"># 判断字符串类型</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'error'</span>)  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@checkParams   # 使用装饰器函数对wrapperfun函数进行修饰</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">fun = wrapperfun('shy')</span><br><span class="line">fun(<span class="number">37</span>)  <span class="comment"># 打印 shy 37</span></span><br><span class="line">fun = wrapperfun2(<span class="number">37</span>)  <span class="comment"># 输入不合法</span></span><br></pre></td></tr></table></figure><strong>4.4.4 偏函数与递归函数</strong></li><li><strong>偏函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 偏函数是重新定义一个函数，并指定了默认参数值</span></span><br><span class="line"><span class="keyword">from</span> funtools <span class="keyword">import</span> partial</span><br><span class="line">partial(fun, *args, **keywords)</span><br></pre></td></tr></table></figure></li><li><strong>递归函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归函数是自己调用自己的函数，所有的函数调用都是压栈的过程，所以耗内存，栈空间有限，如果程序栈空间地址写满，程序最后会崩溃</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> n*fun(n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><strong>4.4.5 eval 和 exec函数</strong></li><li><strong>eval函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># exec执行不返回结果，eval执行返回结果</span></span><br><span class="line">dic = &#123;&#125;</span><br><span class="line">dic[<span class="string">'b'</span>] = <span class="number">3</span></span><br><span class="line">exec(<span class="string">'a=4'</span>,dic)</span><br><span class="line">print(dic.keys())   <span class="comment">#打印dict_keys(['a','__builtins__','b'])</span></span><br><span class="line"><span class="comment"># 使用这两个函数第一个参数一定是可执行代码</span></span><br></pre></td></tr></table></figure><h3 id="4-3-变量的作用域"><a href="#4-3-变量的作用域" class="headerlink" title="4.3 变量的作用域"></a>4.3 变量的作用域</h3><strong>4.3.1 global语句</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global 可以把局部声明为全局</span></span><br><span class="line">a = <span class="number">6</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a</span><br><span class="line">    a = <span class="number">5</span></span><br><span class="line">print(a)   <span class="comment"># 打印5</span></span><br></pre></td></tr></table></figure><strong>4.3.2 nonlocal语句</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nonlocal可以把全局往下一作用域调用</span></span><br><span class="line">a = <span class="number">6</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">7</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nested</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">nonlocal</span> a</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line">    nested()</span><br><span class="line">    print(<span class="string">'本地:'</span>,a)   <span class="comment">#打印 8</span></span><br><span class="line">func()</span><br><span class="line">print(<span class="string">'全局'</span>,a)  <span class="comment"># 打印6</span></span><br><span class="line">print(a)   <span class="comment"># 打印5</span></span><br></pre></td></tr></table></figure><h2 id="5-面向对象的程序设计"><a href="#5-面向对象的程序设计" class="headerlink" title="5. 面向对象的程序设计"></a>5. 面向对象的程序设计</h2><h3 id="5-1-类的结构"><a href="#5-1-类的结构" class="headerlink" title="5.1 类的结构"></a>5.1 类的结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>：</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">__init__</span><span class="params">(self,属性)</span>:</span>  <span class="comment"># 构造函数</span></span><br><span class="line">        self.属性 = 属性</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getname</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">myc = MyClass(属性)   <span class="comment"># 初始化实例对象，构造函数的属性</span></span><br><span class="line">a = myc.getname()   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 类还具有一些内置属性</span></span><br><span class="line">__name__   名称</span><br><span class="line">__doc__  类的文档字符串</span><br><span class="line">__nodule__ 类的模块</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure><h3 id="5-2-类方法"><a href="#5-2-类方法" class="headerlink" title="5.2 类方法"></a>5.2 类方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmthod  # 声明为类方法，可以直接用类名进行调用，当然初始化实例对象进行调用也是正确的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(cls)</span>:</span></span><br><span class="line"><span class="meta">@staticmethod  # 等同于普通函数，只是被封装在类中，独立于整个类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>  # 同上的调用方式，且参数没有限制要求</span></span><br></pre></td></tr></table></figure><h3 id="5-3-类的私有化属性"><a href="#5-3-类的私有化属性" class="headerlink" title="5.3 类的私有化属性"></a>5.3 类的私有化属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 私有化属性方法，在类的属性前面加双下划线，同时会提供一个私有化属性的访问函数，不可以被修改,但可以针对具体实例进行对象修改</span></span><br><span class="line">clas MyClass:</span><br><span class="line">    __Occupation = <span class="string">'scientist'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(sefl,name,age)</span>；</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"><span class="comment"># 使用装饰器函数实现类的私有化</span></span><br><span class="line">clas MyClass:</span><br><span class="line">    __Occupation = <span class="string">'scientist'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(sefl,name,age)</span>；</span></span><br><span class="line"><span class="function">    @<span class="title">property</span>  # 装饰为属性，使类的私有化属性也可以被访问</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br></pre></td></tr></table></figure><h3 id="5-4-类的继承"><a href="#5-4-类的继承" class="headerlink" title="5.4 类的继承"></a>5.4 类的继承</h3><strong>5.4.1 继承结构体</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DerivedClass</span><span class="params">(FatherClass1, FatherClass2)</span>：</span></span><br><span class="line"><span class="class">    <span class="title">pass</span></span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Record</span>:</span></span><br><span class="line">    <span class="string">""" A record class """</span></span><br><span class="line">    __Ocuupation = <span class="string">"scientist"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"Occupation:"</span>,self.getOccupation())</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GirlRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        Record.showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line"></span><br><span class="line">myc = GirlRecord(<span class="string">"Anaa"</span>, <span class="number">21</span>)</span><br><span class="line">myc.showrecode()</span><br></pre></td></tr></table></figure><strong>5.4.2 super()函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保障了调用父类方法时只调用一次</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Record</span>:</span></span><br><span class="line">    <span class="string">""" A record class """</span></span><br><span class="line">    __Ocuupation = <span class="string">"scientist"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"Occupation:"</span>,self.getOccupation())</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GirlRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaleRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThisRecord</span><span class="params">(GirlRecord, MaleRecord)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line">myc = ThisRecord(<span class="string">"Anaa"</span>, <span class="number">21</span>)</span><br><span class="line">myc.showrecode()</span><br></pre></td></tr></table></figure><h3 id="5-5-类相关的内置函数"><a href="#5-5-类相关的内置函数" class="headerlink" title="5.5 类相关的内置函数"></a>5.5 类相关的内置函数</h3></li><li><strong>判断实例（isinstance)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">isinstance(object, class_name)</span><br></pre></td></tr></table></figure></li><li><strong>判断字类（issubclass)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">issubclass(class1, class2)</span><br></pre></td></tr></table></figure></li><li><strong>判断类实例中是否包含某一个属性（hasattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hasattr(object, name)</span><br></pre></td></tr></table></figure></li><li><strong>获得类实例中的某一个属性（getattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getattr(object, name[,default])</span><br></pre></td></tr></table></figure></li><li><strong>设置类实例中的某一个属性值（setattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setattr(object, name, value)</span><br></pre></td></tr></table></figure><h3 id="5-6-重载运算符"><a href="#5-6-重载运算符" class="headerlink" title="5.6 重载运算符"></a>5.6 重载运算符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="string">""" A record class"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span>  <span class="comment"># 将值转化为字符串进行输出</span></span><br><span class="line">        retrun <span class="string">"name:"</span>+self.name;<span class="string">"age:"</span>+str(self.age)</span><br><span class="line">    </span><br><span class="line">    __repr__ = __str__  <span class="comment"># 转化为解释器读取的形式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, record)</span>:</span>  <span class="comment">#重载比较运算符</span></span><br><span class="line">        <span class="keyword">if</span> self.age &lt; record.age:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, record)</span>:</span>   <span class="comment">#重载加号运算符</span></span><br><span class="line">        <span class="keyword">return</span> MyClass(self.name, self.age+record.age)</span><br><span class="line"></span><br><span class="line">myc = MyClass(<span class="string">"A"</span>, <span class="number">42</span>)</span><br><span class="line">myc1 = MyClass(<span class="string">"B"</span>, <span class="number">23</span>)</span><br><span class="line"></span><br><span class="line">print(repr(myc))  </span><br><span class="line">print(myc)</span><br><span class="line">print(str(myc))</span><br><span class="line">print(myc&lt;myc1)</span><br><span class="line">print(myc+myc1)</span><br></pre></td></tr></table></figure></li><li><strong>运算符重载</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"> 方法名                  运算符和表达式      说明</span><br><span class="line">__add__(self,rhs)        self + rhs        加法</span><br><span class="line">__sub__(self,rhs)        self - rhs         减法</span><br><span class="line">__mul__(self,rhs)        self * rhs         乘法</span><br><span class="line">__truediv__(self,rhs)    self / rhs          除法</span><br><span class="line">__floordiv__(self,rhs)   self //rhs          地板除</span><br><span class="line">__mod__(self,rhs)        self % rhs       取模(求余)</span><br><span class="line">__pow__(self,rhs)        self **rhs         幂运算</span><br><span class="line">合赋值算术运算符的重载:</span><br><span class="line">方法名                  运算符和表达式      说明</span><br><span class="line">__iadd__(self,rhs)       self += rhs        加法</span><br><span class="line">__isub__(self,rhs)       self -= rhs         减法</span><br><span class="line">__imul__(self,rhs)       self *= rhs         乘法</span><br><span class="line">__itruediv__(self,rhs)   self /= rhs        除法</span><br><span class="line">__ifloordiv__(self,rhs)  self //=rhs        地板除</span><br><span class="line">__imod__(self,rhs)       self %= rhs     取模(求余)</span><br><span class="line">__ipow__(self,rhs)       self **=rhs       幂运算</span><br><span class="line"></span><br><span class="line">比较算术运算符的重载:</span><br><span class="line">方法名                  运算符和表达式      说明</span><br><span class="line">__lt__(self,rhs)       self &lt; rhs        小于</span><br><span class="line">__le__(self,rhs)       self &lt;= rhs       小于等于</span><br><span class="line">__gt__(self,rhs)       self &gt; rhs        大于</span><br><span class="line">__ge__(self,rhs)       self &gt;= rhs       大于等于</span><br><span class="line">__eq__(self,rhs)       self == rhs       等于</span><br><span class="line">__ne__(self,rhs)       self != rhs       不等于</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">位运算符重载</span><br><span class="line">方法名              运算符和表达式        说明</span><br><span class="line">__and__(self,rhs)       self &amp; rhs           位与</span><br><span class="line">__or__(self,rhs)        self | rhs           位或</span><br><span class="line">__xor__(self,rhs)       self ^ rhs           位异或</span><br><span class="line"> __lshift__(self,rhs)    self &lt;&lt;rhs           左移</span><br><span class="line"> __rshift__(self,rhs)    self &gt;&gt;rhs           右移</span><br><span class="line"></span><br><span class="line">反向位运算符重载</span><br><span class="line"></span><br><span class="line">方法名            运算符和表达式       说明</span><br><span class="line">__and__(self,lhs)       lhs &amp; rhs        位与</span><br><span class="line">__or__(self,lhs)         lhs | rhs       位或</span><br><span class="line">__xor__(self,lhs)       lhs ^ rhs        位异或</span><br><span class="line">__lshift__(self,lhs)    lhs &lt;&lt;rhs        左移</span><br><span class="line">__rshift__(self,lhs)    lhs &gt;&gt;rhs        右移</span><br><span class="line"></span><br><span class="line">复合赋值位相关运算符重载</span><br><span class="line">方法名              运算符和表达式          说明</span><br><span class="line">__iand__(self,rhs)       self &amp; rhs       位与</span><br><span class="line">__ior__(self,rhs)        self | rhs       位或</span><br><span class="line">__ixor__(self,rhs)       self ^ rhs       位异或</span><br><span class="line">__ilshift__(self,rhs)    self &lt;&lt;rhs       左移</span><br><span class="line">__irshift__(self,rhs)    self &gt;&gt;rhs       右移</span><br><span class="line"></span><br><span class="line">一元运算符的重载</span><br><span class="line">方法名              运算符和表达式       说明</span><br><span class="line">__neg__(self)         - self           负号</span><br><span class="line">__pos__(self)         + self           正号</span><br><span class="line">__invert__(self)      ~ self           取反</span><br></pre></td></tr></table></figure><h2 id="6-错误异常与文件读写"><a href="#6-错误异常与文件读写" class="headerlink" title="6 错误异常与文件读写"></a>6 错误异常与文件读写</h2><h3 id="6-1-错误异常捕捉"><a href="#6-1-错误异常捕捉" class="headerlink" title="6.1 错误异常捕捉"></a>6.1 错误异常捕捉</h3><strong>6.1.1 异常语句结构</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">except</span>(ZeroDivisionError, ValueError):</span><br><span class="line">    print(<span class="string">'错误'</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'其它异常’)</span></span><br><span class="line"><span class="string">except Exception as e:  # 捕捉未知异常</span></span><br><span class="line"><span class="string">    print(e)</span></span><br><span class="line"><span class="string">else:</span></span><br><span class="line"><span class="string">    pass   # 没有异常发生时执行</span></span><br><span class="line"><span class="string">finally:</span></span><br><span class="line"><span class="string">    pass  # 最终处理语句，无论是否有异常，都要执行这个语句</span></span><br></pre></td></tr></table></figure><strong>6.1.2 异常类型</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异常名称         异常解释</span></span><br><span class="line">AttributeError试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x</span><br><span class="line">IOError输入/输出异常；基本上是无法打开文件</span><br><span class="line">ImportError无法引入模块或包；基本上是路径问题或名称错误</span><br><span class="line">IndentationError语法错误（的子类） ；代码没有正确对齐</span><br><span class="line">IndexError下标索引超出序列边界，比如当x只有三个元素，却试图访问x[<span class="number">5</span>]</span><br><span class="line">KeyError试图访问字典里不存在的键</span><br><span class="line">KeyboardInterruptCtrl+C被按下</span><br><span class="line">NameError使用一个还未被赋予对象的变量</span><br><span class="line">SyntaxErrorPython代码非法，代码不能编译(个人认为这是语法错误，写错了）</span><br><span class="line">TypeError传入对象类型与要求的不符合</span><br><span class="line">UnboundLocalError</span><br><span class="line">试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它</span><br><span class="line">ValueError传入一个调用者不期望的值，即使值的类型是正确</span><br><span class="line">BaseException                        　　　　所有异常的基类</span><br><span class="line">SystemExit　　　　　　　　 　　　　解释器请求退出</span><br><span class="line">KeyboardInterrupt　　　　　 　　　　用户中断执行(通常是输入^C)</span><br><span class="line">Exception　　　　　　　　　　　　 常规错误的基类</span><br><span class="line">StopIteration 　　　　　　　　　　　　迭代器没有更多的值</span><br><span class="line">GeneratorExit 　　　　　　　　　　生成器(generator)发生异常来通知退出</span><br><span class="line">StandardError　　　　　　　　 　　所有的内建标准异常的基类</span><br><span class="line">ArithmeticError　　　　　　　　　　 所有数值计算错误的基类</span><br><span class="line">FloatingPointError 　　　　　　　　浮点计算错误</span><br><span class="line">OverflowError 　　　　　　　　　　数值运算超出最大限制</span><br><span class="line">ZeroDivisionError　　　　　　 　　除(或取模)零 (所有数据类型)</span><br><span class="line">AssertionError　　　　　　　　 　　断言语句失败</span><br><span class="line">AttributeError 　　　　　　　　　　对象没有这个属性</span><br><span class="line">EOFError 　　　　　　　　　　　　没有内建输入,到达EOF 标记</span><br><span class="line">EnvironmentError　　　　　　 　　操作系统错误的基类</span><br><span class="line">IOError　　　　　　　　　　　　 输入/输出操作失败</span><br><span class="line">OSError　　　　　　　　　　　　 操作系统错误</span><br><span class="line">WindowsError 　　　　　　　　　　系统调用失败</span><br><span class="line">ImportError　　　　　　　　　　 导入模块/对象失败</span><br><span class="line">LookupError　　　　　　　　　　 无效数据查询的基类</span><br><span class="line">IndexError 　　　　　　　　　　序列中没有此索引(index)</span><br><span class="line">KeyError　　　　　　　　　　　　 映射中没有这个键</span><br><span class="line">MemoryError　　　　　　　　　　 内存溢出错误(对于Python 解释器不是致命的)</span><br><span class="line">NameError 　　　　　　　　　　未声明/初始化对象 (没有属性)</span><br><span class="line">UnboundLocalError 　　　　　　　　访问未初始化的本地变量</span><br><span class="line">ReferenceError　　　　　　　　 弱引用(Weak reference)试图访问已经垃圾回收了的对象</span><br><span class="line">RuntimeError　　　　　　　　　　 一般的运行时错误</span><br><span class="line">NotImplementedError 　　　　　　　　尚未实现的方法</span><br><span class="line">SyntaxError Python　　　　　　　　 语法错误</span><br><span class="line">IndentationError　　　　　　　　　　 缩进错误</span><br><span class="line">TabError Tab　　　　　　　　　　 和空格混用</span><br><span class="line">SystemError 　　　　　　　　　　　　一般的解释器系统错误</span><br><span class="line">TypeError　　　　　　　　　　　　 对类型无效的操作</span><br><span class="line">ValueError 　　　　　　　　　　　　传入无效的参数</span><br><span class="line">UnicodeError Unicode　　　　　　　　 相关的错误</span><br><span class="line">UnicodeDecodeError Unicode　　　　 解码时的错误</span><br><span class="line">UnicodeEncodeError Unicode　　　　 编码时错误</span><br><span class="line">UnicodeTranslateError Unicode 　　　　转换时错误</span><br><span class="line">Warning　　　　　　　　　　　　　　 警告的基类</span><br><span class="line">DeprecationWarning　　　　　　　　 关于被弃用的特征的警告</span><br><span class="line">FutureWarning 　　　　　　　　　　　　关于构造将来语义会有改变的警告</span><br><span class="line">OverflowWarning　　　　　　　　　　　 旧的关于自动提升为长整型(long)的警告</span><br><span class="line">PendingDeprecationWarning　　　　　　 关于特性将会被废弃的警告</span><br><span class="line">RuntimeWarning 　　　　　　　　　　　可疑的运行时行为(runtime behavior)的警告</span><br><span class="line">SyntaxWarning　　　　　　　　　　 可疑的语法的警告</span><br><span class="line">UserWarning 　　　　　　　　　　用户代码生成的警告</span><br></pre></td></tr></table></figure><h3 id="6-2-文件读写与导入"><a href="#6-2-文件读写与导入" class="headerlink" title="6.2 文件读写与导入"></a>6.2 文件读写与导入</h3><strong>6.2.1 语句结构</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">f = open(<span class="string">'new/test.txt'</span>,<span class="string">'a+'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">f.write(<span class="string">'\n今天天气很好'</span>)</span><br><span class="line">f.close()</span><br><span class="line">f = open(<span class="string">'new/test.txt'</span>,<span class="string">'rb'</span>)</span><br><span class="line">w=f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(w)</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'workfile'</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'workfile'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    read_data = f.read()</span><br><span class="line">f.close()</span><br><span class="line">文件常见的读写模式</span><br><span class="line">w     以写方式打开，</span><br><span class="line">W     文件若存在，首先要清空，然后（重新）创建</span><br><span class="line">a     以追加模式打开 (从 EOF 开始, 必要时创建新文件)</span><br><span class="line">r+     以读写模式打开</span><br><span class="line">w+     以读写模式打开 (参见 w )</span><br><span class="line">a+     以读写模式打开 (参见 a )</span><br><span class="line">rb     以二进制读模式打开</span><br><span class="line">wb     以二进制写模式打开 (参见 w )</span><br><span class="line">ab     以二进制追加模式打开 (参见 a )</span><br><span class="line">rb+    以二进制读写模式打开 (参见 r+ )</span><br><span class="line">wb+    以二进制读写模式打开 (参见 w+ )</span><br><span class="line">ab+    以二进制读写模式打开 (参见 a+ )</span><br></pre></td></tr></table></figure><strong>6.2.1 文件读写方法</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">f.read()</span><br><span class="line">f.read(size)</span><br><span class="line">f.readline()  <span class="comment"># 读取一行</span></span><br><span class="line">f.readlines()  <span class="comment"># 读取所有行，每一行存储为列表的每一个元素</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    print(line, end=<span class="string">''</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">'This is a test\n'</span>)</span><br><span class="line"><span class="number">15</span></span><br><span class="line">f.tell() <span class="comment"># 返回一个整数，给出文件对象在文件中的当前位置，表示为二进制模式下时从文件开始的字节数，以及文本模式下的不透明数字。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 要改变文件对象的位置，请使用 f.seek(offset, whence)。 通过向一个参考点添加 offset 来计算位置；  </span></span><br><span class="line"><span class="comment"># 参考点由 whence 参数指定。  </span></span><br><span class="line"><span class="comment"># whence 的 0 值表示从文件开头起算，1 表示使用当前文件位置，2 表示使用文件末尾作为参考点。   </span></span><br><span class="line"><span class="comment"># whence 如果省略则默认值为 0，即使用文件开头作为参考点。</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = open(<span class="string">'workfile'</span>, <span class="string">'rb+'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">b'0123456789abcdef'</span>)</span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.seek(<span class="number">5</span>)      <span class="comment"># Go to the 6th byte in the file</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read(<span class="number">1</span>)</span><br><span class="line"><span class="string">b'5'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.seek(<span class="number">-3</span>, <span class="number">2</span>)  <span class="comment"># Go to the 3rd byte before the end</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read(<span class="number">1</span>)</span><br><span class="line"><span class="string">b'd'</span></span><br></pre></td></tr></table></figure><h3 id="6-3-常见文件类型的读取方式"><a href="#6-3-常见文件类型的读取方式" class="headerlink" title="6.3 常见文件类型的读取方式"></a>6.3 常见文件类型的读取方式</h3></li><li><strong>csv文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'C:/users/lenovo/desktop/student_score.csv'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines(): <span class="comment">#逐行读取</span></span><br><span class="line">        print(line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用panda读取数据，存为一个矩阵的形式</span></span><br><span class="line"><span class="keyword">import</span> panda <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(sys.argv[<span class="number">1</span>])</span><br></pre></td></tr></table></figure></li><li><strong>txt文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'my_file.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:      <span class="comment">#逐行读取</span></span><br><span class="line">print(line.strip())  <span class="comment">#使用strip删除空格和空行，否则会有\n在最后</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用numpy读取数据,转化一个数组</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">dataset = np.loadtxt(<span class="string">'路径'</span>)</span><br><span class="line">dataset.shape( )  <span class="comment"># 查看数组维度</span></span><br><span class="line">newset = reshape(dataset, (<span class="number">100</span>,<span class="number">3</span>))  <span class="comment"># 转化为100行 3列的数组</span></span><br></pre></td></tr></table></figure></li><li><strong>excle文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xlrd   <span class="comment">#使用库函数</span></span><br><span class="line"></span><br><span class="line">workbook = xlrd.open_workbook(<span class="string">'C:/users/lenovo/desktop/student_score.xlsx'</span>)  <span class="comment">#读取路径</span></span><br><span class="line">sheet = workbook.sheet_by_name(<span class="string">'Sheet1'</span>)     <span class="comment">#读取excel中的第一个sheet</span></span><br><span class="line"></span><br><span class="line">data_name = sheet.col_values(<span class="number">0</span>)    <span class="comment">#按列读取，读取第一列</span></span><br><span class="line"><span class="comment">#data_name1 = sheet.row_values(0)  #按行读取，读取第一行</span></span><br><span class="line">data_st_ID = sheet.col_values(<span class="number">1</span>)</span><br><span class="line">data_st_score = sheet.col_values(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用panda读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_df = pd.read_excel(<span class="string">r'G:\test.xlsx'</span>)</span><br></pre></td></tr></table></figure></li><li><strong>mat文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line">os.chdir(<span class="string">r'F:/data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line">data = scio.loadmat(<span class="string">'97.mat'</span>)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">de = data[<span class="string">'X097_DE_time'</span>]</span><br><span class="line"></span><br><span class="line">读取的结果是一个字典</span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line"><span class="keyword">from</span> mat4py <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = np.array(loadmat(<span class="string">'test.mat'</span>)[<span class="string">'dictKey'</span>]).astype(<span class="string">'float'</span>)</span><br></pre></td></tr></table></figure><h3 id="6-3-使用json保存结构化数据"><a href="#6-3-使用json保存结构化数据" class="headerlink" title="6.3 使用json保存结构化数据"></a>6.3 使用json保存结构化数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>json.dumps([<span class="number">1</span>, <span class="string">'simple'</span>, <span class="string">'list'</span>])</span><br><span class="line"><span class="string">'[1, "simple", "list"]'</span></span><br></pre></td></tr></table></figure><strong>6.3.1 将数据保存为json文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model=&#123;&#125; <span class="comment">#数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./hmm.json"</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json.dump(model,json_file,ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><strong>6.3.2 从json文件读取数据</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model=&#123;&#125; <span class="comment">#存放读取的数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./hmm.json"</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    model=json.load(json_file)</span><br><span class="line"></span><br><span class="line">读取返回的为python字典</span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"> </span><br><span class="line">str_file = <span class="string">'./960x540/config.json'</span></span><br><span class="line"><span class="keyword">with</span> open(str_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    print(<span class="string">"Load str file from &#123;&#125;"</span>.format(str_file))</span><br><span class="line">    r = json.load(f)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r)</span><br><span class="line">print(r[<span class="string">'under_game_score_y'</span>])</span><br></pre></td></tr></table></figure><h2 id="7-python标准库"><a href="#7-python标准库" class="headerlink" title="7. python标准库"></a>7. python标准库</h2></li></ul><div align=" center"><img src="https://img.vim-cn.com/47/19c6e06be5f16f01de1a5d1ef6733fd9129648.png"></div><p><strong>也可以查看标准库文档</strong><br><a href="https://docs.python.org/zh-cn/3/tutorial/index.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/tutorial/index.html</a><br><a href="https://docs.python.org/zh-cn/3/library/index.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/index.html</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>桶排序</title>
      <link href="/2020/05/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%A1%B6%E6%8E%92%E5%BA%8F/"/>
      <url>/2020/05/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%A1%B6%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>桶排序有三个核心问题：</p><ul><li>每个桶的长度是多少</li><li>总共需要多少个桶</li><li>如何确定元素应该在哪一个桶</li></ul><p>因此桶排序有几个公式需要记忆一下：</p><script type="math/tex; mode=display">length = \frac{max(nums)-min(nums)}{len(nums)-1}</script><script type="math/tex; mode=display">count = \frac{max(nums)-min(nums)}{length}+1</script><script type="math/tex; mode=display">location = \frac{nums[i]-min(nums)}{length}</script><p><strong>leetcode164中有一道题如下：</strong><br>给定一个无序的数组，找出数组在排序之后，相邻元素之间最大的差值。</p><p>如果数组元素个数小于 2，则返回 0。</p><p>示例 1:</p><p>输入: [3,6,9,1]<br>输出: 3<br>解释: 排序后的数组是 [1,3,6,9], 其中相邻元素 (3,6) 和 (6,9) 之间都存在最大差值 3。<br>示例 2:</p><p>输入: [10]<br>输出: 0<br>解释: 数组元素个数小于 2，因此返回 0。<br>说明:</p><p>你可以假设数组中所有元素都是非负整数，且数值在 32 位有符号整数范围内。<br>请尝试在线性时间复杂度和空间复杂度的条件下解决此问题。</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/maximum-gap" target="_blank" rel="noopener">https://leetcode-cn.com/problems/maximum-gap</a></p><p>python代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maximumGap</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums <span class="keyword">or</span> len(nums) &lt; <span class="number">2</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    max_ = max(nums)</span><br><span class="line">    min_ = min(nums)</span><br><span class="line">    max_gap = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    each_bucket_len = max(<span class="number">1</span>,(max_-min_) // (len(nums)<span class="number">-1</span>))</span><br><span class="line">    buckets =[[] <span class="keyword">for</span> _ <span class="keyword">in</span> range((max_-min_) // each_bucket_len + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">        loc = (nums[i] - min_) // each_bucket_len</span><br><span class="line">        buckets[loc].append(nums[i])</span><br><span class="line">    </span><br><span class="line">    prev_max = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(buckets)):</span><br><span class="line">        <span class="keyword">if</span> buckets[i] <span class="keyword">and</span> prev_max != float(<span class="string">'inf'</span>):</span><br><span class="line">            max_gap = max(max_gap, min(buckets[i])-prev_max)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> buckets[i]:</span><br><span class="line">            prev_max = max(buckets[i])</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> max_gap</span><br></pre></td></tr></table></figure></p><p>C++代码如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maximumGap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> minVal = *min_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">int</span> maxVal = *max_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">int</span> d = <span class="built_in">max</span>(<span class="number">1</span>, (maxVal - minVal) / (n - <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">int</span> bucketSize = (maxVal - minVal) / d + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        vector&lt;pair&lt;int, int&gt;&gt; bucket(bucketSize, &#123;-1, -1&#125;);  // 存储 (桶内最小值，桶内最大值) 对，(-1, -1) 表示该桶是空的</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> idx = (nums[i] - minVal) / d;</span><br><span class="line">            <span class="keyword">if</span> (bucket[idx].first == <span class="number">-1</span>) &#123;</span><br><span class="line">                bucket[idx].first = bucket[idx].second = nums[i];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                bucket[idx].first = <span class="built_in">min</span>(bucket[idx].first, nums[i]);</span><br><span class="line">                bucket[idx].second = <span class="built_in">max</span>(bucket[idx].second, nums[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> prev = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bucketSize; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (bucket[i].first == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> (prev != <span class="number">-1</span>) &#123;</span><br><span class="line">                ret = <span class="built_in">max</span>(ret, bucket[i].first - bucket[prev].second);</span><br><span class="line">            &#125;</span><br><span class="line">            prev = i;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构与算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基数排序</title>
      <link href="/2020/03/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
      <url>/2020/03/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><p>基数排序（radix sort)是一种分配排序算法，排序过程种无需比较关键字，而是通过分配与收集的过程实现排序，它们的时间复杂度为O(n)，在线性阶层上。</p><p>排序方法分为LSD 与MSD两种方法，LSD是从低阶往高阶进行排序，MSD方法是从高阶往低阶进行排序。</p><p>实现方法与代码：</p><ul><li>获取数组中的最高位数</li><li>建立一个桶数组，根据个位数进行赋值，遍历数组，将它们分配至编号0-9的桶中去</li><li>将桶中的数值串联起来，通过buf数组暂存，并拷贝到原始数组中</li><li>重复前2个步骤</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxbit1</span><span class="params">(<span class="keyword">int</span> data[], <span class="keyword">int</span> n)</span> <span class="comment">//辅助函数，求解数据的最大位数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> d = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(data[i] &gt;= p)</span><br><span class="line">        &#123;</span><br><span class="line">            p *= <span class="number">10</span>;</span><br><span class="line">            ++d;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxbit2</span><span class="params">(<span class="keyword">int</span> maxVal)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> d = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">while</span>(maxVal &gt;= p)</span><br><span class="line">    &#123;</span><br><span class="line">        p *= <span class="number">10</span>;</span><br><span class="line">        ++d;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">radixSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">buf</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">int</span> maxVal = *max_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>()); </span><br><span class="line">    <span class="keyword">int</span> maxbit =  maxbit2(maxVal);</span><br><span class="line">    <span class="comment">// 进入循环</span></span><br><span class="line">    <span class="keyword">int</span> dev = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; maxbit; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//开始统计每个桶的元素个数</span></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">count</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> digit = (nums[j] / dev) % <span class="number">10</span>;</span><br><span class="line">            count[digit]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将count数组变为数组的索引</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; <span class="number">10</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            count[j] += count[j<span class="number">-1</span>];</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将桶中的数值串联起来</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = n<span class="number">-1</span> ;j &gt;= <span class="number">0</span>; j --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> digit = (nums[j] / dev) % <span class="number">10</span>;</span><br><span class="line">            buf[count[digit] - <span class="number">1</span>] = nums[j];</span><br><span class="line">            count[digit]--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将临时数组拷贝给原数组</span></span><br><span class="line">        copy(buf.<span class="built_in">begin</span>(), buf.<span class="built_in">end</span>(), nums.<span class="built_in">begin</span>());</span><br><span class="line">        dev *= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>leetcode164中有一道题如下：</strong><br>给定一个无序的数组，找出数组在排序之后，相邻元素之间最大的差值。</p><p>如果数组元素个数小于 2，则返回 0。</p><p>示例 1:</p><p>输入: [3,6,9,1]<br>输出: 3<br>解释: 排序后的数组是 [1,3,6,9], 其中相邻元素 (3,6) 和 (6,9) 之间都存在最大差值 3。<br>示例 2:</p><p>输入: [10]<br>输出: 0<br>解释: 数组元素个数小于 2，因此返回 0。<br>说明:</p><p>你可以假设数组中所有元素都是非负整数，且数值在 32 位有符号整数范围内。<br>请尝试在线性时间复杂度和空间复杂度的条件下解决此问题。</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/maximum-gap" target="_blank" rel="noopener">https://leetcode-cn.com/problems/maximum-gap</a></p><p>基于上面的基数排序可以进行求解，只要再加上一个maxGap的函数就可以<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxGap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ret = <span class="built_in">max</span>(ret, nums[i] - nums[i<span class="number">-1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构与算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/2020/02/17/Deep_learning/Transformer/"/>
      <url>/2020/02/17/Deep_learning/Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="transformer模型"><a href="#transformer模型" class="headerlink" title="transformer模型"></a>transformer模型</h2><p>在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：</p><ul><li>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</li><li>RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。<br>为了整合CNN和RNN的优势，[Vaswani et al., 2017] 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。</li></ul><p>Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p><ul><li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li><li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li><li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li></ul><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960"></img></div><p><img src="https://i.bmp.ovh/imgs/2020/02/6801838056cf8851.png" alt=""></p><p><img src="https://i.bmp.ovh/imgs/2020/02/be0c163e470769bb.png" alt=""></p><h2 id="transformer的pytorch实现"><a href="#transformer的pytorch实现" class="headerlink" title="transformer的pytorch实现"></a>transformer的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'../'</span>)</span><br><span class="line"><span class="keyword">import</span> d2l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    X_len = X_len.to(X.device)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)</span><br><span class="line">    mask = mask[<span class="literal">None</span>, :] &lt; X_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h2 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/c20478604f4c831c.png" alt=""></p><h3 id="多头注意力模型"><a href="#多头注意力模型" class="headerlink" title="多头注意力模型"></a>多头注意力模型</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/b9fcbb9a53f2e4db.png" alt=""></p><p>在我们讨论多头注意力层之前，先来迅速理解以下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320"></img></div><p>多头注意力层h包含个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p><p><img src="C:\Users\shy\AppData\Roaming\Typora\typora-user-images\image-20200217212647159.png" alt="image-20200217212647159"></p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640"></img></div><p><img src="https://i.bmp.ovh/imgs/2020/02/5246bbe9920c926b.png" alt=""></p><h3 id="多头注意力pytorch"><a href="#多头注意力pytorch" class="headerlink" title="多头注意力pytorch"></a>多头注意力pytorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saved in the d2l package for later use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># A reversed version of transpose_qkv</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    <span class="keyword">return</span> X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="基于位置的前馈网络（FFN"><a href="#基于位置的前馈网络（FFN" class="headerlink" title="基于位置的前馈网络（FFN)"></a>基于位置的前馈网络（FFN)</h2><p>Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。</p><p>下面我们来实现PositionWiseFFN：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs)</span>:</span></span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)</span><br><span class="line">        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ffn_2(F.relu(self.ffn_1(X)))</span><br></pre></td></tr></table></figure><h2 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h2><p>除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。 </p><p><img src="https://i.bmp.ovh/imgs/2020/02/3799a624c6b07777.png" alt=""></p><p><img src="https://i.bmp.ovh/imgs/2020/02/ef62acc1523f3c3f.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">layernorm = nn.LayerNorm(normalized_shape=<span class="number">2</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">batchnorm = nn.BatchNorm1d(num_features=<span class="number">2</span>, affine=<span class="literal">True</span>)</span><br><span class="line">X = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(<span class="string">'layer norm:'</span>, layernorm(X))</span><br><span class="line">print(<span class="string">'batch norm:'</span>, batchnorm(X))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/2903fe22b21afacc.png" alt=""></p><p>衡量单词向量的距离，与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640"></img></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, dropout, max_len=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = np.zeros((<span class="number">1</span>, max_len, embedding_size))</span><br><span class="line">        X = np.arange(<span class="number">0</span>, max_len).reshape(<span class="number">-1</span>, <span class="number">1</span>) / np.power(</span><br><span class="line">            <span class="number">10000</span>, np.arange(<span class="number">0</span>, embedding_size, <span class="number">2</span>)/embedding_size)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = np.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = np.cos(X)</span><br><span class="line">        self.P = torch.FloatTensor(self.P)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> X.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.P.is_cuda:</span><br><span class="line">            self.P = self.P.cuda()</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h2><p>我们已经有了组成Transformer的各个模块，现在我们可以开始搭建了！编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，我们的输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为我们要将前一层的输出与原始输入相加并归一化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length)</span>:</span></span><br><span class="line">        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>现在我们来实现整个Transformer 编码器模型，整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以$\sqrt{d}$以防止其值过小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                EncoderBlock(embedding_size, ffn_hidden_size,</span><br><span class="line">                             num_heads, dropout))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length, *args)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_length)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h2 id="解码器（Decoder"><a href="#解码器（Decoder" class="headerlink" title="解码器（Decoder)"></a>解码器（Decoder)</h2><p>Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。</p><p><img src="https://i.loli.net/2020/02/17/klqSyL9aibmEQBH.png" alt="image.png"></p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpefhcyg.png?imageView2/0/w/800/h/800"></img></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs)</span>:</span></span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_3 = AddNorm(embedding_size, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, enc_valid_length = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># state[2][self.i] stores all the previous t-1 query state of layer-i</span></span><br><span class="line">        <span class="comment"># len(state[2]) = num_layers</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If training:</span></span><br><span class="line">        <span class="comment">#     state[2] is useless.</span></span><br><span class="line">        <span class="comment"># If predicting:</span></span><br><span class="line">        <span class="comment">#     In the t-th timestep:</span></span><br><span class="line">        <span class="comment">#         state[2][self.i].shape = (batch_size, t-1, hidden_size)</span></span><br><span class="line">        <span class="comment"># Demo:</span></span><br><span class="line">        <span class="comment"># love dogs ! [EOS]</span></span><br><span class="line">        <span class="comment">#  |    |   |   |</span></span><br><span class="line">        <span class="comment">#   Transformer </span></span><br><span class="line">        <span class="comment">#    Decoder</span></span><br><span class="line">        <span class="comment">#  |   |   |   |</span></span><br><span class="line">        <span class="comment">#  I love dogs !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># shape of key_values = (batch_size, t, hidden_size)</span></span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), dim=<span class="number">1</span>) </span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention_1(X, key_values, key_values, valid_length)</span><br><span class="line">        Y = self.addnorm_1(X, X2)</span><br><span class="line">        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)</span><br><span class="line">        Z = self.addnorm_2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。下面让我们来实现一下Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,</span><br><span class="line">                             dropout, i))</span><br><span class="line">        self.dense = nn.Linear(embedding_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_length, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_length, [<span class="literal">None</span>]*self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>Transformer同样基于编码器-解码器架构</li><li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li><li>dd and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li><li>osition encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li><li>增强训练，可以做个label_smoothing</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络基础（CNN)</title>
      <link href="/2020/02/17/Deep_learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88CNN)/"/>
      <url>/2020/02/17/Deep_learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88CNN)/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。<br>我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为$3 \times 3$或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即$2 \times 2$。图5.1中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$0\times0+1\times1+3\times2+4\times3=19$。</p><div align=center><img width="250" src="https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640"/></div><div align=center>图5.1 二维互相关运算</div><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：</p><script type="math/tex; mode=display">0\times0+1\times1+3\times2+4\times3=19,\\1\times0+2\times1+4\times2+5\times3=25,\\3\times0+4\times1+6\times2+7\times3=37,\\4\times0+5\times1+7\times2+8\times3=43.\\</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维互相关运算核心示例</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    H, W = X.shape</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(H - h + <span class="number">1</span>, W - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维卷积层的pytorch示例</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h2><p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。</p><p>那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出图5.1中的核数组。设其他条件不变，使用卷积运算学出的核数组即图5.1中的核数组按上下、左右翻转。也就是说，输入与学出的已翻转的核数组再做卷积运算时，依然得到图5.1中的输出。为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。</p><h2 id="特征图和感受野"><a href="#特征图和感受野" class="headerlink" title="特征图和感受野"></a>特征图和感受野</h2><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图5.1中形状为$2 \times 2$的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p><p>我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。当含义明确时，本书不对这两个术语做严格区分。</p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们使用高和宽为3的输入与高和宽为2的卷积核得到高和宽为2的输出。一般来说，假设输入形状是$n_h\times n_w$，卷积核窗口形状是$k_h\times k_w$，那么输出形状将会是</p><script type="math/tex; mode=display">(n_h-k_h+1) \times (n_w-k_w+1).</script><p>所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5nfl6ejy4.png?imageView2/0/w/640/h/640"></img>    </div><p>一般来说，如果在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是</p><script type="math/tex; mode=display">(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1),</script><p>也就是说，输出的高和宽会分别增加$p_h$和$p_w$。</p><p>在很多情况下，我们会设置$p_h=k_h-1$和$p_w=k_w-1$来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里$k_h$是奇数，我们会在高的两侧分别填充$p_h/2$行。如果$k_h$是偶数，一种可能是在输入的顶端一侧填充$\lceil p_h/2\rceil$行，而在底端一侧填充$\lfloor p_h/2\rfloor$行。在宽的两侧填充同理。</p><p>卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组<code>X</code>，设它的第<code>i</code>行第<code>j</code>列的元素为<code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出<code>Y[i,j]</code>是由输入以<code>X[i,j]</code>为中心的窗口同卷积核进行互相关计算得到的。</p><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。图5.3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图5.3中的阴影部分为输出元素及其计算所使用的输入和核数组元素：$0\times0+0\times1+1\times2+2\times3=8$、$0\times0+6\times1+0\times2+0\times3=6$。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nflohnqg.png?imageView2/0/w/640/h/640"/></div><div align=center>高和宽上步幅分别为3和2的二维互相关运算</div><p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为</p><script type="math/tex; mode=display">\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.</script><p>如果设置$p_h=k_h-1$和$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h/s_h) \times (n_w/s_w)$。</p><h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\times h\times w$的多维数组。我们将大小为3的这一维称为通道（channel）维。本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。设卷积核窗口形状为$k_h\times k_w$。当$c_i=1$时，我们知道卷积核只包含一个形状为$k_h\times k_w$的二维数组。当$c_i &gt; 1$时，我们将会为每个输入通道各分配一个形状为$k_h\times k_w$的核数组。把这$c_i$个数组在输入通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。由于输入和卷积核各有$c_i$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。</p><p>图5.4展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。图5.4中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nfmdnwbq.png?imageView2/0/w/640/h/640"/></div><div align=center>图5.4 含2个输入通道的互相关计算</div><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\times c_i\times k_h\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。</p><h2 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h2><p>卷积窗口形状为$1\times 1$（$k_h=k_w=1$）的多通道卷积层。我们通常称之为$1\times 1$卷积层，并将其中的卷积运算称为$1\times 1$卷积。因为使用了最小窗口，$1\times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times 1$卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么$1\times 1$卷积层的作用与全连接层等价</strong>。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nfmq980r.png?imageView2/0/w/640/h/640"/></div><div align=center> 1x1卷积核的互相关计算。输入和输出具有相同的高和宽</div><h2 id="卷积层与全连接层的比较"><a href="#卷积层与全连接层的比较" class="headerlink" title="卷积层与全连接层的比较"></a>卷积层与全连接层的比较</h2><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p><ul><li><p>一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p></li><li><p>二是卷积层的参数量更少。使用卷积层可以以较少的参数数量来处理更大的图像。</p></li></ul><h2 id="卷积层的pytorch实现"><a href="#卷积层的pytorch实现" class="headerlink" title="卷积层的pytorch实现"></a>卷积层的pytorch实现</h2><ul><li>in_channels (python:int) – Number of channels in the input imag</li><li>out_channels (python:int) – Number of channels produced by the convolution</li><li>kernel_size (python:int or tuple) – Size of the convolving kernel</li><li>stride (python:int or tuple, optional) – Stride of the convolution. Default: 1</li><li>padding (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0</li><li>bias (bool, optional) – If True, adds a learnable bias to the output. Default: True</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), stride=<span class="number">1</span>, padding=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">Y = conv2d(X)</span><br><span class="line">print(<span class="string">'Y.shape: '</span>, Y.shape)</span><br><span class="line">print(<span class="string">'weight.shape: '</span>, conv2d.weight.shape)</span><br><span class="line">print(<span class="string">'bias.shape: '</span>, conv2d.bias.shape)</span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><ul><li>二维最大池化层<br>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。<br><div align=center><img width="300" src="https://cdn.kesci.com/upload/image/q5nfob3odo.png?imageView2/0/w/640/h/640"/></div><br><div align=center>图5.6 池化窗口形状为 2 x 2 的最大池化</div><br>池化窗口形状为$2\times 2$的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为2，其中的4个元素由取最大值运算$\text{max}$得出：</li></ul><script type="math/tex; mode=display">\max(0,1,3,4)=4,\\\max(1,2,4,5)=5,\\\max(3,4,6,7)=7,\\\max(4,5,7,8)=8.\\</script><ul><li>二维平均池化层</li></ul><p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p><p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为$2\times 2$最大池化的输入。设该卷积层输入是<code>X</code>、池化层输出为<code>Y</code>。无论是<code>X[i, j]</code>和<code>X[i, j+1]</code>值不同，还是<code>X[i, j+1]</code>和<code>X[i, j+2]</code>不同，池化层输出均有<code>Y[i, j]=1</code>。也就是说，使用$2\times 2$最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p><h3 id="池化层的pytorch实现"><a href="#池化层的pytorch实现" class="headerlink" title="池化层的pytorch实现"></a>池化层的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">32</span>, dtype=torch.float32).view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均池化层使用的是nn.AvgPool2d，使用方法与nn.MaxPool2d相同。</span></span><br><span class="line"></span><br><span class="line">pool2d = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">Y = pool2d(X)</span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeNet、AlexNet、VGG、NiN、GoogLeNet</title>
      <link href="/2020/02/17/Deep_learning/LeNet%E3%80%81AlexNet%E3%80%81VGG%E3%80%81NiN%E3%80%81GooLeNet/"/>
      <url>/2020/02/17/Deep_learning/LeNet%E3%80%81AlexNet%E3%80%81VGG%E3%80%81NiN%E3%80%81GooLeNet/</url>
      
        <content type="html"><![CDATA[<h1 id="LeNet、AlexNet、VGG、NiN、GoogLeNet"><a href="#LeNet、AlexNet、VGG、NiN、GoogLeNet" class="headerlink" title="LeNet、AlexNet、VGG、NiN、GoogLeNet"></a>LeNet、AlexNet、VGG、NiN、GoogLeNet</h1><h2 id="全连接层与卷积层的优势对比"><a href="#全连接层与卷积层的优势对比" class="headerlink" title="全连接层与卷积层的优势对比"></a>全连接层与卷积层的优势对比</h2><p>使用全连接层的局限性：</p><ul><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li><p>对于大尺寸的输入图像，使用全连接层容易导致模型过大。<br>使用卷积层的优势：</p></li><li><p>卷积层保留输入形状。</p></li><li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><h3 id="LeNet模型"><a href="#LeNet模型" class="headerlink" title="LeNet模型"></a>LeNet模型</h3>LeNet分为卷积层块和全连接层块两个部分。<br><div align=center><img width="600" src="https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960"/></div><br><div align=center>LeNet网络结构</div><br>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</li></ul><p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><h3 id="LeNet的pytorch实现"><a href="#LeNet的pytorch实现" class="headerlink" title="LeNet的pytorch实现"></a>LeNet的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"../"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment">#展平操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment">#将图像大小重定型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)      <span class="comment">#(B x C x H x W)</span></span><br><span class="line">    </span><br><span class="line">net = torch.nn.Sequential(     <span class="comment">#Lelet                                                  </span></span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment">#b*1*28*28  =&gt;b*6*28*28</span></span><br><span class="line">    nn.Sigmoid(),                                                       </span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                              <span class="comment">#b*6*28*28  =&gt;b*6*14*14</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>),           <span class="comment">#b*6*14*14  =&gt;b*16*10*10</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                              <span class="comment">#b*16*10*10  =&gt; b*16*5*5</span></span><br><span class="line">    Flatten(),                                                          <span class="comment">#b*16*5*5   =&gt; b*400</span></span><br><span class="line">    nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>LeNet: 在大的真实数据集上的表现并不尽如⼈意。<br>1.神经网络计算复杂。<br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。</p><p>机器学习的特征提取:手工定义的特征提取函数<br>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。</p><p>神经网络发展的限制:数据、硬件</p><h3 id="AlexNet模型"><a href="#AlexNet模型" class="headerlink" title="AlexNet模型"></a>AlexNet模型</h3><p>AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p><div align=center><img width="600" src="https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640"/></div><div align=center>AlexNet网络结构</div><p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p><p>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面我们来详细描述这些层的设计。</p><p>AlexNet第一层中的卷积窗口形状是$11\times11$。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到$5\times5$，之后全采用$3\times3$。此外，第一、第二和第五个卷积层之后都使用了窗口形状为$3\times3$、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。</p><p>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p><p>第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p><p>第三，AlexNet通过丢弃法（参见3.13节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</p><p>第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p><h3 id="AlexNet的pytorch实现"><a href="#AlexNet的pytorch实现" class="headerlink" title="AlexNet的pytorch实现"></a>AlexNet的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input/"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment">#由于使用CPU镜像，精简网络，若为GPU镜像可添加该层</span></span><br><span class="line">            <span class="comment">#nn.Linear(4096, 4096),</span></span><br><span class="line">            <span class="comment">#nn.ReLU(),</span></span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line"></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br></pre></td></tr></table></figure><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><h3 id="VGG模型"><a href="#VGG模型" class="headerlink" title="VGG模型"></a>VGG模型</h3><p>VGG：通过重复使⽤简单的基础块来构建深度模型。<br>Block:数个相同的填充为1、窗口形状为的卷积层,接上一个步幅为2、窗口形状为的最大池化层。<br>卷积层保持输入的高和宽不变，而池化层则对其减半。</p><p><img src="https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640" alt="VGG"></p><p>与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个<code>vgg_block</code>，其超参数由变量<code>conv_arch</code>定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。</p><h3 id="VGG的实现"><a href="#VGG的实现" class="headerlink" title="VGG的实现"></a>VGG的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span> <span class="comment">#卷积层个数，输入通道数，输出通道数</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">"vgg_block_"</span> + str(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h2 id="NiN-网络中的网络）"><a href="#NiN-网络中的网络）" class="headerlink" title="NiN(网络中的网络）"></a>NiN(网络中的网络）</h2><p>LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。<br>NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。<br>⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。</p><h3 id="NiN模型"><a href="#NiN模型" class="headerlink" title="NiN模型"></a>NiN模型</h3><p><img src="https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960" alt="NiN"><br>1×1卷积核作用</p><ol><li>放缩通道数：通过控制卷积核的数量达到通道数的放缩。</li><li>增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。</li><li>计算参数少</li></ol><h3 id="NiN的pytorch实现"><a href="#NiN的pytorch实现" class="headerlink" title="NiN的pytorch实现"></a>NiN的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure><h2 id="GooLeNet"><a href="#GooLeNet" class="headerlink" title="GooLeNet"></a>GooLeNet</h2><ol><li>由Inception基础块组成。</li><li>Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。</li><li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</li></ol><h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><p><img src="https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640" alt="goolnet"><br>Inception块里有4条并行的线路。前3条线路使用窗口大小分别是$1\times 1$、$3\times 3$和$5\times 5$的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做$1\times 1$卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用$3\times 3$最大池化层，后接$1\times 1$卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p><p>Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</p><h3 id="完整goolenet模型"><a href="#完整goolenet模型" class="headerlink" title="完整goolenet模型"></a>完整goolenet模型</h3><p>GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的$3\times 3$最大池化层来减小输出高宽。</p><ul><li>第一模块使用一个64通道的$7\times 7$卷积层。</li><li>第二模块使用2个卷积层：首先是64通道的$1\times 1$卷积层，然后是将通道增大3倍的$3\times 3$卷积层。它对应Inception块中的第二条线路。</li><li>第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为$64+128+32+32=256$，其中4条线路的输出通道数比例为$64:128:32:32=2:4:1:1$。其中第二、第三条线路先分别将输入通道数减小至$96/192=1/2$和$16/192=1/12$后，再接上第二层卷积层。第二个Inception块输出通道数增至$128+192+96+64=480$，每条线路的输出通道数之比为$128:192:96:64 = 4:6:3:2$。其中第二、第三条线路先分别将输入通道数减小至$128/256=1/2$和$32/256=1/8$</li><li>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。这些线路的通道数分配和第三模块中的类似，首先含$3\times 3$卷积层的第二条线路输出最多通道，其次是仅含$1\times 1$卷积层的第一条线路，之后是含$5\times 5$卷积层的第三条线路和含$3\times 3$最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。</li><li>第五模块有输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。<br><div align=center><img width="500" src="https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640"/></div><br><div align=center>完整模型</div><h3 id="GooLeNet的pytorch"><a href="#GooLeNet的pytorch" class="headerlink" title="GooLeNet的pytorch"></a>GooLeNet的pytorch</h3></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, </span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(<span class="string">'output shape: '</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>卷积神经网络就是含卷积层的网络。</li><li>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</li><li>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</li><li>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</li></ul><ul><li>VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。</li></ul><ul><li>NiN重复使用由卷积层和代替全连接层的$1\times 1$卷积层构成的NiN块来构建深层网络。</li><li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。</li><li><p>NiN的以上设计思想影响了后面一系列卷积神经网络的设计。</p></li><li><p>Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1\times 1$卷积层减少通道数从而降低模型复杂度。</p></li><li>GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li><li>GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。</li></ul>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制（Attention)</title>
      <link href="/2020/02/16/Deep_learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2020/02/16/Deep_learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><p>在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p><p><img src="https://i.bmp.ovh/imgs/2020/02/0d5061d054b296a1.png" alt=""></p><p>与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。<br><img src="https://img-blog.csdnimg.cn/20200216222846156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h2><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 Query  , attention layer得到输出与value的维度一致 . 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量则是value的加权求和，而每个key计算的权重与value一一对应。</p><p>为了计算输出，我们首先假设有一个函数 用于计算query和key的相似性，然后可以计算所有的 attention scores ${a_1, \ldots, a_n }$by</p><script type="math/tex; mode=display">a_i = \alpha(\mathbf q, \mathbf k_i)</script><p>我们使用 softmax函数 获得注意力权重：</p><script type="math/tex; mode=display">b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n)</script><p>最终的输出就是value的加权求和：</p><script type="math/tex; mode=display">\mathbf o = \sum_{i=1}^n b_i \mathbf v_i</script><p><img src="https://img-blog.csdnimg.cn/20200216224849212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。</p><h3 id="softmax的屏蔽"><a href="#softmax的屏蔽" class="headerlink" title="softmax的屏蔽"></a>softmax的屏蔽</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen),dtype=torch.float)[<span class="literal">None</span>, :] &gt;= X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line">masked_softmax(torch.rand((<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),dtype=torch.float), torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>]))</span><br></pre></td></tr></table></figure><h3 id="超出二维矩阵的乘法"><a href="#超出二维矩阵的乘法" class="headerlink" title="超出二维矩阵的乘法"></a>超出二维矩阵的乘法</h3><p> X和  Y是维度分别为(b,n,m)和(b, m, k)的张量，进行 b次二维矩阵乘法后得到 , 维度为 (b, n, k)。</p><script type="math/tex; mode=display">Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,…,n\</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bmm(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>), dtype = torch.float), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>), dtype = torch.float))</span><br></pre></td></tr></table></figure><h2 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h2><p>The dot product 假设query和keys有相同的维度, 即 . 通过计算query和key转置的乘积来计算attention score,通常还会除去sqrt{d}减少计算出来的score对维度𝑑的依赖性，如下<br>𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \sqrt{d}<br>假设𝐐∈ℝ^{𝑚×𝑑}有m个query， 有n个keys. 我们可以通过矩阵运算的方式计算所有mn个score：<br>𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\sqrt{d}<br>现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        </span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        print(<span class="string">"attention_weight\n"</span>,attention_weights)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h2 id="多层感知机注意力"><a href="#多层感知机注意力" class="headerlink" title="多层感知机注意力"></a>多层感知机注意力</h2><p>将score函数定义:</p><script type="math/tex; mode=display">a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),</script><p>. 然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为 ℎ and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPAttention</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units,ipt_dim,dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MLPAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Use flatten=True to keep query's and key's 3-D shapes.</span></span><br><span class="line">        self.W_k = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v = nn.Linear(units, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        query, key = self.W_k(query), self.W_q(key)</span><br><span class="line">        <span class="comment">#print("size",query.size(),key.size())</span></span><br><span class="line">        <span class="comment"># expand query to (batch_size, #querys, 1, units), and key to</span></span><br><span class="line">        <span class="comment"># (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.</span></span><br><span class="line">        features = query.unsqueeze(<span class="number">2</span>) + key.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print("features:",features.size())  #--------------开启</span></span><br><span class="line">        scores = self.v(features).squeeze(<span class="number">-1</span>) </span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h3 id="计算背景变量"><a href="#计算背景变量" class="headerlink" title="计算背景变量"></a>计算背景变量</h3><p>我们先描述第一个关键点，即计算背景变量。图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。<br><img src="https://img-blog.csdnimg.cn/20200216230638792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>具体来说，令编码器在时间步$t$的隐藏状态为$\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t’$的背景变量为所有编码器隐藏状态的加权平均：</p><script type="math/tex; mode=display">\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,</script><p>其中给定$t’$时，权重$\alpha_{t’ t}$在$t=1,\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:</p><script type="math/tex; mode=display">\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.</script><p>现在，我们需要定义如何计算上式中softmax运算的输入$e_{t’ t}$。由于$e_{t’ t}$同时取决于解码器的时间步$t’$和编码器的时间步$t$，我们不妨以解码器在时间步$t’-1$的隐藏状态$\boldsymbol{s}_{t’ - 1}$与编码器在时间步$t$的隐藏状态$\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t’ t}$：</p><script type="math/tex; mode=display">e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).</script><p>这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：</p><script type="math/tex; mode=display">a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),</script><p>其中$\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$都是可以学习的模型参数。</p><h3 id="矢量化计算"><a href="#矢量化计算" class="headerlink" title="矢量化计算"></a>矢量化计算</h3><p>我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p><p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。<br>让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\boldsymbol{s}_{t’ - 1} \in \mathbb{R}^{h}$和编码器所有隐藏状态$\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T$来计算背景向量$\boldsymbol{c}_{t’}\in \mathbb{R}^{h}$。<br>我们可以将查询项矩阵$\boldsymbol{Q} \in \mathbb{R}^{1 \times h}$设为$\boldsymbol{s}_{t’ - 1}^\top$，并令键项矩阵$\boldsymbol{K} \in \mathbb{R}^{T \times h}$和值项矩阵$\boldsymbol{V} \in \mathbb{R}^{T \times h}$相同且第$t$行均为$\boldsymbol{h}_t^\top$。此时，我们只需要通过矢量化计算</p><script type="math/tex; mode=display">\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}</script><p>即可算出转置后的背景向量$\boldsymbol{c}_{t’}^\top$。当查询项矩阵$\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p><h2 id="引入注意力机制的S2S"><a href="#引入注意力机制的S2S" class="headerlink" title="引入注意力机制的S2S"></a>引入注意力机制的S2S</h2><p>本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入拼接起来一起送到解码器：<br><img src="https://img-blog.csdnimg.cn/2020021623101586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构<br><img src="https://img-blog.csdnimg.cn/20200216231034160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:</p><ul><li>the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values</li><li>the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state</li><li>the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）  </li></ul><p>在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_len, *args)</span>:</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line"><span class="comment">#         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())</span></span><br><span class="line">        <span class="comment"># Transpose outputs to (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>), hidden_state, enc_valid_len)</span><br><span class="line">        <span class="comment">#outputs.swapaxes(0, 1)</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_len = state</span><br><span class="line">        <span class="comment">#("X.size",X.size())</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print("Xembeding.size2",X.size())</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> l, x <span class="keyword">in</span> enumerate(X):</span><br><span class="line"><span class="comment">#             print(f"\n&#123;l&#125;-th token")</span></span><br><span class="line"><span class="comment">#             print("x.first.size()",x.size())</span></span><br><span class="line">            <span class="comment"># query shape: (batch_size, 1, hidden_size)</span></span><br><span class="line">            <span class="comment"># select hidden state of the last rnn layer as query</span></span><br><span class="line">            query = hidden_state[<span class="number">0</span>][<span class="number">-1</span>].unsqueeze(<span class="number">1</span>) <span class="comment"># np.expand_dims(hidden_state[0][-1], axis=1)</span></span><br><span class="line">            <span class="comment"># context has same shape as query</span></span><br><span class="line"><span class="comment">#             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())</span></span><br><span class="line">            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)</span><br><span class="line">            <span class="comment"># Concatenate on the feature dimension</span></span><br><span class="line"><span class="comment">#             print("context.size:",context.size())</span></span><br><span class="line">            x = torch.cat((context, x.unsqueeze(<span class="number">1</span>)), dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># Reshape x to (1, batch_size, embed_size+hidden_size)</span></span><br><span class="line"><span class="comment">#             print("rnn",x.size(), len(hidden_state))</span></span><br><span class="line">            out, hidden_state = self.rnn(x.transpose(<span class="number">0</span>,<span class="number">1</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.transpose(<span class="number">0</span>, <span class="number">1</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                        enc_valid_len]</span><br><span class="line"></span><br><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                            num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># encoder.initialize()</span></span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                                  num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>),dtype=torch.long)</span><br><span class="line">print(<span class="string">"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n"</span>)</span><br><span class="line">print(<span class="string">'encoder output size:'</span>, encoder(X)[<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder hidden size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder memory size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">1</span>].size())</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">out, state = decoder(X, state)</span><br><span class="line">out.shape, len(state), state[<span class="number">0</span>].shape, len(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型选择与过拟合与欠拟合</title>
      <link href="/2020/02/16/Deep_learning/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
      <url>/2020/02/16/Deep_learning/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h1 id="模型选择与过拟合与欠拟合"><a href="#模型选择与过拟合与欠拟合" class="headerlink" title="模型选择与过拟合与欠拟合"></a>模型选择与过拟合与欠拟合</h1><h2 id="训练误差与泛化误差"><a href="#训练误差与泛化误差" class="headerlink" title="训练误差与泛化误差"></a>训练误差与泛化误差</h2><p>训练误差（training error）指模型在训练数据集上表现出的误差。<br>泛化误差（generalization error）指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。<br>计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。<br>所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。</p><p>机器学习模型应关注降低泛化误差。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><p>测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。<br>预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是$K$折交叉验证（$K$-fold cross-validation）。在$K$折交叉验证中，我们把原始训练数据集分割成$K$个不重合的子数据集，然后我们做$K$次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他$K-1$个子数据集来训练模型。在这$K$次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这$K$次训练误差和验证误差分别求平均。</p><h2 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h2><ul><li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。 在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/cf7248d423fab8a5.png"/></div><div align=center>图3.4 模型复杂度对欠拟合和过拟合的影响</div><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p><h2 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h2><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/aca411216e606cf8.png"/></div><div align=center>正常拟合</div><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/4b81e7b5075cc20d.png"/></div><div align=center>欠拟合</div><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/6ee4f9ffe1f7059f.png"/></div><div align=center>过拟合</div><h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>上一节中我们观察了过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述$L_2$范数正则化，再解释它为何又称权重衰减。</p><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以3.1节（线性回归）中的线性回归损失函数</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>为例，其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2,</script><p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为</p><script type="math/tex; mode=display">\begin{aligned}w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).\end{aligned}</script><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p><h3 id="权重衰减的pytorch实现"><a href="#权重衰减的pytorch实现" class="headerlink" title="权重衰减的pytorch实现"></a>权重衰减的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>多层感知机的计算表达式为</p><script type="math/tex; mode=display">h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)</script><p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1-p} h_i</script><p>由于$E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display">E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即<strong>丢弃法不改变其输入的期望值</strong>。</p><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/ec3bfe47de764684.png"/></div><div align=center> 隐藏层使用了丢弃法的多层感知机</div><h3 id="dropout的pytorch实现"><a href="#dropout的pytorch实现" class="headerlink" title="dropout的pytorch实现"></a>dropout的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h1 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h1><p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(L)}$的权重参数为$\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（衰减）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p><h1 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h1><h2 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h2><p>随机初始化模型参数的方法有很多。使用<code>torch.nn.init.normal_()</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。不过，PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p><h2 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h2><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化[1]。<br>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).</script><p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul><li>正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</li><li>权重衰减等价于$L_2$范数正则化，通常会使学到的权重参数的元素较接近0。</li><li>权重衰减可以通过优化器中的<code>weight_decay</code>超参数来指定。</li><li>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</li><li>我们可以通过使用丢弃法应对过拟合。</li><li>丢弃法只在训练模型时使用</li><li>深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。</li><li>我们通常需要随机初始化神经网络的模型参数，如权重参数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器翻译基础</title>
      <link href="/2020/02/16/Deep_learning/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
      <url>/2020/02/16/Deep_learning/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><p>机器翻译是指将一段文本从一种语言自动翻译到另一种语言。因为一段文本序列在不同语言中的长度不一定相同，所以我们使用机器翻译为例来介绍编码器—解码器和注意力机制的应用。</p><h2 id="读取和预处理数据"><a href="#读取和预处理数据" class="headerlink" title="读取和预处理数据"></a>读取和预处理数据</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%导入模块</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'.'</span>)</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">    out = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">            out += <span class="string">' '</span></span><br><span class="line">        out += char</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_examples = <span class="number">50000</span></span><br><span class="line">source, target = [], []</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; num_examples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">        source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">        target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        </span><br><span class="line">source[<span class="number">0</span>:<span class="number">3</span>], target[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/46ee15360205c7fc.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> d2l.data.base.Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">src_vocab = build_vocab(source)</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure><h3 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/a1e6875ba2581929.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len)</span>:</span> <span class="comment"># This function is saved in d2l.</span></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure><h2 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h2><p>可以应用在对话系统、生成式任务中。<br><img src="https://i.bmp.ovh/imgs/2020/02/e025e4ae861f8d1c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_X, dec_X, *args)</span>:</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/95d2ba1473ca6471.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens=num_hiddens</span><br><span class="line">        self.num_layers=num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),</span><br><span class="line">                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        X = self.embedding(X) <span class="comment"># X shape: (batch_size, seq_len, embed_size)</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># RNN needs first axes to be time</span></span><br><span class="line">        <span class="comment"># state = self.begin_state(X.shape[1], device=X.device)</span></span><br><span class="line">        out, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># The shape of out is (seq_len, batch_size, num_hiddens).</span></span><br><span class="line">        <span class="comment"># state contains the hidden state and the memory cell</span></span><br><span class="line">        <span class="comment"># of the last time step, the shape is (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> out, state</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        out, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Make the batch to be the first dimension to simplify loss computation.</span></span><br><span class="line">        out = self.dense(out).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure><h2 id="Beamsearch"><a href="#Beamsearch" class="headerlink" title="Beamsearch"></a>Beamsearch</h2><p><img src="attachment:image.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>循环神经网络</title>
      <link href="/2020/02/14/Deep_learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/14/Deep_learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="简单循环神经网络的构造"><a href="#简单循环神经网络的构造" class="headerlink" title="简单循环神经网络的构造"></a>简单循环神经网络的构造</h2><p><img src="https://img.vim-cn.com/a8/d90fe522138ebfb79547e687f5fd82684648fa.png" alt=""></p><h2 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h2><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量  g ，并设裁剪的阈值是 θ 。裁剪后的梯度</p><h2 id="循环神经网络的pytorch实现"><a href="#循环神经网络的pytorch实现" class="headerlink" title="循环神经网络的pytorch实现"></a>循环神经网络的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">num_steps, batch_size = <span class="number">35</span>, <span class="number">2</span></span><br><span class="line">X = torch.rand(num_steps, batch_size, vocab_size)</span><br><span class="line">state = <span class="literal">None</span></span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">print(Y.shape, state_new.shape)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># inputs.shape: (batch_size, num_steps)</span></span><br><span class="line">        X = to_onehot(inputs, vocab_size)</span><br><span class="line">        X = torch.stack(X)  <span class="comment"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class="line">        hiddens, state = self.rnn(X, state)</span><br><span class="line">        hiddens = hiddens.view(<span class="number">-1</span>, hiddens.shape[<span class="number">-1</span>])  <span class="comment"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class="line">        output = self.dense(hiddens)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]  <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        (Y, state) = model(X, state)  <span class="comment"># 前向计算不需要传入模型参数</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = RNNModel(rnn_layer, vocab_size).to(device)</span><br><span class="line">predict_rnn_pytorch(<span class="string">'分开'</span>, <span class="number">10</span>, model, vocab_size, device, idx_to_char, char_to_idx)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        state = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state[<span class="number">0</span>].detach_()</span><br><span class="line">                    state[<span class="number">1</span>].detach_()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    state.detach_()</span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><p>RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）<br>⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系<br><img src="https://img.vim-cn.com/aa/5f1857a2a2320a5a6dd637d88c7018dffcbe43.png" alt="rnn1"></p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><img src="https://img.vim-cn.com/7d/cdb99137827b9038f1e5f34593f7169bbc3d87.png" alt="gru"></p><ul><li>重置⻔有助于捕捉时间序列⾥短期的依赖关系；</li><li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul><li>长短期记忆long short-term memory :</li><li>遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入</li><li>输出门:控制从记忆细胞到隐藏状态</li><li>记忆细胞：⼀种特殊的隐藏状态的信息的流动<br><img src="https://img.vim-cn.com/de/2d40e304a6f05b02fc8747d5e2a6f947fc064a.png" alt="lstm"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="深度循环网络"><a href="#深度循环网络" class="headerlink" title="深度循环网络"></a>深度循环网络</h3><p>通过<code>num_layers</code>来进行控制</p><p><img src="https://img.vim-cn.com/d3/51bc59f0ae24e767576ee017fa06a031892f2b.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class="number">2</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="双向循环网络"><a href="#双向循环网络" class="headerlink" title="双向循环网络"></a>双向循环网络</h3><p><img src="https://img.vim-cn.com/4f/b21d208beaf51f1d33ef0455772236dc512ac0.png" alt=""></p><p>通过参数<code>bidirectional=True</code>来进行控制</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">128</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e-2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本预处理与语言模型</title>
      <link href="/2020/02/14/Deep_learning/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/02/14/Deep_learning/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><ol><li>读入文本</li><li>分词</li><li>建立字典，将每个词映射到一个唯一的索引（index）</li><li>将文本从词的序列转换为索引的序列，方便输入模型</li></ol><h2 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>将一个句子划分为若干个<code>token</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将词转化为索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure><h2 id="用现有工具包分词"><a href="#用现有工具包分词" class="headerlink" title="用现有工具包分词"></a>用现有工具包分词</h2><h3 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure><h3 id="SPACY"><a href="#SPACY" class="headerlink" title="SPACY"></a>SPACY</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure><h1 id="语言模型（基于统计的语言模型）"><a href="#语言模型（基于统计的语言模型）" class="headerlink" title="语言模型（基于统计的语言模型）"></a>语言模型（基于统计的语言模型）</h1><p><img src="https://img.vim-cn.com/74/bb98fdc35dd04377837535271dcebd321dfa19.png" alt=""></p><h1 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h1><p><img src="https://img.vim-cn.com/14/04dcd7cd184f97d2cbaed69a419d20bdd74c96.png" alt=""></p><h2 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h2><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻</p><h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机</title>
      <link href="/2020/02/13/Deep_learning/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2020/02/13/Deep_learning/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。在本节中，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。图3.3展示了一个多层感知机的神经网络图。</p><p><img src="https://img.vim-cn.com/2e/80d067a824cf71512d77c655855fe8c3488cc3.png" alt="带有隐藏层的多层感知机。它含有一个隐藏层，该层中有5个隐藏单元"></p><p>在图3.3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3.3中的多层感知机的层数为2。由图3.3可见，隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。</p><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><p>$$<br>\begin{aligned}<br>\boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\<br>\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,<br>\end{aligned}<br>$$</p><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><p>$$<br>\boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><p>$$\text{ReLU}(x) = \max(x, 0).$$</p><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数<code>xyplot</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">'..'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xyplot</span><span class="params">(x_vals,y_vals,name)</span>:</span></span><br><span class="line">    x_vals=x_vals.detach().numpy() <span class="comment"># we can't directly use var.numpy() because varibles might </span></span><br><span class="line">    y_vals=y_vals.detach().numpy() <span class="comment"># already required grad.,thus using var.detach().numpy() </span></span><br><span class="line">    plt.plot(x_vals,y_vals) </span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(name+<span class="string">'(x)'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.nn.functional.relu(x)</span><br><span class="line">xyplot(x,y,<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_2_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">"grad of relu"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_3_0.png" alt="png"></p><h3 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h3><p>sigmod函数可将元素的值变为0，1之间</p><p>$$\sigma(sigmod)= \frac{1}{1+exp^(-x)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.sigmoid(x)</span><br><span class="line">xyplot(x,y,<span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">'grad of sigmoid'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_6_0.png" alt="png"></p><h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><p>tanh函数可以将元素的值变为-1，1之间<br>$$ tanh(x) = \frac{1-exp^(-2x)}{1+exp^(-2x)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.tanh(x)</span><br><span class="line">xyplot(x,y,<span class="string">"tanh"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">"grad of tanh"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_9_0.png" alt="png"></p><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h2 id="多层感知机的pytorch实现"><a href="#多层感知机的pytorch实现" class="headerlink" title="多层感知机的pytorch实现"></a>多层感知机的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 输出如下</span></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0031</span>, train acc <span class="number">0.703</span>, test acc <span class="number">0.757</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0019</span>, train acc <span class="number">0.824</span>, test acc <span class="number">0.822</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0016</span>, train acc <span class="number">0.845</span>, test acc <span class="number">0.825</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0015</span>, train acc <span class="number">0.855</span>, test acc <span class="number">0.811</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0014</span>, train acc <span class="number">0.865</span>, test acc <span class="number">0.846</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Softmax与分类模型</title>
      <link href="/2020/02/13/Deep_learning/Softmax%E4%B8%8E%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/02/13/Deep_learning/Softmax%E4%B8%8E%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h1><p>前几节介绍的线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。</p><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为$x_1, x_2, x_3, x_4$。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值$y_1, y_2, y_3$。</p><p>我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。</p><h2 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h2><p>softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的$w$）、偏差包含3个标量（带下标的$b$），且对每个输入计算$o_1, o_2, o_3$这3个输出：</p><script type="math/tex; mode=display">\begin{aligned}o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.\end{aligned}</script><p>图3.2用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。</p><p><img src="../img/softmaxreg.svg" alt="softmax回归是一个单层神经网络"></p><h3 id="softmax运算"><a href="#softmax运算" class="headerlink" title="softmax运算"></a>softmax运算</h3><p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出$\operatorname*{argmax}_i o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。</p><p>然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</p><p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3),</script><p>其中</p><script type="math/tex; mode=display">\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad\hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad\hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><p>容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一个合法的概率分布。这时候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p><script type="math/tex; mode=display">\operatorname*{argmax}_i o_i = \operatorname*{argmax}_i \hat y_i,</script><p>因此softmax运算不改变预测类别输出。</p><h2 id="单样本分类的矢量计算表达式"><a href="#单样本分类的矢量计算表达式" class="headerlink" title="单样本分类的矢量计算表达式"></a>单样本分类的矢量计算表达式</h2><p>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</p><script type="math/tex; mode=display">\boldsymbol{W} = \begin{bmatrix}    w_{11} & w_{12} & w_{13} \\    w_{21} & w_{22} & w_{23} \\    w_{31} & w_{32} & w_{33} \\    w_{41} & w_{42} & w_{43}\end{bmatrix},\quad\boldsymbol{b} = \begin{bmatrix}    b_1 & b_2 & b_3\end{bmatrix},</script><p>设高和宽分别为2个像素的图像样本$i$的特征为</p><script type="math/tex; mode=display">\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},</script><p>输出层的输出为</p><script type="math/tex; mode=display">\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},</script><p>预测为狗、猫或鸡的概率分布为</p><script type="math/tex; mode=display">\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.</script><p>softmax回归对样本$i$分类的矢量计算表达式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\\boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}).\end{aligned}</script><h2 id="小批量样本分类的矢量计算表达式"><a href="#小批量样本分类的矢量计算表达式" class="headerlink" title="小批量样本分类的矢量计算表达式"></a>小批量样本分类的矢量计算表达式</h2><p>为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假设softmax回归的权重和偏差参数分别为$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax回归的矢量计算表达式为</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\\boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}),\end{aligned}</script><p>其中的加法运算使用了广播机制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\boldsymbol{o}^{(i)}$和概率分布$\boldsymbol{\hat{y}}^{(i)}$。</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>前面提到，使用softmax运算后可以更方便地与离散标签计算误差。我们已经知道，softmax运算将输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本$i$，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$尽可能接近真实的标签概率分布$\boldsymbol{y}^{(i)}$。</p><p>我们可以像线性回归那样使用平方损失函数$|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2$。然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\hat{y}^{(i)}_3$比其他两个预测值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了。即使$\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。</p><p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：</p><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>其中带下标的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}_{y^{(i)}}$为1，其余全为0，于是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p><p>假设训练数据集的样本数为$n$，交叉熵损失函数定义为</p><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><p>其中$\boldsymbol{\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\ell(\boldsymbol{\Theta}) = -(1/n)  \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\ell(\boldsymbol{\Theta})$等价于最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p><h1 id="softmax的pytorch实现"><a href="#softmax的pytorch实现" class="headerlink" title="softmax的pytorch实现"></a>softmax的pytorch实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载各种包或者模块</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">r"D:\Documents\learning"</span>)</span><br><span class="line"><span class="comment"># import d2lzh as d2l</span></span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络模型</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_inputs, num_outputs)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line"><span class="comment"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># x 的形状: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        <span class="comment"># FlattenLayer(),</span></span><br><span class="line">        <span class="comment"># LinearNet(num_inputs, num_outputs) </span></span><br><span class="line">        OrderedDict([</span><br><span class="line">           (<span class="string">'flatten'</span>, FlattenLayer()),</span><br><span class="line">           (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))]) <span class="comment"># 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 下面是他的函数原型</span></span><br><span class="line"><span class="comment"># class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化函数</span></span><br></pre></td></tr></table></figure><pre><code>1.3.1</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">              params=None, lr=None, trainer=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train and evaluate a model with CPU."""</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                y_hat = net(X)</span><br><span class="line">                l = loss(y_hat, y).sum()</span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> trainer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.step(batch_size)</span><br><span class="line">            y = y.astype(<span class="string">'float32'</span>)</span><br><span class="line">            train_l_sum += l.asscalar()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(axis=<span class="number">1</span>) == y).sum().asscalar()</span><br><span class="line">            n += y.size</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归</title>
      <link href="/2020/02/12/Deep_learning/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/02/12/Deep_learning/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><ul><li><p>模型<br>$$y = wx + b$$</p></li><li><p>损失函数<br>$$\ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2.$$</p></li><li><p>优化函数<br>$$<br>\begin{aligned}<br>w_1 &amp;\leftarrow w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} = w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\<br>w_2 &amp;\leftarrow w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} = w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\<br>b &amp;\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).<br>\end{aligned}<br>$$</p></li><li><p>神经网络图（单层神经网络）</p></li></ul><h2 id="线性回归的pytorch实现"><a href="#线性回归的pytorch实现" class="headerlink" title="线性回归的pytorch实现"></a>线性回归的pytorch实现</h2><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.normal(mean=torch.zeros(labels.shape), std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data <span class="keyword">as</span> tdata</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = tdata.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = tdata.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.2115,  1.4861],        [-0.2630,  0.8898],        [ 0.8301, -2.6101],        [ 1.5199, -0.5050],        [-0.4478,  0.6990],        [ 1.4203,  1.1574],        [ 1.3185,  1.1949],        [ 2.0129,  0.8379],        [ 1.1585, -0.1882],        [ 0.9050,  0.0398]]) tensor([-0.4229,  0.6551, 14.7292,  8.9412,  0.9225,  3.1058,  2.7778,  5.3856,         7.1542,  5.8629])</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>先导入<code>nn</code>模块。实际上，<code>“nn”</code>是<code>neural networks</code>（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。我们先定义一个模型变量<code>net</code>，它是一个<code>Sequential</code>实例。在<code>nn</code>中，<code>Sequential</code>实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 全连接层是一个线性层，特征数为2，输出个数为1</span></span><br><span class="line">net.add_module(<span class="string">'linear'</span>, nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>这里主要是初始化线性回归模型中的权重与偏差。使用<code>nn.init</code>模块<br>如<code>nn.init.normal(tensor, std=0.01)</code>指定随机初始化将随机采样均值为0、标准差为0.01的正态分布。偏差初始化默认为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">params_init</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(model, nn.Linear):</span><br><span class="line">        init.normal_(tensor=model.weight.data, std=<span class="number">0.01</span>)</span><br><span class="line">        init.constant_(tensor=model.bias.data, val=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">net.apply(params_init)</span><br></pre></td></tr></table></figure><pre><code>Sequential(  (linear): Linear(in_features=2, out_features=1, bias=True))</code></pre><h3 id="定义损失函数与优化算法"><a href="#定义损失函数与优化算法" class="headerlink" title="定义损失函数与优化算法"></a>定义损失函数与优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss() <span class="comment"># 均方误差损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)  <span class="comment"># lr为学习率</span></span><br></pre></td></tr></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span> <span class="comment"># 初始化训练周期</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        net.zero_grad()</span><br><span class="line">        l = loss(net(X), y.reshape(batch_size, <span class="number">-1</span>))  <span class="comment"># -1表示自动计算列</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        l = loss(net(features), labels.reshape(num_examples, <span class="number">-1</span>))</span><br><span class="line">        print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.data.numpy()))</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss: 0.000093epoch 2, loss: 0.000094epoch 3, loss: 0.000094epoch 4, loss: 0.000093epoch 5, loss: 0.000093</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear = net[<span class="number">0</span>]</span><br><span class="line">true_w, linear.weight.data</span><br></pre></td></tr></table></figure><pre><code>([2, -3.4], tensor([[ 2.0001, -3.4001]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">true_b, linear.bias.data</span><br></pre></td></tr></table></figure><pre><code>(4.2, tensor([4.2001]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net</span><br></pre></td></tr></table></figure><pre><code>Sequential(  (linear): Linear(in_features=2, out_features=1, bias=True))</code></pre>]]></content>
      
      
      <categories>
          
          <category> deep_learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MYSQL安装配置</title>
      <link href="/2020/01/16/SQL/MySQL%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/01/16/SQL/MySQL%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL的安装与配置"><a href="#MySQL的安装与配置" class="headerlink" title="MySQL的安装与配置"></a>MySQL的安装与配置</h1><h2 id="下载软件与安装"><a href="#下载软件与安装" class="headerlink" title="下载软件与安装"></a>下载软件与安装</h2><ol><li>到<code>mySQL</code>的官网：<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">点击这里</a>，看到下图，下载即可。</li><li>解压缩到你所需要的路径<h2 id="MySQL配置"><a href="#MySQL配置" class="headerlink" title="MySQL配置"></a>MySQL配置</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[mysqld]</span></span><br><span class="line"><span class="comment"># 设置3306端口</span></span><br><span class="line"><span class="string">port=3306</span></span><br><span class="line"><span class="comment"># 设置mysql的安装目录</span></span><br><span class="line"><span class="string">basedir=&lt;你的路径&gt;\mysql-8.0.11-winx64</span></span><br><span class="line"><span class="comment"># 设置mysql数据库的数据的存放目录</span></span><br><span class="line"><span class="string">datadir=&lt;你的路径&gt;\mysql-8.0.11-winx64\\data</span></span><br><span class="line"><span class="comment"># 允许最大连接数</span></span><br><span class="line"><span class="string">max_connections=200</span></span><br><span class="line"><span class="comment"># 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统</span></span><br><span class="line"><span class="string">max_connect_errors=10000</span></span><br><span class="line"><span class="comment"># 服务端使用的字符集默认为UTF8</span></span><br><span class="line"><span class="string">character-set-server=utf8</span></span><br><span class="line"><span class="comment"># 创建新表时将使用的默认存储引擎</span></span><br><span class="line"><span class="string">default-storage-engine=INNODB</span></span><br><span class="line"><span class="string">wait_timeout=31536000</span></span><br><span class="line"><span class="string">interactive_timeout=31536000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sql_mode=ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION</span></span><br><span class="line"><span class="string">[mysql]</span></span><br><span class="line"><span class="comment"># 设置mysql客户端默认字符集</span></span><br><span class="line"><span class="string">default-character-set=utf8</span></span><br><span class="line"></span><br><span class="line"><span class="string">[client]</span></span><br><span class="line"><span class="comment"># 设置mysql客户端连接服务端时默认使用的端口</span></span><br><span class="line"><span class="string">port=3306</span></span><br><span class="line"><span class="string">default-character-set=utf8</span></span><br></pre></td></tr></table></figure><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2></li><li><p>系统变量添加</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">MYSQL_HOME  :</span> <span class="string">path</span></span><br></pre></td></tr></table></figure></li><li><p>环境变量添加</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">%MYSQL_HOME%\bin</span></span><br></pre></td></tr></table></figure></li><li>如果上面2个方法都不想使用，可以使用下面这个方法：<br><img src="https://img-blog.csdn.net/20180416193135760?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3Nzg4MzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></li></ol><h2 id="MySQL初始化"><a href="#MySQL初始化" class="headerlink" title="MySQL初始化"></a>MySQL初始化</h2><ol><li>CMD运行<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mysqld --initialize --user=mysql --console</span><br></pre></td></tr></table></figure><img src="https://img-blog.csdn.net/20180416190435744?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3Nzg4MzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""><br>务必记住这个初始密码，一会需要用这个初始密码登录mysql；<br>修改密码如果emm你把他点没了 你只要把datadir配置的那个data的文件删除了，然后重新执行初始化即可然后输入：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld --install</span><br></pre></td></tr></table></figure>再打开服务：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net start mysql</span><br></pre></td></tr></table></figure></li></ol><h2 id="修改默认密码"><a href="#修改默认密码" class="headerlink" title="修改默认密码"></a>修改默认密码</h2><p>执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><br>登录,输入初始密码，接下来修改密码：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER USER <span class="string">"root@"</span>localhost<span class="string">" IDENTIFIED BY "</span>your password<span class="string">"</span></span><br></pre></td></tr></table></figure></p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>如果你在数据库连接工具的时候出现错误，则可以再<code>my.ini</code>文件中的<code>mysqld</code>中加入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ default_authentication_plugin=mysql_native_password</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL语言基础</title>
      <link href="/2020/01/15/SQL/SQL%E8%AF%AD%E8%A8%80%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2020/01/15/SQL/SQL%E8%AF%AD%E8%A8%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="SQL语言"><a href="#SQL语言" class="headerlink" title="SQL语言"></a>SQL语言</h1><p>简单来说，你需要在数据库上执行的操作大部分都要由SQL语句完成<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites</span><br></pre></td></tr></table></figure><br>但是，SQL语句并不对大小写敏感，同时我们在本例教程中，每个语句都加上分号<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use RUNOOB;</span><br><span class="line">Database changed</span><br><span class="line"></span><br><span class="line">mysql&gt; set names utf8;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; SELECT * FROM Websites;</span><br><span class="line">+<span class="comment">----+--------------+---------------------------+-------+---------+</span></span><br><span class="line">| id | name         | url                       | alexa | country |</span><br><span class="line">+<span class="comment">----+--------------+---------------------------+-------+---------+</span></span><br><span class="line">| 1  | Google       | https://www.google.cm/    | 1     | USA     |</span><br><span class="line">| 2  | 淘宝          | https://www.taobao.com/   | 13    | CN      |</span><br><span class="line">| 3  | 菜鸟教程      | http://www.runoob.com/    | 4689  | CN      |</span><br><span class="line">| 4  | 微博          | http://weibo.com/         | 20    | CN      |</span><br><span class="line">| 5  | Facebook     | https://www.facebook.com/ | 3     | USA     |</span><br><span class="line">+<span class="comment">----+--------------+---------------------------+-------+---------+</span></span><br><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><br>下面的大部分都是基于这个表格的</p><h2 id="SQL-SELECT-语句"><a href="#SQL-SELECT-语句" class="headerlink" title="SQL SELECT 语句"></a>SQL SELECT 语句</h2><p>SELECT语句用于从数据库中选取数据，结果被存在一个结果表中，称为结果集</p><h3 id="SELECT-语法"><a href="#SELECT-语法" class="headerlink" title="SELECT 语法"></a>SELECT 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name,column_name <span class="keyword">FROM</span> table_name;  <span class="comment"># 选出指定列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> table_name;  <span class="comment"># 选出所有的列</span></span><br></pre></td></tr></table></figure><h2 id="SQL-SELECT-DISTINCT-语句"><a href="#SQL-SELECT-DISTINCT-语句" class="headerlink" title="SQL SELECT DISTINCT 语句"></a>SQL SELECT DISTINCT 语句</h2><p>在表中，一个列可能会包含多个重复值，DISTINCT关键词用于返回唯一不同的值,就是会删掉重复的值</p><h3 id="SQL-SELECT-DISTINCT-语法"><a href="#SQL-SELECT-DISTINCT-语法" class="headerlink" title="SQL SELECT DISTINCT 语法"></a>SQL SELECT DISTINCT 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> column_name, column_name <span class="keyword">FROM</span> table_name;</span><br></pre></td></tr></table></figure><h2 id="SELECT-WHERE子句"><a href="#SELECT-WHERE子句" class="headerlink" title="SELECT WHERE子句"></a>SELECT WHERE子句</h2><p>WHERE子句用于提取那些阿玛尼组指定条件的记录</p><h3 id="SQL-WHERE-语法"><a href="#SQL-WHERE-语法" class="headerlink" title="SQL WHERE 语法"></a>SQL WHERE 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name,column_name <span class="keyword">FROM</span> table_name <span class="keyword">WHERE</span> column_name <span class="keyword">operator</span> <span class="keyword">value</span>;</span><br></pre></td></tr></table></figure><h3 id="文本字段-vs-数值字段"><a href="#文本字段-vs-数值字段" class="headerlink" title="文本字段 vs 数值字段"></a>文本字段 vs 数值字段</h3><p>SQL中使用<strong>单引号</strong>来环绕文本数值，但是如果是数值就不用使用单引号了，如：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">WHERE</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">WHERE</span> country = <span class="string">'CN'</span>;</span><br></pre></td></tr></table></figure></p><h3 id="WHERE子句的运算符"><a href="#WHERE子句的运算符" class="headerlink" title="WHERE子句的运算符"></a>WHERE子句的运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>&lt;&gt;或者!=</td><td>不等于</td></tr><tr><td>BETWEEN</td><td>在某个范围内</td></tr><tr><td>LIKE</td><td>搜索某种模式</td></tr><tr><td>IN</td><td>指定针对某个列的可能值</td></tr></tbody></table></div><h2 id="SQL-AND-amp-OR-运算符"><a href="#SQL-AND-amp-OR-运算符" class="headerlink" title="SQL AND &amp; OR 运算符"></a>SQL AND &amp; OR 运算符</h2><p>AND &amp; OR 运算符基于一个以上条件对记录进行过滤  </p><ul><li>如果第一个条件与第二个条件都成立，则ADN运算符显示一条记录</li><li>如果第一个条件与第二个条件有一个成立，则OR运算符显示一条记录</li></ul><h3 id="SQL-AND-amp-OR-语法"><a href="#SQL-AND-amp-OR-语法" class="headerlink" title="SQL AND &amp; OR 语法"></a>SQL AND &amp; OR 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Websites <span class="keyword">WHERE</span> country = <span class="string">'CN'</span> <span class="keyword">AND</span> alexa &gt; <span class="number">50</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">WHERE</span> country = <span class="string">'USA'</span> <span class="keyword">OR</span> country = <span class="string">'CN'</span>;</span><br></pre></td></tr></table></figure><h3 id="结合AND-和-OR"><a href="#结合AND-和-OR" class="headerlink" title="结合AND 和 OR"></a>结合AND 和 OR</h3><p>用法示例如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">WHERE</span> alexa &gt; <span class="number">15</span> </span><br><span class="line"><span class="keyword">AND</span> (country = <span class="string">'CN'</span> <span class="keyword">OR</span> country = <span class="string">'USA'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="SQL-ORDER-BY-关键词"><a href="#SQL-ORDER-BY-关键词" class="headerlink" title="SQL ORDER BY 关键词"></a>SQL ORDER BY 关键词</h2><p>ORDER BY 关键词用于对于结果集按照一个列或者多个列进行排序</p><h3 id="SQL-ORDER-BY-的语法"><a href="#SQL-ORDER-BY-的语法" class="headerlink" title="SQL ORDER BY 的语法"></a>SQL ORDER BY 的语法</h3><p>ORDER BY关键词默认按照升序进行排列，如果需要降序，则可以使用DESC关键字<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> column_name, column_name <span class="keyword">FROM</span> table_name </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> column_name, column_name <span class="keyword">ASC</span>|<span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></p><h3 id="SQL-ORDER-BY-DESC-实例"><a href="#SQL-ORDER-BY-DESC-实例" class="headerlink" title="SQL ORDER BY(DESC)实例"></a>SQL ORDER BY(DESC)实例</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">ORDER</span> <span class="keyword">BY</span> alexa ;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">ORDER</span> <span class="keyword">BY</span> alexa <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><h3 id="ORDER-BY-多列"><a href="#ORDER-BY-多列" class="headerlink" title="ORDER BY 多列"></a>ORDER BY 多列</h3><p>当有多列进行排序时，我们优先选择第一列<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> websites <span class="keyword">ORDER</span> <span class="keyword">BY</span> country,alexa;</span><br></pre></td></tr></table></figure></p><h2 id="SQL-INSERT-INTO语句"><a href="#SQL-INSERT-INTO语句" class="headerlink" title="SQL INSERT INTO语句"></a>SQL INSERT INTO语句</h2><p>INSERT INTO语句向表中插入新纪录</p><h3 id="SQL-INSERT-INTO语句的语法"><a href="#SQL-INSERT-INTO语句的语法" class="headerlink" title="SQL INSERT INTO语句的语法"></a>SQL INSERT INTO语句的语法</h3><p>INSERT 语句有两种编写形式：</p><ul><li>第一钟形式无需指定要插入数据的列名，只需要提供被插入的值即可：<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_name <span class="keyword">VALUES</span> (value1,value2,value3);</span><br></pre></td></tr></table></figure></li><li>第二种形式需要指定列名及被插入的值<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> table_name (column1, column2,column3) <span class="keyword">VALUES</span> (value1,value2,value3)</span><br></pre></td></tr></table></figure><h3 id="INSERT-INTO实例"><a href="#INSERT-INTO实例" class="headerlink" title="INSERT INTO实例"></a>INSERT INTO实例</h3>假设我们需要向‘websites’表钟插入一个新行<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> websites(<span class="keyword">name</span>, <span class="keyword">url</span>, alexa, country) <span class="keyword">VALUES</span>(<span class="string">'百度’, '</span>https://www.baidu.com/<span class="string">','</span><span class="number">4</span><span class="string">','</span>CN<span class="string">');</span></span><br></pre></td></tr></table></figure><h3 id="在指定的列插入数据"><a href="#在指定的列插入数据" class="headerlink" title="在指定的列插入数据"></a>在指定的列插入数据</h3>下面的SQL语句将插入一个新行，但是只在’name’、‘url’、’country‘列插入数据<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> websites(<span class="keyword">name</span>, <span class="keyword">url</span>, country) <span class="keyword">VALUES</span> (<span class="string">'stackoverflow'</span>, <span class="string">'http://stackoverflow.com/'</span>, <span class="string">'IND'</span>)</span><br></pre></td></tr></table></figure><h2 id="SQL-UPDATE-语句"><a href="#SQL-UPDATE-语句" class="headerlink" title="SQL UPDATE 语句"></a>SQL UPDATE 语句</h2>UPDATE语句用于更新表中已经存在的记录</li></ul><h3 id="UPDATE语句的语法"><a href="#UPDATE语句的语法" class="headerlink" title="UPDATE语句的语法"></a>UPDATE语句的语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> table_name <span class="keyword">SET</span> <span class="keyword">column</span> = value1, column2 = value2,... <span class="keyword">WHERE</span> some_column = some_value;</span><br></pre></td></tr></table></figure><h3 id="UPDATE实例"><a href="#UPDATE实例" class="headerlink" title="UPDATE实例"></a>UPDATE实例</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPADATE Websites <span class="keyword">SET</span> alexa = <span class="string">'500'</span>, country = <span class="string">'USA'</span> <span class="keyword">WHERE</span> <span class="keyword">name</span> = <span class="string">'菜鸟教程’;</span></span><br></pre></td></tr></table></figure><h2 id="SQL-DELETE"><a href="#SQL-DELETE" class="headerlink" title="SQL DELETE"></a>SQL DELETE</h2><p>DELET语句用于删除表中的行</p><h3 id="SQL-DELETE语法和实例"><a href="#SQL-DELETE语法和实例" class="headerlink" title="SQL DELETE语法和实例"></a>SQL DELETE语法和实例</h3><p>从表中删除网站名是百度且国家为CN的网站<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELET FROM websites WHERE name = '百度' AND country = 'CN';</span><br></pre></td></tr></table></figure></p><h3 id="删除所有数据"><a href="#删除所有数据" class="headerlink" title="删除所有数据"></a>删除所有数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> table_name;</span><br><span class="line"><span class="keyword">DELETE</span> * <span class="keyword">FROM</span> table_name;</span><br></pre></td></tr></table></figure><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>非常感谢菜鸟教程，本文借鉴了菜鸟教程的表格和相关代码：<a href="https://www.runoob.com/sql/sql-syntax.html" target="_blank" rel="noopener">菜鸟教程</a>: <a href="https://www.runoob.com/sql/sql-syntax.html" target="_blank" rel="noopener">https://www.runoob.com/sql/sql-syntax.html</a></p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（八）- K均值聚类</title>
      <link href="/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888-K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%89/"/>
      <url>/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888-K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="K均值聚类"><a href="#K均值聚类" class="headerlink" title="K均值聚类"></a>K均值聚类</h1><p>算法伪代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为起始质心（经常是随机选择）</span><br><span class="line">当任意一个点的簇分配结果发生改变时</span><br><span class="line">    对数据集中的每个数据点</span><br><span class="line">        对每个质心</span><br><span class="line">            计算质心与数据点之间的距离</span><br><span class="line">        将数据点分配到距离其最近的簇</span><br><span class="line">    对每一个簇，计算簇中所有点的均值，并且将该值作为质心</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：k-means聚类算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 用于聚类的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    distMeas - 距离计算方法,默认欧氏距离distEclud()</span></span><br><span class="line"><span class="string">    createCent - 获取k个质心的方法,默认随机获取randCent()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centroids - k个聚类的聚类结果</span></span><br><span class="line"><span class="string">    clusterAssment - 聚类误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据集样本数</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化一个（m,2）全零矩阵</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 创建初始的k个质心向量</span></span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    <span class="comment"># 聚类结果是否发生变化的布尔类型</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        <span class="comment"># 聚类结果变化布尔类型置为False</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 遍历数据集每一个样本向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 初始化最小距离为正无穷，最小距离对应的索引为-1</span></span><br><span class="line">            minDist = float(<span class="string">'inf'</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="comment"># 循环k个类的质心</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                <span class="comment"># 计算数据点到质心的欧氏距离</span></span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">                <span class="comment"># 如果距离小于当前最小距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    <span class="comment"># 当前距离为最小距离，最小距离对应索引应为j(第j个类)</span></span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="comment"># 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法</span></span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex: </span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 更新当前变化样本的聚类结果和平方误差</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">            <span class="comment"># 打印k-means聚类的质心</span></span><br><span class="line">        <span class="comment"># print(centroids)</span></span><br><span class="line">        <span class="comment"># 遍历每一个质心</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            <span class="comment"># 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来</span></span><br><span class="line">            ptsInClust = dataSet[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]]</span><br><span class="line">            <span class="comment"># 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量</span></span><br><span class="line">            centroids[cent, :] = np.mean(ptsInClust, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 返回k个聚类，聚类结果及误差</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Thu Aug  2 21:20:03 2018</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: wzy</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：将文本文档中的数据读入到python中</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float, curLine))</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：数据向量计算欧式距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    vecA - 数据向量A</span></span><br><span class="line"><span class="string">    vecB - 数据向量B</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    两个向量之间的欧几里德距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-08-02</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：随机初始化k个质心（质心满足数据边界之内）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 输入的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centroids - 返回初始化得到的k个质心向量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    <span class="comment"># 得到数据样本的维度</span></span><br><span class="line">    n = np.shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化为一个(k,n)的全零矩阵</span></span><br><span class="line">    centroids = np.mat(np.zeros((k, n)))</span><br><span class="line">    <span class="comment"># 遍历数据集的每一个维度</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 得到该列数据的最小值,最大值</span></span><br><span class="line">        minJ = np.min(dataSet[:, j])</span><br><span class="line">        maxJ = np.max(dataSet[:, j])</span><br><span class="line">        <span class="comment"># 得到该列数据的范围(最大值-最小值)</span></span><br><span class="line">        rangeJ = float(maxJ - minJ)</span><br><span class="line">        <span class="comment"># k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值</span></span><br><span class="line">        <span class="comment"># Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).</span></span><br><span class="line">        centroids[:, j] = minJ + rangeJ * np.random.rand(k, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 返回初始化得到的k个质心向量</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：k-means聚类算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 用于聚类的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    distMeas - 距离计算方法,默认欧氏距离distEclud()</span></span><br><span class="line"><span class="string">    createCent - 获取k个质心的方法,默认随机获取randCent()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centroids - k个聚类的聚类结果</span></span><br><span class="line"><span class="string">    clusterAssment - 聚类误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据集样本数</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化一个（m,2）全零矩阵</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 创建初始的k个质心向量</span></span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    <span class="comment"># 聚类结果是否发生变化的布尔类型</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        <span class="comment"># 聚类结果变化布尔类型置为False</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 遍历数据集每一个样本向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 初始化最小距离为正无穷，最小距离对应的索引为-1</span></span><br><span class="line">            minDist = float(<span class="string">'inf'</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="comment"># 循环k个类的质心</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                <span class="comment"># 计算数据点到质心的欧氏距离</span></span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">                <span class="comment"># 如果距离小于当前最小距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    <span class="comment"># 当前距离为最小距离，最小距离对应索引应为j(第j个类)</span></span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="comment"># 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法</span></span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex: </span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 更新当前变化样本的聚类结果和平方误差</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">            <span class="comment"># 打印k-means聚类的质心</span></span><br><span class="line">        <span class="comment"># print(centroids)</span></span><br><span class="line">        <span class="comment"># 遍历每一个质心</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            <span class="comment"># 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来</span></span><br><span class="line">            ptsInClust = dataSet[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]]</span><br><span class="line">            <span class="comment"># 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量</span></span><br><span class="line">            centroids[cent, :] = np.mean(ptsInClust, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 返回k个聚类，聚类结果及误差</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：绘制数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotDataSet</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="comment"># 导入数据</span></span><br><span class="line">    datMat = np.mat(loadDataSet(filename))</span><br><span class="line">    <span class="comment"># 进行k-means算法其中k为4</span></span><br><span class="line">    myCentroids, clustAssing = kMeans(datMat, <span class="number">4</span>)</span><br><span class="line">    clustAssing = clustAssing.tolist()</span><br><span class="line">    myCentroids = myCentroids.tolist()</span><br><span class="line">    xcord = [[], [], [], []]</span><br><span class="line">    ycord = [[], [], [], []]</span><br><span class="line">    datMat = datMat.tolist()</span><br><span class="line">    m = len(clustAssing)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">if</span> int(clustAssing[i][<span class="number">0</span>]) == <span class="number">0</span>:</span><br><span class="line">            xcord[<span class="number">0</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">0</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> int(clustAssing[i][<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">            xcord[<span class="number">1</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">1</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> int(clustAssing[i][<span class="number">0</span>]) == <span class="number">2</span>:</span><br><span class="line">            xcord[<span class="number">2</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">2</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> int(clustAssing[i][<span class="number">0</span>]) == <span class="number">3</span>:</span><br><span class="line">            xcord[<span class="number">3</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">3</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># 绘制样本点</span></span><br><span class="line">    ax.scatter(xcord[<span class="number">0</span>], ycord[<span class="number">0</span>], s=<span class="number">20</span>, c=<span class="string">'b'</span>, marker=<span class="string">'*'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(xcord[<span class="number">1</span>], ycord[<span class="number">1</span>], s=<span class="number">20</span>, c=<span class="string">'r'</span>, marker=<span class="string">'D'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(xcord[<span class="number">2</span>], ycord[<span class="number">2</span>], s=<span class="number">20</span>, c=<span class="string">'c'</span>, marker=<span class="string">'&gt;'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(xcord[<span class="number">3</span>], ycord[<span class="number">3</span>], s=<span class="number">20</span>, c=<span class="string">'k'</span>, marker=<span class="string">'o'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 绘制质心</span></span><br><span class="line">    ax.scatter(myCentroids[<span class="number">0</span>][<span class="number">0</span>], myCentroids[<span class="number">0</span>][<span class="number">1</span>], s=<span class="number">100</span>, c=<span class="string">'k'</span>, marker=<span class="string">'+'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(myCentroids[<span class="number">1</span>][<span class="number">0</span>], myCentroids[<span class="number">1</span>][<span class="number">1</span>], s=<span class="number">100</span>, c=<span class="string">'k'</span>, marker=<span class="string">'+'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(myCentroids[<span class="number">2</span>][<span class="number">0</span>], myCentroids[<span class="number">2</span>][<span class="number">1</span>], s=<span class="number">100</span>, c=<span class="string">'k'</span>, marker=<span class="string">'+'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(myCentroids[<span class="number">3</span>][<span class="number">0</span>], myCentroids[<span class="number">3</span>][<span class="number">1</span>], s=<span class="number">100</span>, c=<span class="string">'k'</span>, marker=<span class="string">'+'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    plt.title(<span class="string">'DataSet'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    plotDataSet(<span class="string">'testSet.txt'</span>)</span><br></pre></td></tr></table></figure><h1 id="二分K均值聚类"><a href="#二分K均值聚类" class="headerlink" title="二分K均值聚类"></a>二分K均值聚类</h1><p>算法伪代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将所有点看成一个簇</span><br><span class="line">当簇数目小于K时</span><br><span class="line">    对每一个簇</span><br><span class="line">        计算总误差</span><br><span class="line">        在给定的簇上面进行K-均值聚类（k&#x3D;2)</span><br><span class="line">        计算将该簇一分为二之后的总误差</span><br><span class="line">    选择使得误差最小的那个簇进行划分操作</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：二分k-means聚类算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 用于聚类的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    distMeas - 距离计算方法,默认欧氏距离distEclud()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centList - k个聚类的聚类结果</span></span><br><span class="line"><span class="string">    clusterAssment - 聚类误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据集的样本数</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化一个元素均值0的(m, 2)矩阵</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 获取数据集每一列数据的均值，组成一个列表</span></span><br><span class="line">    centroid0 = np.mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 当前聚类列表为将数据集聚为一类</span></span><br><span class="line">    centList = [centroid0]</span><br><span class="line">    <span class="comment"># 遍历每个数据集样本</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># 计算当前聚为一类时各个数据点距离质心的平方距离</span></span><br><span class="line">        clusterAssment[j, <span class="number">1</span>] = distMeas(np.mat(centroid0), dataSet[j, :])**<span class="number">2</span></span><br><span class="line">    <span class="comment"># 循环，直至二分k-Means值达到k类为止</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):</span><br><span class="line">        <span class="comment"># 将当前最小平方误差置为正无穷</span></span><br><span class="line">        lowerSSE = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="comment"># 遍历当前每个聚类</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">            <span class="comment"># 通过数组过滤筛选出属于第i类的数据集合</span></span><br><span class="line">            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == i)[<span class="number">0</span>], :]</span><br><span class="line">            <span class="comment"># 对该类利用二分k-means算法进行划分，返回划分后的结果以及误差</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)</span><br><span class="line">            <span class="comment"># 计算该类划分后两个类的误差平方和</span></span><br><span class="line">            sseSplit = np.sum(splitClustAss[:, <span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 计算数据集中不属于该类的数据的误差平方和</span></span><br><span class="line">            sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, <span class="number">0</span>].A != i)[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 打印这两项误差值</span></span><br><span class="line">            print(<span class="string">'sseSplit = %f, and notSplit = %f'</span> % (sseSplit, sseNotSplit))</span><br><span class="line">            <span class="comment"># 划分第i类后总误差小于当前最小总误差</span></span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowerSSE:</span><br><span class="line">                <span class="comment"># 第i类作为本次划分类</span></span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                <span class="comment"># 第i类划分后得到的两个质心向量</span></span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                <span class="comment"># 复制第i类中数据点的聚类结果即误差值</span></span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                <span class="comment"># 将划分第i类后的总误差作为当前最小误差</span></span><br><span class="line">                lowerSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 数组过滤选出本次2-means聚类划分后类编号为1数据点，将这些数据点类编号变为</span></span><br><span class="line">        <span class="comment"># 当前类个数+1， 作为新的一个聚类</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>], <span class="number">0</span>] = len(centList)</span><br><span class="line">        <span class="comment"># 同理，将划分数据中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号</span></span><br><span class="line">        <span class="comment"># 连续不出现空缺</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>], <span class="number">0</span>] = bestCentToSplit</span><br><span class="line">        <span class="comment"># 打印本次执行2-means聚类算法的类</span></span><br><span class="line">        print(<span class="string">'the bestCentToSplit is %d'</span> % bestCentToSplit)</span><br><span class="line">        <span class="comment"># 打印被划分的类的数据个数</span></span><br><span class="line">        print(<span class="string">'the len of bestClustAss is %d'</span> % len(bestClustAss))</span><br><span class="line">        <span class="comment"># 更新质心列表中变化后的质心向量</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>, :]</span><br><span class="line">        <span class="comment"># 添加新的类的质心向量</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>, :])</span><br><span class="line">        <span class="comment"># 更新clusterAssment列表中参与2-means聚类数据点变化后的分类编号，及数据该类的误差平方</span></span><br><span class="line">        clusterAssment[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>], :] = bestClustAss</span><br><span class="line">    <span class="comment"># 返回聚类结果</span></span><br><span class="line">    <span class="keyword">return</span> centList, clusterAssment</span><br></pre></td></tr></table></figure><h2 id="源代码-1"><a href="#源代码-1" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Fri Aug  3 13:53:40 2018</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: wzy</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：将文本文档中的数据读入到python中</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = list(map(float, curLine))</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：数据向量计算欧式距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    vecA - 数据向量A</span></span><br><span class="line"><span class="string">    vecB - 数据向量B</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    两个向量之间的欧几里德距离</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.sum(np.power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：随机初始化k个质心（质心满足数据边界之内）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 输入的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centroids - 返回初始化得到的k个质心向量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    <span class="comment"># 得到数据样本的维度</span></span><br><span class="line">    n = np.shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化为一个(k,n)的全零矩阵</span></span><br><span class="line">    centroids = np.mat(np.zeros((k, n)))</span><br><span class="line">    <span class="comment"># 遍历数据集的每一个维度</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 得到该列数据的最小值,最大值</span></span><br><span class="line">        minJ = np.min(dataSet[:, j])</span><br><span class="line">        maxJ = np.max(dataSet[:, j])</span><br><span class="line">        <span class="comment"># 得到该列数据的范围(最大值-最小值)</span></span><br><span class="line">        rangeJ = float(maxJ - minJ)</span><br><span class="line">        <span class="comment"># k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值</span></span><br><span class="line">        <span class="comment"># Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).</span></span><br><span class="line">        centroids[:, j] = minJ + rangeJ * np.random.rand(k, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 返回初始化得到的k个质心向量</span></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：k-means聚类算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 用于聚类的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    distMeas - 距离计算方法,默认欧氏距离distEclud()</span></span><br><span class="line"><span class="string">    createCent - 获取k个质心的方法,默认随机获取randCent()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centroids - k个聚类的聚类结果</span></span><br><span class="line"><span class="string">    clusterAssment - 聚类误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据集样本数</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化一个（m,2）全零矩阵</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 创建初始的k个质心向量</span></span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    <span class="comment"># 聚类结果是否发生变化的布尔类型</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        <span class="comment"># 聚类结果变化布尔类型置为False</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 遍历数据集每一个样本向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 初始化最小距离为正无穷，最小距离对应的索引为-1</span></span><br><span class="line">            minDist = float(<span class="string">'inf'</span>)</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="comment"># 循环k个类的质心</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">                <span class="comment"># 计算数据点到质心的欧氏距离</span></span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">                <span class="comment"># 如果距离小于当前最小距离</span></span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    <span class="comment"># 当前距离为最小距离，最小距离对应索引应为j(第j个类)</span></span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="comment"># 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法</span></span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex: </span><br><span class="line">                clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 更新当前变化样本的聚类结果和平方误差</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">            <span class="comment"># 打印k-means聚类的质心</span></span><br><span class="line">        <span class="comment"># print(centroids)</span></span><br><span class="line">        <span class="comment"># 遍历每一个质心</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            <span class="comment"># 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来</span></span><br><span class="line">            ptsInClust = dataSet[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]]</span><br><span class="line">            <span class="comment"># 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量</span></span><br><span class="line">            centroids[cent, :] = np.mean(ptsInClust, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 返回k个聚类，聚类结果及误差</span></span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：二分k-means聚类算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 用于聚类的数据集</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    distMeas - 距离计算方法,默认欧氏距离distEclud()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    centList - k个聚类的聚类结果</span></span><br><span class="line"><span class="string">    clusterAssment - 聚类误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据集的样本数</span></span><br><span class="line">    m = np.shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 初始化一个元素均值0的(m, 2)矩阵</span></span><br><span class="line">    clusterAssment = np.mat(np.zeros((m, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 获取数据集每一列数据的均值，组成一个列表</span></span><br><span class="line">    centroid0 = np.mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 当前聚类列表为将数据集聚为一类</span></span><br><span class="line">    centList = [centroid0]</span><br><span class="line">    <span class="comment"># 遍历每个数据集样本</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># 计算当前聚为一类时各个数据点距离质心的平方距离</span></span><br><span class="line">        clusterAssment[j, <span class="number">1</span>] = distMeas(np.mat(centroid0), dataSet[j, :])**<span class="number">2</span></span><br><span class="line">    <span class="comment"># 循环，直至二分k-Means值达到k类为止</span></span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):</span><br><span class="line">        <span class="comment"># 将当前最小平方误差置为正无穷</span></span><br><span class="line">        lowerSSE = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="comment"># 遍历当前每个聚类</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">            <span class="comment"># 通过数组过滤筛选出属于第i类的数据集合</span></span><br><span class="line">            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == i)[<span class="number">0</span>], :]</span><br><span class="line">            <span class="comment"># 对该类利用二分k-means算法进行划分，返回划分后的结果以及误差</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)</span><br><span class="line">            <span class="comment"># 计算该类划分后两个类的误差平方和</span></span><br><span class="line">            sseSplit = np.sum(splitClustAss[:, <span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 计算数据集中不属于该类的数据的误差平方和</span></span><br><span class="line">            sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, <span class="number">0</span>].A != i)[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 打印这两项误差值</span></span><br><span class="line">            print(<span class="string">'sseSplit = %f, and notSplit = %f'</span> % (sseSplit, sseNotSplit))</span><br><span class="line">            <span class="comment"># 划分第i类后总误差小于当前最小总误差</span></span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowerSSE:</span><br><span class="line">                <span class="comment"># 第i类作为本次划分类</span></span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                <span class="comment"># 第i类划分后得到的两个质心向量</span></span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                <span class="comment"># 复制第i类中数据点的聚类结果即误差值</span></span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                <span class="comment"># 将划分第i类后的总误差作为当前最小误差</span></span><br><span class="line">                lowerSSE = sseSplit + sseNotSplit</span><br><span class="line">        <span class="comment"># 数组过滤选出本次2-means聚类划分后类编号为1数据点，将这些数据点类编号变为</span></span><br><span class="line">        <span class="comment"># 当前类个数+1， 作为新的一个聚类</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>], <span class="number">0</span>] = len(centList)</span><br><span class="line">        <span class="comment"># 同理，将划分数据中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号</span></span><br><span class="line">        <span class="comment"># 连续不出现空缺</span></span><br><span class="line">        bestClustAss[np.nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>], <span class="number">0</span>] = bestCentToSplit</span><br><span class="line">        <span class="comment"># 打印本次执行2-means聚类算法的类</span></span><br><span class="line">        print(<span class="string">'the bestCentToSplit is %d'</span> % bestCentToSplit)</span><br><span class="line">        <span class="comment"># 打印被划分的类的数据个数</span></span><br><span class="line">        print(<span class="string">'the len of bestClustAss is %d'</span> % len(bestClustAss))</span><br><span class="line">        <span class="comment"># 更新质心列表中变化后的质心向量</span></span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>, :]</span><br><span class="line">        <span class="comment"># 添加新的类的质心向量</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>, :])</span><br><span class="line">        <span class="comment"># 更新clusterAssment列表中参与2-means聚类数据点变化后的分类编号，及数据该类的误差平方</span></span><br><span class="line">        clusterAssment[np.nonzero(clusterAssment[:, <span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>], :] = bestClustAss</span><br><span class="line">    <span class="comment"># 返回聚类结果</span></span><br><span class="line">    <span class="keyword">return</span> centList, clusterAssment</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：绘制数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">    k - 选取k个质心</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotDataSet</span><span class="params">(filename, k)</span>:</span></span><br><span class="line">    <span class="comment"># 导入数据</span></span><br><span class="line">    datMat = np.mat(loadDataSet(filename))</span><br><span class="line">    <span class="comment"># 进行k-means算法其中k为4</span></span><br><span class="line">    centList, clusterAssment = biKmeans(datMat, k)</span><br><span class="line">    clusterAssment = clusterAssment.tolist()</span><br><span class="line">    xcord = [[], [], []]</span><br><span class="line">    ycord = [[], [], []]</span><br><span class="line">    datMat = datMat.tolist()</span><br><span class="line">    m = len(clusterAssment)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">if</span> int(clusterAssment[i][<span class="number">0</span>]) == <span class="number">0</span>:</span><br><span class="line">            xcord[<span class="number">0</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">0</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> int(clusterAssment[i][<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">            xcord[<span class="number">1</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">1</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">elif</span> int(clusterAssment[i][<span class="number">0</span>]) == <span class="number">2</span>:</span><br><span class="line">            xcord[<span class="number">2</span>].append(datMat[i][<span class="number">0</span>])</span><br><span class="line">            ycord[<span class="number">2</span>].append(datMat[i][<span class="number">1</span>])</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># 绘制样本点</span></span><br><span class="line">    ax.scatter(xcord[<span class="number">0</span>], ycord[<span class="number">0</span>], s=<span class="number">20</span>, c=<span class="string">'b'</span>, marker=<span class="string">'*'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(xcord[<span class="number">1</span>], ycord[<span class="number">1</span>], s=<span class="number">20</span>, c=<span class="string">'r'</span>, marker=<span class="string">'D'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    ax.scatter(xcord[<span class="number">2</span>], ycord[<span class="number">2</span>], s=<span class="number">20</span>, c=<span class="string">'c'</span>, marker=<span class="string">'&gt;'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 绘制质心</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        ax.scatter(centList[i].tolist()[<span class="number">0</span>][<span class="number">0</span>], centList[i].tolist()[<span class="number">0</span>][<span class="number">1</span>], s=<span class="number">100</span>, c=<span class="string">'k'</span>, marker=<span class="string">'+'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># ax.scatter(centList[0].tolist()[0][0], centList[0].tolist()[0][1], s=100, c='k', marker='+', alpha=.5)</span></span><br><span class="line">    <span class="comment"># ax.scatter(centList[1].tolist()[0][0], centList[1].tolist()[0][1], s=100, c='k', marker='+', alpha=.5)</span></span><br><span class="line">    <span class="comment"># ax.scatter(centList[2].tolist()[0][0], centList[2].tolist()[0][1], s=100, c='k', marker='+', alpha=.5)</span></span><br><span class="line">    plt.title(<span class="string">'DataSet'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    datMat = np.mat(loadDataSet(<span class="string">'testSet2.txt'</span>))</span><br><span class="line">    centList, myNewAssments = biKmeans(datMat, <span class="number">3</span>)</span><br><span class="line">    plotDataSet(<span class="string">'testSet2.txt'</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（七）- SVM</title>
      <link href="/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%20-SVM)/"/>
      <url>/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%20-SVM)/</url>
      
        <content type="html"><![CDATA[<h1 id="SVM（支持向量机）"><a href="#SVM（支持向量机）" class="headerlink" title="SVM（支持向量机）"></a>SVM（支持向量机）</h1><p>支持向量（support vector)就是离分隔超平面最近的那些点。最大化支持向量到分割面的距离。</p><div align = center><img src = "https://img.vim-cn.com/25/6dd0e73f2ac658c2e82d39ddc79adc759cf28e.png"></div><h2 id="核函数（kneral-function"><a href="#核函数（kneral-function" class="headerlink" title="核函数（kneral function)"></a>核函数（kneral function)</h2><p>核函数就是将数据转化成易于分类器理解的一种形式，目前相对流行的一种核函数：径向基函数</p><h3 id="径向基函数"><a href="#径向基函数" class="headerlink" title="径向基函数"></a>径向基函数</h3><p>把数据从一个特征空间映射到另一个特征空间<br>$k(x,y)=exp(\frac{-||x-y||^2}{2\sigma^2})$<br>其中$\sigma$是确定到达率，是函数值跌倒0的速度参数</p><h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Jul 25 11:04:01 2018</span></span><br><span class="line"><span class="string">k1越大会过拟合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: wzy</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">类说明：维护所有需要操作的值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMatIn - 数据矩阵</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    C - 松弛变量</span></span><br><span class="line"><span class="string">    toler - 容错率</span></span><br><span class="line"><span class="string">    kTup - 包含核函数信息的元组，第一个参数存放该核函数类别，第二个参数存放必要的核函数需要用到的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">optStruct</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dataMatIn, classLabels, C, toler, kTup)</span>:</span></span><br><span class="line">        <span class="comment"># 数据矩阵</span></span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        <span class="comment"># 数据标签</span></span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        <span class="comment"># 松弛变量</span></span><br><span class="line">        self.C = C</span><br><span class="line">        <span class="comment"># 容错率</span></span><br><span class="line">        self.tol = toler</span><br><span class="line">        <span class="comment"># 矩阵的行数</span></span><br><span class="line">        self.m = np.shape(dataMatIn)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 根据矩阵行数初始化alphas矩阵，一个m行1列的全零列向量</span></span><br><span class="line">        self.alphas = np.mat(np.zeros((self.m, <span class="number">1</span>)))</span><br><span class="line">        <span class="comment"># 初始化b参数为0</span></span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 根据矩阵行数初始化误差缓存矩阵，第一列为是否有效标志位，第二列为实际的误差E的值</span></span><br><span class="line">        self.eCache = np.mat(np.zeros((self.m, <span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># 初始化核K</span></span><br><span class="line">        self.K = np.mat(np.zeros((self.m, self.m)))</span><br><span class="line">        <span class="comment"># 计算所有数据的核K</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.m):</span><br><span class="line">            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：通过核函数将数据转换更高维空间</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    X - 数据矩阵</span></span><br><span class="line"><span class="string">    A - 单个数据的向量</span></span><br><span class="line"><span class="string">    kTup - 包含核函数信息的元组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    K - 计算的核K</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span><span class="params">(X, A, kTup)</span>:</span></span><br><span class="line">    <span class="comment"># 读取X的行列数</span></span><br><span class="line">    m, n = np.shape(X)</span><br><span class="line">    <span class="comment"># K初始化为m行1列的零向量</span></span><br><span class="line">    K = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 线性核函数只进行内积</span></span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>] == <span class="string">'lin'</span>:</span><br><span class="line">        K = X * A.T</span><br><span class="line">    <span class="comment"># 高斯核函数，根据高斯核函数公式计算</span></span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>] == <span class="string">'rbf'</span>:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            deltaRow = X[j, :] - A</span><br><span class="line">            K[j] = deltaRow * deltaRow.T</span><br><span class="line">        K = np.exp(K / (<span class="number">-1</span> * kTup[<span class="number">1</span>] ** <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">'核函数无法识别'</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line">     </span><br><span class="line">        </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：读取数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="comment"># 数据矩阵</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    <span class="comment"># 标签向量</span></span><br><span class="line">    labelMat = []</span><br><span class="line">    <span class="comment"># 打开文件</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="comment"># 逐行读取</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment"># 去掉每一行首尾的空白符，例如'\n','\r','\t',' '</span></span><br><span class="line">        <span class="comment"># 将每一行内容根据'\t'符进行切片</span></span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># 添加数据(100个元素排成一行)</span></span><br><span class="line">        dataMat.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        <span class="comment"># 添加标签(100个元素排成一行)</span></span><br><span class="line">        labelMat.append(float(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：计算误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    oS - 数据结构</span></span><br><span class="line"><span class="string">    k - 标号为k的数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    Ek - 标号为k的数据误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEk</span><span class="params">(oS, k)</span>:</span></span><br><span class="line">    <span class="comment"># multiply(a,b)就是个乘法，如果a,b是两个数组，那么对应元素相乘</span></span><br><span class="line">    <span class="comment"># .T为转置</span></span><br><span class="line">    fXk = float(np.multiply(oS.alphas, oS.labelMat).T * oS.K[:, k]  + oS.b)</span><br><span class="line">    <span class="comment"># 计算误差项</span></span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    <span class="comment"># 返回误差项</span></span><br><span class="line">    <span class="keyword">return</span> Ek</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：随机选择alpha_j</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    i - alpha_i的索引值</span></span><br><span class="line"><span class="string">    m - alpha参数个数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    j - alpha_j的索引值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJrand</span><span class="params">(i, m)</span>:</span></span><br><span class="line">    j = i</span><br><span class="line">    <span class="keyword">while</span>(j == i):</span><br><span class="line">        <span class="comment"># uniform()方法将随机生成一个实数，它在[x, y)范围内</span></span><br><span class="line">        j = int(random.uniform(<span class="number">0</span>, m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：内循环启发方式2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    i - 标号为i的数据的索引值</span></span><br><span class="line"><span class="string">    oS - 数据结构</span></span><br><span class="line"><span class="string">    Ei - 标号为i的数据误差</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    j - 标号为j的数据的索引值</span></span><br><span class="line"><span class="string">    maxK - 标号为maxK的数据的索引值</span></span><br><span class="line"><span class="string">    Ej - 标号为j的数据误差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJ</span><span class="params">(i, oS, Ei)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    maxK = <span class="number">-1</span></span><br><span class="line">    maxDeltaE = <span class="number">0</span></span><br><span class="line">    Ej = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 根据Ei更新误差缓存</span></span><br><span class="line">    oS.eCache[i] = [<span class="number">1</span>, Ei]</span><br><span class="line">    <span class="comment"># 对一个矩阵.A转换为Array类型</span></span><br><span class="line">    <span class="comment"># 返回误差不为0的数据的索引值</span></span><br><span class="line">    validEcacheList = np.nonzero(oS.eCache[:, <span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 有不为0的误差</span></span><br><span class="line">    <span class="keyword">if</span>(len(validEcacheList) &gt; <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 遍历，找到最大的Ek</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> validEcacheList:</span><br><span class="line">            <span class="comment"># 不计算k==i节省时间</span></span><br><span class="line">            <span class="keyword">if</span> k == i:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 计算Ek</span></span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            <span class="comment"># 计算|Ei - Ek|</span></span><br><span class="line">            deltaE = abs(Ei - Ek)</span><br><span class="line">            <span class="comment"># 找到maxDeltaE</span></span><br><span class="line">            <span class="keyword">if</span>(deltaE &gt; maxDeltaE):</span><br><span class="line">                maxK = k</span><br><span class="line">                maxDeltaE = deltaE</span><br><span class="line">                Ej = Ek</span><br><span class="line">        <span class="comment"># 返回maxK，Ej</span></span><br><span class="line">        <span class="keyword">return</span> maxK, Ej</span><br><span class="line">    <span class="comment"># 没有不为0的误差</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 随机选择alpha_j的索引值</span></span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        <span class="comment"># 计算Ej</span></span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    <span class="comment"># 返回j，Ej</span></span><br><span class="line">    <span class="keyword">return</span> j, Ej</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：计算Ek,并更新误差缓存</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    oS - 数据结构</span></span><br><span class="line"><span class="string">    k - 标号为k的数据的索引值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateEk</span><span class="params">(oS, k)</span>:</span></span><br><span class="line">    <span class="comment"># 计算Ek</span></span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    <span class="comment"># 更新误差缓存</span></span><br><span class="line">    oS.eCache[k] = [<span class="number">1</span>, Ek]</span><br><span class="line">    </span><br><span class="line">      </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：修剪alpha_j</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    aj - alpha_j值</span></span><br><span class="line"><span class="string">    H - alpha上限</span></span><br><span class="line"><span class="string">    L - alpha下限</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    aj - alpha_j值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-24</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clipAlpha</span><span class="params">(aj, H, L)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> aj &gt; H:</span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：优化的SMO算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    i - 标号为i的数据的索引值</span></span><br><span class="line"><span class="string">    oS - 数据结构</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    1 - 有任意一对alpha值发生变化</span></span><br><span class="line"><span class="string">    0 - 没有任意一对alpha值发生变化或变化太小</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerL</span><span class="params">(i, oS)</span>:</span></span><br><span class="line">    <span class="comment"># 步骤1：计算误差Ei</span></span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    <span class="comment"># 优化alpha,设定一定的容错率</span></span><br><span class="line">    <span class="keyword">if</span>((oS.labelMat[i] * Ei &lt; -oS.tol) <span class="keyword">and</span> (oS.alphas[i] &lt; oS.C)) <span class="keyword">or</span> ((oS.labelMat[i] * Ei &gt; oS.tol) <span class="keyword">and</span> (oS.alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">        <span class="comment"># 使用内循环启发方式2选择alpha_j,并计算Ej</span></span><br><span class="line">        j, Ej = selectJ(i, oS, Ei)</span><br><span class="line">        <span class="comment"># 保存更新前的alpha值，使用深层拷贝</span></span><br><span class="line">        alphaIold = oS.alphas[i].copy()</span><br><span class="line">        alphaJold = oS.alphas[j].copy()</span><br><span class="line">        <span class="comment"># 步骤2：计算上界H和下界L</span></span><br><span class="line">        <span class="keyword">if</span>(oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        <span class="keyword">if</span> L == H:</span><br><span class="line">            print(<span class="string">"L == H"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 步骤3：计算eta</span></span><br><span class="line">        eta = <span class="number">2.0</span> * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]</span><br><span class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"eta &gt;= 0"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 步骤4：更新alpha_j</span></span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta</span><br><span class="line">        <span class="comment"># 步骤5：修剪alpha_j</span></span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)</span><br><span class="line">        <span class="comment"># 更新Ej至误差缓存</span></span><br><span class="line">        updateEk(oS, j)</span><br><span class="line">        <span class="keyword">if</span>(abs(oS.alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">            print(<span class="string">"alpha_j变化太小"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 步骤6：更新alpha_i</span></span><br><span class="line">        oS.alphas[i] += oS.labelMat[i] * oS.labelMat[j] * (alphaJold - oS.alphas[j])</span><br><span class="line">        <span class="comment"># 更新Ei至误差缓存</span></span><br><span class="line">        updateEk(oS, i)</span><br><span class="line">        <span class="comment"># 步骤7：更新b_1和b_2:</span></span><br><span class="line">        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, i]</span><br><span class="line">        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, j]</span><br><span class="line">        <span class="comment"># 步骤8：根据b_1和b_2更新b</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="number">0</span> &lt; oS.alphas[i] &lt; oS.C):</span><br><span class="line">            oS.b = b1</span><br><span class="line">        <span class="keyword">elif</span>(<span class="number">0</span> &lt; oS.alphas[j] &lt; oS.C):</span><br><span class="line">            oS.b = b2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            oS.b = (b1 + b2) / <span class="number">2.0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：完整的线性SMO算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMatIn - 数据矩阵</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    C - 松弛变量</span></span><br><span class="line"><span class="string">    toler - 容错率</span></span><br><span class="line"><span class="string">    maxIter - 最大迭代次数</span></span><br><span class="line"><span class="string">    kTup - 包含核函数信息的元组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    oS.b - SMO算法计算的b</span></span><br><span class="line"><span class="string">    oS.alphas - SMO算法计算的alphas</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span><span class="params">(dataMatIn, classLabels, C, toler, maxIter, kTup = <span class="params">(<span class="string">'lin'</span>, <span class="number">0</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化数据结构</span></span><br><span class="line">    oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler, kTup)</span><br><span class="line">    <span class="comment"># 初始化当前迭代次数</span></span><br><span class="line">    iter = <span class="number">0</span></span><br><span class="line">    entrieSet = <span class="literal">True</span></span><br><span class="line">    alphaPairsChanged = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历整个数据集alpha都没有更新或者超过最大迭代次数，则退出循环</span></span><br><span class="line">    <span class="keyword">while</span>(iter &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entrieSet)):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 遍历整个数据集</span></span><br><span class="line">        <span class="keyword">if</span> entrieSet:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(oS.m):</span><br><span class="line">                <span class="comment"># 使用优化的SMO算法</span></span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                print(<span class="string">"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d"</span> % (iter, i, alphaPairsChanged))</span><br><span class="line">            iter += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 遍历非边界值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 遍历不在边界0和C的alpha</span></span><br><span class="line">            nonBoundIs = np.nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                print(<span class="string">"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d"</span> % (iter, i, alphaPairsChanged))</span><br><span class="line">            iter += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 遍历一次后改为非边界遍历</span></span><br><span class="line">        <span class="keyword">if</span> entrieSet:</span><br><span class="line">            entrieSet = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果alpha没有更新，计算全样本遍历</span></span><br><span class="line">        <span class="keyword">elif</span>(alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            entrieSet = <span class="literal">True</span></span><br><span class="line">        print(<span class="string">"迭代次数:%d"</span> % iter)</span><br><span class="line">    <span class="comment"># 返回SMO算法计算的b和alphas</span></span><br><span class="line">    <span class="keyword">return</span> oS.b, oS.alphas</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：测试函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    k1 - 使用高斯核函数的时候表示到达率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testRbf</span><span class="params">(k1 = <span class="number">1.3</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 加载训练集</span></span><br><span class="line">    dataArr, labelArr = loadDataSet(<span class="string">'testSetRBF.txt'</span>)</span><br><span class="line">    <span class="comment"># 根据训练集计算b, alphas</span></span><br><span class="line">    b, alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">100</span>, (<span class="string">'rbf'</span>, k1))</span><br><span class="line">    datMat = np.mat(dataArr)</span><br><span class="line">    labelMat = np.mat(labelArr).transpose()</span><br><span class="line">    <span class="comment"># 获得支持向量</span></span><br><span class="line">    svInd = np.nonzero(alphas.A &gt; <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    sVs = datMat[svInd]</span><br><span class="line">    labelSV = labelMat[svInd]</span><br><span class="line">    print(<span class="string">"支持向量个数:%d"</span> % np.shape(sVs)[<span class="number">0</span>])</span><br><span class="line">    m, n = np.shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># 计算各个点的核</span></span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], (<span class="string">'rbf'</span>, k1))</span><br><span class="line">        <span class="comment"># 根据支持向量的点计算超平面，返回预测结果</span></span><br><span class="line">        predict = kernelEval.T * np.multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="comment"># 返回数组中各元素的正负号，用1和-1表示，并统计错误个数</span></span><br><span class="line">        <span class="keyword">if</span> np.sign(predict) != np.sign(labelArr[i]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 打印错误率</span></span><br><span class="line">    print(<span class="string">'训练集错误率:%.2f%%'</span> % ((float(errorCount) / m) * <span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 加载测试集</span></span><br><span class="line">    dataArr, labelArr = loadDataSet(<span class="string">'testSetRBF2.txt'</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat = np.mat(dataArr)</span><br><span class="line">    labelMat = np.mat(labelArr).transpose()</span><br><span class="line">    m, n = np.shape(datMat)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># 计算各个点的核</span></span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], (<span class="string">'rbf'</span>, k1))</span><br><span class="line">        <span class="comment"># 根据支持向量的点计算超平面，返回预测结果</span></span><br><span class="line">        predict = kernelEval.T * np.multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="comment"># 返回数组中各元素的正负号，用1和-1表示，并统计错误个数</span></span><br><span class="line">        <span class="keyword">if</span> np.sign(predict) != np.sign(labelArr[i]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 打印错误率</span></span><br><span class="line">    print(<span class="string">'训练集错误率:%.2f%%'</span> % ((float(errorCount) / m) * <span class="number">100</span>))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：数据可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showDataSet</span><span class="params">(dataMat, labelMat)</span>:</span></span><br><span class="line">    <span class="comment"># 正样本</span></span><br><span class="line">    data_plus = []</span><br><span class="line">    <span class="comment"># 负样本</span></span><br><span class="line">    data_minus = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataMat)):</span><br><span class="line">        <span class="keyword">if</span> labelMat[i] &gt; <span class="number">0</span>:</span><br><span class="line">            data_plus.append(dataMat[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_minus.append(dataMat[i])</span><br><span class="line">    <span class="comment"># 转换为numpy矩阵</span></span><br><span class="line">    data_plus_np = np.array(data_plus)</span><br><span class="line">    <span class="comment"># 转换为numpy矩阵</span></span><br><span class="line">    data_minus_np = np.array(data_minus)</span><br><span class="line">    <span class="comment"># 正样本散点图（scatter）</span></span><br><span class="line">    <span class="comment"># transpose转置</span></span><br><span class="line">    plt.scatter(np.transpose(data_plus_np)[<span class="number">0</span>], np.transpose(data_plus_np)[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 负样本散点图（scatter）</span></span><br><span class="line">    plt.scatter(np.transpose(data_minus_np)[<span class="number">0</span>], np.transpose(data_minus_np)[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    testRbf()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（六）- logist回归</title>
      <link href="/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886-%20Logistic%E5%9B%9E%E5%BD%92%EF%BC%89/"/>
      <url>/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886-%20Logistic%E5%9B%9E%E5%BD%92%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="logist回归"><a href="#logist回归" class="headerlink" title="logist回归"></a>logist回归</h1><h2 id="最佳回归系数与Sigmod函数"><a href="#最佳回归系数与Sigmod函数" class="headerlink" title="最佳回归系数与Sigmod函数"></a>最佳回归系数与Sigmod函数</h2><p>$\sigma = 1/(1 + e^{-z})$<br>$z = w_0x_0+ w_1x_1+ w_2x_2+ ….w_nx_n$<br>$z = w^TX$</p><h3 id="梯度上升法"><a href="#梯度上升法" class="headerlink" title="梯度上升法"></a>梯度上升法</h3><p>$w = w + α* grad f(w)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：梯度上升算法测试函数</span></span><br><span class="line"><span class="string">        求函数f(x) = -x^2+4x的极大值</span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Gradient_Ascent_test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># f(x)的导数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f_prime</span><span class="params">(x_old)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-2</span> * x_old + <span class="number">4</span></span><br><span class="line">    <span class="comment"># 初始值，给一个小于x_new的值</span></span><br><span class="line">    x_old = <span class="number">-1</span></span><br><span class="line">    <span class="comment"># 梯度上升算法初始值，即从(0, 0)开始</span></span><br><span class="line">    x_new = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 步长，也就是学习速率，控制更新的幅度</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># 精度，也就是更新阈值</span></span><br><span class="line">    presision = <span class="number">0.00000001</span></span><br><span class="line">    <span class="keyword">while</span> abs(x_new - x_old) &gt; presision:</span><br><span class="line">        x_old = x_new</span><br><span class="line">        <span class="comment"># 利用上面的公式</span></span><br><span class="line">        x_new = x_old + alpha * f_prime(x_old)</span><br><span class="line">    <span class="comment"># 打印最终求解的极值近似值</span></span><br><span class="line">    print(x_new)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：sigmoid函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    inX - 数据</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    sigmoid函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-inX))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：梯度上升法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMath - 数据集</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    weights.getA() - 求得的权重数组（最优参数）</span></span><br><span class="line"><span class="string">    weights_array - 每次更新的回归系数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMath, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转换成numpy的mat(矩阵)</span></span><br><span class="line">    dataMatrix = np.mat(dataMath)</span><br><span class="line">    <span class="comment"># 转换成numpy的mat(矩阵)并进行转置</span></span><br><span class="line">    labelMat = np.mat(classLabels).transpose()</span><br><span class="line">    <span class="comment"># 返回dataMatrix的大小，m为行数，n为列数</span></span><br><span class="line">    m, n = np.shape(dataMatrix)</span><br><span class="line">    <span class="comment"># 移动步长，也就是学习效率，控制更新的幅度</span></span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># 最大迭代次数</span></span><br><span class="line">    maxCycles = <span class="number">500</span></span><br><span class="line">    weights = np.ones((n, <span class="number">1</span>))</span><br><span class="line">    weights_array = np.array([])</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        <span class="comment"># 梯度上升矢量化公式</span></span><br><span class="line">        h = sigmoid(dataMatrix * weights)</span><br><span class="line">        error = labelMat - h</span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error</span><br><span class="line">        <span class="comment"># numpy.append(arr, values, axis=None):就是arr和values会重新组合成一个新的数组，做为返回值。</span></span><br><span class="line">        <span class="comment"># 当axis无定义时，是横向加成，返回总是为一维数组</span></span><br><span class="line">        weights_array = np.append(weights_array, weights)</span><br><span class="line">    weights_array = weights_array.reshape(maxCycles, n)</span><br><span class="line">    <span class="comment"># 将矩阵转换为数组，返回权重数组</span></span><br><span class="line">    <span class="comment"># mat.getA()将自身矩阵变量转化为ndarray类型变量</span></span><br><span class="line">    <span class="keyword">return</span> weights.getA(), weights_array</span><br></pre></td></tr></table></figure><h3 id="绘制决策边界"><a href="#绘制决策边界" class="headerlink" title="绘制决策边界"></a>绘制决策边界</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：绘制数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    weights - 权重参数数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights)</span>:</span></span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()</span><br><span class="line">    <span class="comment"># 转换成numpy的array数组</span></span><br><span class="line">    dataArr = np.array(dataMat)</span><br><span class="line">    <span class="comment"># 数据个数</span></span><br><span class="line">    <span class="comment"># 例如建立一个4*2的矩阵c，c.shape[1]为第一维的长度2， c.shape[0]为第二维的长度4</span></span><br><span class="line">    n = np.shape(dataMat)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 正样本</span></span><br><span class="line">    xcord1 = []</span><br><span class="line">    ycord1 = []</span><br><span class="line">    <span class="comment"># 负样本</span></span><br><span class="line">    xcord2 = []</span><br><span class="line">    ycord2 = []</span><br><span class="line">    <span class="comment"># 根据数据集标签进行分类</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 1为正样本</span></span><br><span class="line">            xcord1.append(dataArr[i, <span class="number">1</span>])</span><br><span class="line">            ycord1.append(dataArr[i, <span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 0为负样本</span></span><br><span class="line">            xcord2.append(dataArr[i, <span class="number">1</span>])</span><br><span class="line">            ycord2.append(dataArr[i, <span class="number">2</span>])</span><br><span class="line">    <span class="comment"># 新建图框</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    <span class="comment"># 添加subplot</span></span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># 绘制正样本</span></span><br><span class="line">    ax.scatter(xcord1, ycord1, s=<span class="number">20</span>, c=<span class="string">'red'</span>, marker=<span class="string">'s'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 绘制负样本</span></span><br><span class="line">    ax.scatter(xcord2, ycord2, s=<span class="number">20</span>, c=<span class="string">'green'</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># x轴坐标</span></span><br><span class="line">    x = np.arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># w0*x0 + w1*x1 * w2*x2 = 0</span></span><br><span class="line">    <span class="comment"># x0 = 1, x1 = x, x2 = y</span></span><br><span class="line">    y = (-weights[<span class="number">0</span>] - weights[<span class="number">1</span>] * x) / weights[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 绘制title</span></span><br><span class="line">    plt.title(<span class="string">'BestFit'</span>)</span><br><span class="line">    <span class="comment"># 绘制label</span></span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'y2'</span>)</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="随机梯度上升法"><a href="#随机梯度上升法" class="headerlink" title="随机梯度上升法"></a>随机梯度上升法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：改进的随机梯度上升法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMatrix - 数据数组</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    numIter - 迭代次数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    weights - 求得的回归系数数组（最优参数）</span></span><br><span class="line"><span class="string">    weights_array - 每次更新的回归系数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 返回dataMatrix的大小，m为行数，n为列数</span></span><br><span class="line">    m, n = np.shape(dataMatrix)</span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    weights = np.ones(n)</span><br><span class="line">    weights_array = np.array([])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 每次都降低alpha的大小</span></span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span></span><br><span class="line">            <span class="comment"># 随机选择样本</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</span><br><span class="line">            <span class="comment"># 随机选择一个样本计算h</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[randIndex] * weights))</span><br><span class="line">            <span class="comment"># 计算误差</span></span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            <span class="comment"># 更新回归系数</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            <span class="comment"># 添加返回系数到数组中当axis为0时，数组是加在下面（列数要相同）</span></span><br><span class="line">            weights_array = np.append(weights_array, weights, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 删除已使用的样本</span></span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="comment"># 改变维度</span></span><br><span class="line">    weights_array = weights_array.reshape(numIter*m, n)</span><br><span class="line">    <span class="comment"># 返回</span></span><br><span class="line">    <span class="keyword">return</span> weights, weights_array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：绘制回归系数与迭代次数的关系</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    weights_array1 - 回归系数数组1</span></span><br><span class="line"><span class="string">    weights_array2 - 回归系数数组2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotWeights</span><span class="params">(weights_array1, weights_array2)</span>:</span></span><br><span class="line">    <span class="comment"># 设置汉字格式为14号简体字</span></span><br><span class="line">    font = FontProperties(fname=<span class="string">r"C:\Windows\Fonts\simsun.ttc"</span>, size=<span class="number">14</span>)</span><br><span class="line">    <span class="comment"># 将fig画布分隔成1行1列，不共享x轴和y轴，fig画布的大小为（20, 10）</span></span><br><span class="line">    <span class="comment"># 当nrows=3，ncols=2时，代表fig画布被分为6个区域，axs[0][0]代表第一行第一个区域</span></span><br><span class="line">    fig, axs = plt.subplots(nrows=<span class="number">3</span>, ncols=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="comment"># x1坐标轴的范围</span></span><br><span class="line">    x1 = np.arange(<span class="number">0</span>, len(weights_array1), <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 绘制w0与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">0</span>].plot(x1, weights_array1[:, <span class="number">0</span>])</span><br><span class="line">    axs0_title_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_title(<span class="string">u'改进的梯度上升算法，回归系数与迭代次数关系'</span>, FontProperties=font)</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_ylabel(<span class="string">u'w0'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs0_title_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 绘制w1与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">0</span>].plot(x1, weights_array1[:, <span class="number">1</span>])</span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">1</span>][<span class="number">0</span>].set_ylabel(<span class="string">u'w1'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 绘制w2与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">2</span>][<span class="number">0</span>].plot(x1, weights_array1[:, <span class="number">2</span>])</span><br><span class="line">    axs2_title_text = axs[<span class="number">2</span>][<span class="number">0</span>].set_title(<span class="string">u'迭代次数'</span>, FontProperties=font)</span><br><span class="line">    axs2_ylabel_text = axs[<span class="number">2</span>][<span class="number">0</span>].set_ylabel(<span class="string">u'w2'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs2_title_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs2_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># x2坐标轴的范围</span></span><br><span class="line">    x2 = np.arange(<span class="number">0</span>, len(weights_array2), <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 绘制w0与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">1</span>].plot(x2, weights_array2[:, <span class="number">0</span>])</span><br><span class="line">    axs0_title_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_title(<span class="string">u'梯度上升算法，回归系数与迭代次数关系'</span>, FontProperties=font)</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_ylabel(<span class="string">u'w0'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs0_title_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 绘制w1与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">1</span>].plot(x2, weights_array2[:, <span class="number">1</span>])</span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">1</span>][<span class="number">1</span>].set_ylabel(<span class="string">u'w1'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 绘制w2与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">2</span>][<span class="number">1</span>].plot(x2, weights_array2[:, <span class="number">2</span>])</span><br><span class="line">    axs2_title_text = axs[<span class="number">2</span>][<span class="number">1</span>].set_title(<span class="string">u'迭代次数'</span>, FontProperties=font)</span><br><span class="line">    axs2_ylabel_text = axs[<span class="number">2</span>][<span class="number">1</span>].set_ylabel(<span class="string">u'w2'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs2_title_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs2_ylabel_text, size=<span class="number">20</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 测试简单梯度上升法</span></span><br><span class="line">    <span class="comment"># Gradient_Ascent_test()</span></span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()</span><br><span class="line">    <span class="comment"># 训练权重</span></span><br><span class="line">    weights2, weights_array2 = gradAscent(dataMat, labelMat)</span><br><span class="line">    <span class="comment"># 新方法训练权重</span></span><br><span class="line">    weights1, weights_array1 = stocGradAscent1(np.array(dataMat), labelMat)</span><br><span class="line">    <span class="comment"># 绘制数据集中的y和x的散点图</span></span><br><span class="line">    <span class="comment"># plotBestFit(weights)</span></span><br><span class="line">    <span class="comment"># print(gradAscent(dataMat, labelMat))</span></span><br><span class="line">    plotWeights(weights_array1, weights_array2)</span><br></pre></td></tr></table></figure><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>$w = w - α*gradf(w)$</p>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（五）- 朴素贝叶斯</title>
      <link href="/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%89/"/>
      <url>/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h1><h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>$p(c|x) = \frac{p(x|c)p(c)}{p(x)}$<br>其中c为类别，x为实例具有特征${x_1,x_2,…x_i}$<br>如果 $p(c_1|x) &gt; p(c_2|x)$，那么就属于类别c1,反之属于c2</p><h2 id="朴素贝叶斯的一般过程"><a href="#朴素贝叶斯的一般过程" class="headerlink" title="朴素贝叶斯的一般过程"></a>朴素贝叶斯的一般过程</h2><ol><li>收集数据，可以使用RSS源</li><li>准备数据，需要使用<strong>数值型或者布尔型</strong></li><li>分析数据，大量特征时，可以绘制直方图</li><li>训练算法，计算不同的独立特征的条件概率</li><li>测试算法，计算错误率</li><li>使用算法，封装贝叶斯分类器</li></ol><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><h3 id="准备数据，从文本中构建词向量"><a href="#准备数据，从文本中构建词向量" class="headerlink" title="准备数据，从文本中构建词向量"></a>准备数据，从文本中构建词向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：创建实验样本</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    postingList - 实验样本切分的词条</span></span><br><span class="line"><span class="string">    classVec - 类别标签向量</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 切分的词条</span></span><br><span class="line">    postingList = [[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</span><br><span class="line">                   [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                   [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                   [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                   [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    <span class="comment"># 类别标签向量，1代表侮辱性词汇，0代表不是</span></span><br><span class="line">    classVec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 返回实验样本切分的词条、类别标签向量</span></span><br><span class="line">    <span class="keyword">return</span> postingList, classVec</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 整理的样本数据集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    vocabSet - 返回不重复的词条列表，也就是词汇表</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 创建一个空的不重复列表</span></span><br><span class="line">    <span class="comment"># set是一个无序且不重复的元素集合</span></span><br><span class="line">    vocabSet = set([])</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 取并集</span></span><br><span class="line">        vocabSet = vocabSet | set(document)</span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    vocabList - createVocabList返回的列表</span></span><br><span class="line"><span class="string">    inputSet - 切分的词条列表</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    returnVec - 文档向量，词集模型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    <span class="comment"># 创建一个其中所含元素都为0的向量</span></span><br><span class="line">    returnVec = [<span class="number">0</span>] * len(vocabList)</span><br><span class="line">    <span class="comment"># 遍历每个词条</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            <span class="comment"># 如果词条存在于词汇表中，则置1</span></span><br><span class="line">            <span class="comment"># index返回word出现在vocabList中的索引</span></span><br><span class="line">            <span class="comment"># 若这里改为+=则就是基于词袋的模型，遇到一个单词会增加单词向量中德对应值</span></span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"the word: %s is not in my Vocabulary"</span> % word)</span><br><span class="line">    <span class="comment"># 返回文档向量</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br><span class="line"></span><br><span class="line">&gt;&gt; imort bayes</span><br><span class="line">&gt;&gt; listOposts, listClasses = bayes.loadDataSet()</span><br><span class="line">&gt;&gt; myVocabList = bayes.createVocabList(listOpists)</span><br><span class="line">&gt;&gt; bayes.setOfWords2Vec(myVocabList, listOpists)</span><br><span class="line">[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0.</span>........]</span><br></pre></td></tr></table></figure><h3 id="训练算法，从词向量计算概率"><a href="#训练算法，从词向量计算概率" class="headerlink" title="训练算法，从词向量计算概率"></a>训练算法，从词向量计算概率</h3><p>$p(c|w) = \frac{p(w|c)p(c)}{p(w)}$<br>$p(c) = \frac{类别c的实例数}{总的实例数}$<br>$p(w|c)  = p(w_0,w_1,w_2,w_3,w_4 |c_i)$<br>$p(w|c)  = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)…p(w_n|c_i)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">计算每个类别中的文档数目</span><br><span class="line">对每篇训练文档</span><br><span class="line">    对每个类被</span><br><span class="line">        如果词条出现在文档中，增加该词条的计数值</span><br><span class="line">        增加所有词条的计数值</span><br><span class="line">    对每个类别</span><br><span class="line">        对每个词条：</span><br><span class="line">            将该词条的数目除以总的词条数目<span class="number">1</span>得到条件概率</span><br><span class="line">    返回每个类别的条件概率</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：朴素贝叶斯分类器训练函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵</span></span><br><span class="line"><span class="string">    trainCategory - 训练类标签向量，即loadDataSet返回的classVec</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    p0Vect - 侮辱类的条件概率数组</span></span><br><span class="line"><span class="string">    p1Vect - 非侮辱类的条件概率数组</span></span><br><span class="line"><span class="string">    pAbusive - 文档属于侮辱类的概率</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="comment"># 计算训练文档数目</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    <span class="comment"># 计算每篇文档的词条数目</span></span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 文档属于侮辱类的概率</span></span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 创建numpy.zeros数组，词条出现数初始化为0</span></span><br><span class="line">    <span class="comment"># p0Num = np.zeros(numWords)</span></span><br><span class="line">    <span class="comment"># p1Num = np.zeros(numWords)</span></span><br><span class="line">    <span class="comment"># 创建numpy.ones数组，词条出现数初始化为1,拉普拉斯平滑</span></span><br><span class="line">    p0Num = np.ones(numWords)</span><br><span class="line">    p1Num = np.ones(numWords)</span><br><span class="line">    <span class="comment"># 分母初始化为0</span></span><br><span class="line">    <span class="comment"># p0Denom = 0.0</span></span><br><span class="line">    <span class="comment"># p1Denom = 0.0</span></span><br><span class="line">    <span class="comment"># 分母初始化为2，拉普拉斯平滑</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="comment"># 统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)...</span></span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 统计所有侮辱类文档中每个单词出现的个数</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 统计一共出现的侮辱单词的个数</span></span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="comment"># 统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)...</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 统计所有非侮辱类文档中每个单词出现的个数</span></span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            <span class="comment"># 统计一共出现的非侮辱单词的个数</span></span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 每个侮辱类单词分别出现的概率</span></span><br><span class="line">    <span class="comment"># p1Vect = p1Num / p1Denom</span></span><br><span class="line">    <span class="comment"># 取对数，防止下溢出</span></span><br><span class="line">    p1Vect = np.log(p1Num / p1Denom)</span><br><span class="line">    <span class="comment"># 每个非侮辱类单词分别出现的概率</span></span><br><span class="line">    <span class="comment"># p0Vect = p0Num / p0Denom</span></span><br><span class="line">    <span class="comment"># 取对数，防止下溢出</span></span><br><span class="line">    p0Vect = np.log(p0Num / p0Denom)</span><br><span class="line">    <span class="comment"># 返回属于侮辱类的条件概率数组、属于非侮辱类的条件概率数组、文档属于侮辱类的概率</span></span><br><span class="line">    <span class="keyword">return</span> p0Vect, p1Vect, pAbusive</span><br><span class="line">```           </span><br><span class="line"><span class="comment">### 测试算法，改进</span></span><br><span class="line">为了避免$p(w|c)  = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)...p(w_n|c_i)$中出现某一项为<span class="number">0</span>的情况，这里采用对数矫正：</span><br><span class="line">$ln(a*b) = lna + lnb$</span><br><span class="line">```python</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：朴素贝叶斯分类器分类函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    vec2Classify - 待分类的词条数组</span></span><br><span class="line"><span class="string">    p0Vec - 侮辱类的条件概率数组</span></span><br><span class="line"><span class="string">    p1Vec - 非侮辱类的条件概率数组</span></span><br><span class="line"><span class="string">    pClass1 - 文档属于侮辱类的概率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    0 - 属于非侮辱类</span></span><br><span class="line"><span class="string">    1 - 属于侮辱类</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-21</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></span><br><span class="line">    <span class="comment"># 对应元素相乘</span></span><br><span class="line">    <span class="comment"># p1 = reduce(lambda x,y:x*y, vec2Classify * p1Vec) * pClass1</span></span><br><span class="line">    <span class="comment"># p0 = reduce(lambda x,y:x*y, vec2Classify * p0Vec) * (1.0 - pClass1)</span></span><br><span class="line">    <span class="comment"># 对应元素相乘，logA*B = logA + logB所以这里是累加</span></span><br><span class="line">    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)</span><br><span class="line">    p0 = sum(vec2Classify * p0Vec) + np.log(<span class="number">1.0</span> - pClass1)</span><br><span class="line">    <span class="comment"># print('p0:', p0)</span></span><br><span class="line">    <span class="comment"># print('p1:', p1)</span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：测试朴素贝叶斯分类器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-21</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 创建实验样本</span></span><br><span class="line">    listOPosts, listclasses = loadDataSet()</span><br><span class="line">    <span class="comment"># 创建词汇表,将输入文本中的不重复的单词进行提取组成单词向量</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        <span class="comment"># 将实验样本向量化若postinDoc中的单词在myVocabList出现则将returnVec该位置的索引置1</span></span><br><span class="line">        <span class="comment"># 将6组数据list存储在trainMat中</span></span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment"># 训练朴素贝叶斯分类器</span></span><br><span class="line">    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listclasses))</span><br><span class="line">    <span class="comment"># 测试样本1</span></span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    <span class="comment"># 测试样本向量化返回这三个单词出现位置的索引</span></span><br><span class="line">    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">if</span> classifyNB(thisDoc, p0V, p1V, pAb):</span><br><span class="line">        <span class="comment"># 执行分类并打印结果</span></span><br><span class="line">        print(testEntry, <span class="string">'属于侮辱类'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 执行分类并打印结果</span></span><br><span class="line">        print(testEntry, <span class="string">'属于非侮辱类'</span>)</span><br><span class="line">    <span class="comment"># 测试样本2</span></span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    <span class="comment"># 将实验样本向量化</span></span><br><span class="line">    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">if</span> classifyNB(thisDoc, p0V, p1V, pAb):</span><br><span class="line">        <span class="comment"># 执行分类并打印结果</span></span><br><span class="line">        print(testEntry, <span class="string">'属于侮辱类'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 执行分类并打印结果</span></span><br><span class="line">        print(testEntry, <span class="string">'属于非侮辱类'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（四）- 决策树</title>
      <link href="/2019/11/27/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884-DecisionTree)/"/>
      <url>/2019/11/27/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884-DecisionTree)/</url>
      
        <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树本质上是一种流程图，长方形代表<strong>判断模块</strong>，椭圆代表<strong>终止模块</strong>，左右箭头指引<strong>节点的上下分支</strong><br>决策树相比较于KNN，其重要的原因就是其数据形式非常容易理解，而KNN的数据形式所包含的内在含义却不是很容易理解</p><div align = center><img src = " https://img.vim-cn.com/10/8d6b3cb5f550b03b25681ed2bdc0f7cc424005.png"></div><p><strong>优点</strong><br>计算复杂度不高，可以处理不相关特征数据，对中间数据的缺省值不敏感<br><strong>缺点</strong><br>会产生过度匹配问题</p><h2 id="信息论划分数据集"><a href="#信息论划分数据集" class="headerlink" title="信息论划分数据集"></a>信息论划分数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""划分数据集的伪代码"""</span></span><br><span class="line">检测数据集是否属于同一类：</span><br><span class="line">    <span class="keyword">if</span> so  <span class="keyword">return</span> 类标签</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        寻找划分数据集的最好特征</span><br><span class="line">        划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">            <span class="keyword">for</span> 每个划分的子类</span><br><span class="line">                调用函数createBranch并增加返回节点到结果上</span><br><span class="line">        <span class="keyword">return</span> 分支节点</span><br></pre></td></tr></table></figure><h2 id="决策树的流程"><a href="#决策树的流程" class="headerlink" title="决策树的流程"></a>决策树的流程</h2><ol><li>收集数据：可以使用任何方法</li><li>准备数据：<strong>树构造算法适用于标称型数据，如果数据是数值型数据，需要先把数据进行离散化</strong></li><li>分析数据：构造树完成，进行检查</li><li>训练算法：构造树的数据结构</li><li>测试算法：使用经验树计算错误率</li><li>使用算法</li></ol><h2 id="信息增益与信息熵"><a href="#信息增益与信息熵" class="headerlink" title="信息增益与信息熵"></a>信息增益与信息熵</h2><p><strong>划分数据集的最大原则是，将无序的数据变得更加有序</strong><br><strong>信息增益</strong>：划分数据集前后信息发生的变化称为信息增益<br>计算每个特征划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择<br><strong>如何计算信息增益？</strong><br>集合信息的度量方式称为<strong>香农熵</strong><br>熵定义为信息的期望值：<br>符号$x_i$的信息定义为$l(x_i) = -log_2p(x_i)$，p(x_i)是选择该分类的概率<br>则信息熵为：$H = -\sum\nolimits_{i=1}^{n}p(x_i)log_2p(x_i)$,其中n是分类的数目</p><h2 id="计算信息熵的源代码"><a href="#计算信息熵的源代码" class="headerlink" title="计算信息熵的源代码"></a>计算信息熵的源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">用于计算给定的信息熵</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：计算给定数据集的经验熵（香农熵）</span></span><br><span class="line"><span class="string">        H = -SUM（kp*Log2（kp））</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 数据集</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    shannonEnt - 经验熵（香农熵）</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 返回数据集的行数</span></span><br><span class="line">    numEntires = len(dataSet)</span><br><span class="line">    <span class="comment"># 保存每个标签（Label）出现次数的“字典”</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># 对每组特征向量进行统计</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:     <span class="comment"># 按行进行遍历</span></span><br><span class="line">        <span class="comment"># 提取标签（Label）信息</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]    <span class="comment"># 取每一行最后一列特征值</span></span><br><span class="line">        <span class="comment"># 如果标签（Label）没有放入统计次数的字典，添加进去</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            <span class="comment"># 创建一个新的键值对，键为currentLabel值为0</span></span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Label计数</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 经验熵（香农熵）</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 计算香农熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        <span class="comment"># 选择该标签（Label）的概率</span></span><br><span class="line">        prob = float(labelCounts[key]) / numEntires</span><br><span class="line">        <span class="comment"># 利用公式计算</span></span><br><span class="line">        shannonEnt -= prob*log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 返回经验熵（香农熵）</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><p><strong>创建数据集</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>：</span></span><br><span class="line">    dataSet = [[1, ,1 , 'yes'],[1, 1, 'yes'],[1, 0, 'no'],[0, 1, 'no'],[0, 1, 'no']]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line">&gt;&gt; reload(trees.py)</span><br><span class="line">&gt;&gt; myDat, labels = trees.createDataSet()</span><br><span class="line">&gt;&gt; trees.calcShannonEnt(mydata)</span><br></pre></td></tr></table></figure><h2 id="划分数据集的算法与代码"><a href="#划分数据集的算法与代码" class="headerlink" title="划分数据集的算法与代码"></a>划分数据集的算法与代码</h2><p>一般可用二分法划分数据集，这里采用<strong>ID3算法</strong>进行划分数据集<br>香农熵可用来度量数据集的无序度，数据无序度越大，香农熵值越大。<br>分类算法除了需要测量信息熵还需要一个<strong>度量数据划分准确度的熵值</strong>，这就像在二维坐标中画直线进行划分平面坐标系、</p><ol><li><strong>按照给定特征划分数据集</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数：按照给定的特征划分数据集</span></span><br><span class="line"><span class="string">输入：</span></span><br><span class="line"><span class="string">    dataSet,数据集</span></span><br><span class="line"><span class="string">    axis, 划分数据集的特征</span></span><br><span class="line"><span class="string">    value, 需要返回的特征值</span></span><br><span class="line"><span class="string">Return:</span></span><br><span class="line"><span class="string">    retDataSet,划分的数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  <span class="comment"># 按行遍历数据集</span></span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:   <span class="comment"># 去掉特征为axis的数据集</span></span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])  <span class="comment"># 扩展列表元素</span></span><br><span class="line">            retDataSet.append(reducedFeatVec)  <span class="comment"># 添加嵌套列表</span></span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="keyword">import</span> DecisionTree <span class="keyword">as</span> DT</span><br><span class="line">&gt;&gt; mydat, labels = DT.createDataSet()</span><br><span class="line">&gt;&gt; DT.splitDataSet(mydat, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">&gt;&gt; DT.splitDataSet(mydat, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li>选择最好的数据集划分方式<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数：选择最好的数据集划分方式</span></span><br><span class="line"><span class="string">Para:</span></span><br><span class="line"><span class="string">    dataSet:数据集</span></span><br><span class="line"><span class="string">return:</span></span><br><span class="line"><span class="string">    bestFeature:最好的特征的索引值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line"></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  <span class="comment"># 特征的个数-1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)  <span class="comment"># 计算数据集的香农熵</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span> ; bestFeature = <span class="number">-1</span>  <span class="comment"># 最大信息增益和最优划分特征的索引值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures): <span class="comment"># 遍历特征</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet] <span class="comment"># 列表生成式生成特征所有的取值</span></span><br><span class="line">        uniqueVals = set(featList) <span class="comment"># 删去重复值</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span> <span class="comment"># 香农熵</span></span><br><span class="line">        <span class="keyword">for</span> values <span class="keyword">in</span> uniqueVals:  <span class="comment"># 遍历特征值</span></span><br><span class="line">            subDataSet = splitDataSet(dataset, i , value)  <span class="comment"># 划分数据集</span></span><br><span class="line">            prob = len(subDataSet) / float(len(dataSet))  <span class="comment"># 计算概率</span></span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)  <span class="comment"># 计算经验条件熵</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy  <span class="comment"># 信息增益值</span></span><br><span class="line">        print(<span class="string">"第%d个特征的增益为%.3f"</span> % (i, infoGain)) <span class="comment"># 打印每个特征的信息增益，取正</span></span><br><span class="line">        <span class="keyword">if</span>(infoGain &gt; baseEntropy):    <span class="comment"># 找到对应最大信息增益的特征</span></span><br><span class="line">            baseInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br><span class="line"></span><br><span class="line">&gt;&gt; DecisionTree.chooseBestFeaturToSplit(mydat)</span><br></pre></td></tr></table></figure><h2 id="递归构建决策树"><a href="#递归构建决策树" class="headerlink" title="递归构建决策树"></a>递归构建决策树</h2><div align = center><img src = "https://img.vim-cn.com/81/18e71a83e22a268a8899ee9aff50fe083ddbd4.png"></div></li></ol><p>当特征值多与两个的时候，就可能存在大于两个分支的数据集划分，这时我们就通过递归调用来实现它！<br><strong>递归结束的条件</strong><br>程序遍历完所有划分数据的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例都具有相同的分类，则得到一个叶子节点或者终止快。任何到达叶子结点所属的分类必然属于叶子节点的类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：统计classList中出现次数最多的元素（类标签）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    classList - 类标签列表</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    sortedClassCount[0][0] - 出现次数最多的元素（类标签）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span>   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="comment"># 统计classList中每个元素出现的次数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 根据字典的值降序排序</span></span><br><span class="line">    <span class="comment"># operator.itemgetter(1)获取对象的第1列的值</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 返回classList中出现次数最多的元素</span></span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>创建决策树的代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：创建决策树（ID3算法）</span></span><br><span class="line"><span class="string">        递归有两个终止条件：1、所有的类标签完全相同，直接返回类标签</span></span><br><span class="line"><span class="string">                        2、用完所有标签但是得不到唯一类别的分组，即特征不够用，挑选出现数量最多的类别作为返回</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 训练数据集</span></span><br><span class="line"><span class="string">    labels - 分类属性标签</span></span><br><span class="line"><span class="string">    featLabels - 存储选择的最优特征标签</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    myTree - 决策树</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels, featLabels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]   <span class="comment"># 取分类标签（是否放贷：yes or no）</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):  <span class="comment"># 如果类别完全相同则停止继续划分</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:  <span class="comment"># 遍历完所有特征时返回出现次数最多的类标签</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet) <span class="comment"># 选择最优特征</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat]  <span class="comment"># 最优特征的标签</span></span><br><span class="line">    featLabels.append(bestFeatLabel)</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125; <span class="comment"># 根据最优特征的标签生成树</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])      <span class="comment"># 删除已经使用的特征标签</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet] <span class="comment"># 得到训练集中所有最优解特征的属性值</span></span><br><span class="line">    uniqueVals = set(featValues) <span class="comment"># 去掉重复的属性值</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals: <span class="comment"># 遍历特征，创建决策树</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line">&gt;&gt; my tree = DecisionTree.createTree(mydata,labels)</span><br><span class="line">&gt;&gt; my tree</span><br></pre></td></tr></table></figure><h2 id="测试算法与分类器"><a href="#测试算法与分类器" class="headerlink" title="测试算法与分类器"></a>测试算法与分类器</h2><h3 id="测试算法构造分类器"><a href="#测试算法构造分类器" class="headerlink" title="测试算法构造分类器"></a>测试算法构造分类器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数：使用决策树的分类函数</span></span><br><span class="line"><span class="string">para:</span></span><br><span class="line"><span class="string">    inputree,</span></span><br><span class="line"><span class="string">    featLabels,</span></span><br><span class="line"><span class="string">    testVec</span></span><br><span class="line"><span class="string">return:</span></span><br><span class="line"><span class="string">    classlabel,分类器</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLabels, testVec)</span>:</span></span><br><span class="line">    <span class="comment"># 获取决策树结点</span></span><br><span class="line">    firstStr = next(iter(inputTree))</span><br><span class="line">    <span class="comment"># 下一个字典</span></span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> testVec[featIndex] == key:</span><br><span class="line">            <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">                classLabel = classify(secondDict[key], featLabels, testVec)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                classLabel = secondDict[key]</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure><h3 id="使用算法"><a href="#使用算法" class="headerlink" title="使用算法"></a>使用算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：存储决策树</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    inputTree - 已经生成的决策树</span></span><br><span class="line"><span class="string">    filename - 决策树的存储文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">"""</span>   </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        pickle.dump(inputTree, fw)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：读取决策树</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    filename - 决策树的存储文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    pickle.load(fr) - 决策树字典</span></span><br><span class="line"><span class="string">"""</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（二）- 模型评估与选择</title>
      <link href="/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89/"/>
      <url>/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="《机器学习实战》《西瓜书》笔记（二）-模型评估与选择"><a href="#《机器学习实战》《西瓜书》笔记（二）-模型评估与选择" class="headerlink" title="《机器学习实战》《西瓜书》笔记（二）- 模型评估与选择"></a>《机器学习实战》《西瓜书》笔记（二）- 模型评估与选择</h1><h2 id="经验误差与过拟合"><a href="#经验误差与过拟合" class="headerlink" title="经验误差与过拟合"></a>经验误差与过拟合</h2><ul><li><strong>错误率与精度</strong><br>错误率是分类错误的样本数占样本总数的比例，精度是分类正确的样本数占样本总数的比例。</li><li><strong>误差与经验误差</strong><br>学习器的实际预测输出与样本的真实输出之间的差异称为“误差”，学习器在训练集上的误差称为训练误差/经验误差，在新样本上的误差称为“泛化误差”。</li><li><strong>过拟合与欠拟合</strong><br>然而，当学习器把训练样本学得”太 好”了的时候，很可能巳经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下降，叫做 <strong>过拟合（overfitting)</strong><br>训练样本的一般性质尚未学好叫做 <strong>欠拟合</strong><div align = center><img src= "https://img.vim-cn.com/30/0e0722ac9770db63a1185dd505a770f82ec37a.png"></div></li></ul><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>通常我们使用测试集上样本产生的泛化误差作为评估标准，就是以“测试误差”来代表学习器的“泛化误差”</p><h2 id="划分数据集-D-与测试机-T-的常用方法"><a href="#划分数据集-D-与测试机-T-的常用方法" class="headerlink" title="划分数据集(D)与测试机(T)的常用方法"></a>划分数据集(D)与测试机(T)的常用方法</h2><h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3><p>“留出法”直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为训练集T。训练/测试集的划分要尽可能保持数据分布的一致性，例如在分类任务中至少要保持样本的类别比例相似。如果从采样的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”。<br>单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。通常将2/3~4/5的样本用于训练，剩余样本用于测试。</p><h3 id="交叉验证法"><a href="#交叉验证法" class="headerlink" title="交叉验证法"></a>交叉验证法</h3><p>先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从D中通过分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。<br>交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，通常又称为“k折交叉验证”。与留出法相似，将数据集划分为k个子集同样存在多种划分方式，为减少因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值。<br>假定数据集中包含m个样本，若令k=m，则得到了交叉验证法的一个特例：留一法。</p><div align = center><img src = "https://img.vim-cn.com/a3/a25652e1f7a9e0ef4038c3fbf67f4d31bc04be.png"></div><h3 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h3><p>为减少训练样本规模不同造成的影响，同时比较高效地进行实验估计。自助法（bootstrapping）以自助采样法为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D’：每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D’。显然，D中有一部分样本在D’中多次出现，而一部分样本不出现。通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D’中。于是将D’用作训练集，D/D’用作测试集。这样的测试结果，亦称“包外估计”。<br>没有留出法，和交叉验证法常用</p><h2 id="调参与最终模型"><a href="#调参与最终模型" class="headerlink" title="调参与最终模型"></a>调参与最终模型</h2><p>大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模 型的性能往往有显著差别.因此，在进行模型评估与选择时，除了要对适用学习 算法进行选择，还需对算法参数进行设定，这就是通常所说的”参数调节”或 简称”调参” (parameter tuning).<br>现实中常用的做法?是对每个参数选定一个 范围和变化步长，例如在 [0 ，0.2] 范围内以 0.05 为步长，则实际要评估的候选参 数值有 5 个，最终是从这 5 个候选值中产生选定值.显然，这样选定的参数值往 往不是”最佳”值，但这是在计算开销和性能估计之间进行折中的结果，通过 这个折中，学习过程才变得可行.事实上，即便在进行这样的折中后，调参往往 仍很困难.可以简单估算一下:假定算法有 3 个参数，每个参数仅考虑 5 个候选 值，这样对每一组训练/测试集就有 53 = 125 个模型需考察<br>另外，需注意的是，我们通常把学得模型在实际使用中遇到的数据称为测 试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为”验 证集” (validation set). 例如，在研究对比不同算法的泛化性能时，我们用测试 集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分 为训练集和验证集，基于验证集上的性能来进行模型选择和调参.</p><h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><p>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure).</p><h3 id="错误率与精度"><a href="#错误率与精度" class="headerlink" title="错误率与精度"></a>错误率与精度</h3><p>错误率是分类错误的样本数占样本总数的比例，精度是分类正确的样本数占样本总数的比例。</p><h3 id="查准率、查全率、F1"><a href="#查准率、查全率、F1" class="headerlink" title="查准率、查全率、F1"></a>查准率、查全率、F1</h3><p>对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反倒(true negative) 、 假反例(false negative)四种情形，令 TP、 FP、 TN、 FN 分别表示其对应的 样例数，则显然有 TP+FP+TN+FN=样例总数.分类结果的”泪淆矩 阵” (co时usion matrix),如下表所示</p><div align = center><img src = "https://img.vim-cn.com/42/14fe59f1eb909caaeb22588f2f4594c29c6682.png"></div>准率和查全率是一对矛盾的度量。查准率-查全率曲线，简称“P-R曲线”。在进行比较时，若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者。如果两个学习器的P-R曲线交叉，难以一般性断言两者孰优孰劣。比较合理的判据是比较P-R曲线下面积的大小。为综合考虑查准率、查全率的性能度量，“平衡点”即“查准率=查全率”时的取值，更常用的是F1度量。当对查准率和查全率的重视程度有所不同，F1度量的一般形式Fβ.　β>0度量了查全率对查准率的相对重要性。β=1时退化为标准的F1，β>1时查全率有更大影响，β<1,查准率有更大影响。对于有多个二分类混淆矩阵，可以在各混淆矩阵上分别计算查准率和查全率，再计算平均值，这样就得到“宏查准率”、“宏查全率”以及“宏F1”；还可将各混淆矩阵的对应元素进行平均，得到TP、FP、TN、 FN的平均值，再基于这些平均值计算出“微查准率”、“微查全率”和“微F1”。<div align = center><img src = "https://img.vim-cn.com/4e/de127ae4820dca223d54d1e8cf1fb9eab42d37.png"></div><h3 id="ROC-与-RUC"><a href="#ROC-与-RUC" class="headerlink" title="ROC 与 RUC"></a>ROC 与 RUC</h3><p>很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与 神经网络参几第 5 章<br>一个分类阔值(threshold)进行比较，若大于|词值则分为正类，否则为反类.例 如，神经网络在一般情形下是对每个测试样本预测出一个 [0.0 ，1.0] 之间的实值， 然后将这个值与 0.5 进行比较，大于 0.5 则判为正例，否则为反例.这个实值或 概率预测结果的好坏，直接决定了学习器的泛化能力.实际上?根据这个实值或 概率预测结果，我们可将测试样本进行排序，”最可能”是正例的排在最前面， “最不可能”是正例的排在最后面.这样，分类过程就相当于在这个排序中以 某个”截断点” (cut point)将样本分为两部分，前一部分判作正例，后一部分则 判作反例.<br>我们根据学习器的预 测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算 出两个重要量的值，分别以它们为横、纵坐标作图’就得到了 “ROC 曲线 与 P卫-R 曲线使用查准率、查全率为纵、横轴不同， ROC 曲线的纵轴是”真正 例率” (True Positive Rate，简称 TPR)，横轴是”假正例率” (False Positive Rate，简称 FPR)，基于表 2.1 中的符号，两者分别定义为,<br>$TPR= \frac{TP} {TP+FN}$<br>$FPR= \frac{FP} {TN+FP}$</p><p>进行学习器的比较时， 与 P-R 图相似， 若一个学习器的 ROC 曲线被另一<br>个学习器的曲线完全”包住”， 则可断言后者的性能优于前者;若两个学习器 的 ROC 曲线发生交叉，则难以-般性地断言两者孰优孰劣. 此时如果一定要进 行比较， 则较为合理的判据是比较 ROC 曲线下的面积，即 AUC (Area Under ROC Curve)</p><h3 id="代价敏感错误率与代价曲线"><a href="#代价敏感错误率与代价曲线" class="headerlink" title="代价敏感错误率与代价曲线"></a>代价敏感错误率与代价曲线</h3><div align = center><img src = "https://img.vim-cn.com/a7/92725fd772dbf66d9a108906ca5154fa42081a.png"></div><div align = center><img src = "https://img.vim-cn.com/1b/655c2c88f7541eaef782c9fc1b2611c6174fc8.png"></div><div align = center><img src = "https://img.vim-cn.com/08/4616ba375fc1fdf425257fe78da623dd23ce4d.png"></div><h3 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h3><p>有了实验评估方法和性能度量，看起来就能对学习器的性能进行评估比较 了:先使用某种实验评估方法测得学习器的某个性能度量结果，然后对这些结 果进行比较.但怎么来做这个”比较”呢?是直接取得性能度量的值然后”比 大小”吗?实际上，机器学习中性能比较这件事要比大家想象的复杂得多.这 里面涉及几个重要因素:首先，我们希望比较的是泛化性能，然而通过实验评估 方法我们获得的是测试集上的性能，两者的对比结果可能未必相同;第二，测试 集上的性能与测试集本身的选择有很大关系，且不论使用不同大小的测试集会 得到不同的结果，即使用相同大小的测试集?若包含的测试样例不同，测试结果 也会有不同;第二，很多机器学习算法本身有一定的随机性，即便用相同的参数 设置在同一个测试集上多次运行，其结果也会有不同.那么，有没有适当的方法 对学习器的性能进行比较呢? 统计假设检验(hypothesis test)为我们进行学习器t性能比较提供了重要依 据.基于假设检验结果我们可推断出，若在测试集上观察到学习器 A 比 B 好， 则 A 的泛化性能是否在统计意义上优于 B，以及这个结论的把握有多大.下面 我们先介绍两种最基本的假设检验，然后介绍几种常用的机器学习性能比较方 法.为便于讨论，本节默认以错误率为性能度量，用 E 表示.</p><h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><p>偏差方差分解试图对学习算法的期望泛化错误率进行拆解.我们知道，算 法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布. 对测试样本队令 YD 为 m 在数据集中的标记， y 为 2 的真实标记， f(x; D) 为训 练集 D 上学得模型 f 在 m 上的预测输出.<br><strong>期望输出与真实标记的差别称为偏差(bias)</strong></p><div align = center><img src = "https://img.vim-cn.com/11/682d2fceff5b7f82736f9d08f3646f50cc1fe3.png"></div><p><strong>泛化误差为偏差与噪声值与方差之和</strong></p><div align = center><img src = "https://img.vim-cn.com/5e/aa3982956888fcbd37ef36f06d6cde0e0c4b36.png"></div>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（三）- KNN</title>
      <link href="/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883-KNN)/"/>
      <url>/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883-KNN)/</url>
      
        <content type="html"><![CDATA[<h1 id="《机器学习实战》《西瓜书》笔记（三）-KNN"><a href="#《机器学习实战》《西瓜书》笔记（三）-KNN" class="headerlink" title="《机器学习实战》《西瓜书》笔记（三）- KNN"></a>《机器学习实战》《西瓜书》笔记（三）- KNN</h1><h2 id="KNN原理"><a href="#KNN原理" class="headerlink" title="KNN原理"></a>KNN原理</h2><ol><li>输入带有标签的训练集</li><li>输入没有标签的新数据</li><li>算法将输入数据的特征与训练集的数据的特征进行比较</li><li>求新数据与样本集中数据的距离</li><li>算法提取样本集中最相似数据（最近邻）的分类标签，只选择前K个最相似的数据</li><li>选取k个相似数据频率最多的分类属性作为新数据的分类属性</li></ol><h2 id="KNN伪代码"><a href="#KNN伪代码" class="headerlink" title="KNN伪代码"></a>KNN伪代码</h2><ol><li>计算已知类别数据集中点与当前点的距离</li><li>按照距离从小到大递增排序</li><li>选取与当前点距离最小的k个点</li><li>确定前k个点所在类别出现的频率</li><li>返回前k个点出现频率最高的类别作为当前点的预测分类</li></ol><h2 id="KNN源代码"><a href="#KNN源代码" class="headerlink" title="KNN源代码"></a>KNN源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    <span class="comment"># numpy函数shape[0]返回dataSet的行数</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 将inX重复dataSetSize次并排成一列</span></span><br><span class="line">    diffMat = np.tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</span><br><span class="line">    <span class="comment"># 二维特征相减后平方（用diffMat的转置乘diffMat）</span></span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    <span class="comment"># sum()所有元素相加，sum(0)列相加，sum(1)行相加</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 开方，计算出距离</span></span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span></span><br><span class="line">    <span class="comment"># argsort函数返回的是distances值从小到大的--索引值</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    <span class="comment"># 定义一个记录类别次数的字典</span></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="comment"># 选择距离最小的k个点</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        <span class="comment"># 取出前k个元素的类别</span></span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        <span class="comment"># 字典的get()方法，返回指定键的值，如果值不在字典中返回0</span></span><br><span class="line">        <span class="comment"># 计算类别次数</span></span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># python3中用items()替换python2中的iteritems()</span></span><br><span class="line">    <span class="comment"># key = operator.itemgetter(1)根据字典的值进行排序</span></span><br><span class="line">    <span class="comment"># key = operator.itemgetter(0)根据字典的键进行排序</span></span><br><span class="line">    <span class="comment"># reverse降序排序字典</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),\</span><br><span class="line">                              key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 返回次数最多的类别，即所要分类的类别</span></span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>越预测数据所在的分类</strong><br>在终端中的交互解释器执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; classify([<span class="number">1.0</span>, <span class="number">1.0</span>], group, lables, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>即可测试得到一个结果</p><h2 id="简单的一个示例"><a href="#简单的一个示例" class="headerlink" title="简单的一个示例"></a>简单的一个示例</h2><h3 id="数据的准备"><a href="#数据的准备" class="headerlink" title="数据的准备"></a>数据的准备</h3><p>在KNN的模块中添加一个实现数据样本的函数，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1.1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br></pre></td></tr></table></figure><p>在交互解释器中执行:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; <span class="keyword">import</span> KNN</span><br><span class="line">&gt;&gt; group, lables = KNN.creatDataSet()</span><br><span class="line">&gt;&gt; KNN.classify0([<span class="number">0</span>,<span class="number">0</span>], group, labels, <span class="number">3</span>)</span><br><span class="line">&gt;&gt; B</span><br></pre></td></tr></table></figure><p>可生成一个初步应用于KNN的4维2列的数据集</p><h2 id="如何测试分类器"><a href="#如何测试分类器" class="headerlink" title="如何测试分类器"></a>如何测试分类器</h2><p>使用分类器的错误率，即分类器给出的错误结果除以测试执行的综述来判断分类器的好坏</p><h2 id="示例1-KNN进行约会匹配"><a href="#示例1-KNN进行约会匹配" class="headerlink" title="示例1 KNN进行约会匹配"></a>示例1 KNN进行约会匹配</h2><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ol><li>收集数据 ： 导入文本文件</li><li>准备数据: 使用python解析</li><li>分析数据： matplotlib绘图</li><li>训练算法：k-近邻不适用</li><li>测试算法</li><li>使用算法<h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：打开解析文件，对数据进行分类，1代表不喜欢，2代表魅力一般，3代表极具魅力</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    filename - 文件名</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    returnMat - 特征矩阵</span></span><br><span class="line"><span class="string">    classLabelVector - 分类label向量</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="comment"># 打开文件</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="comment"># 读取文件所有内容</span></span><br><span class="line">    arrayOlines = fr.readlines()</span><br><span class="line">    <span class="comment"># 得到文件行数</span></span><br><span class="line">    numberOfLines = len(arrayOlines)</span><br><span class="line">    <span class="comment"># 返回的NumPy矩阵numberOfLines行，3列</span></span><br><span class="line">    returnMat = np.zeros((numberOfLines, <span class="number">3</span>))</span><br><span class="line">    <span class="comment"># 创建分类标签向量</span></span><br><span class="line">    classLabelVector = []</span><br><span class="line">    <span class="comment"># 行的索引值</span></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 读取每一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> arrayOlines:</span><br><span class="line">        <span class="comment"># 去掉每一行首尾的空白符，例如'\n','\r','\t',' '</span></span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># 将每一行内容根据'\t'符进行切片,本例中一共有4列</span></span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="comment"># 将数据的前3列进行提取保存在returnMat矩阵中，也就是特征矩阵</span></span><br><span class="line">        returnMat[index,:] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 根据文本内容进行分类1：不喜欢；2：一般；3：喜欢</span></span><br><span class="line">        <span class="keyword">if</span> listFromLine[<span class="number">-1</span>] == <span class="string">'didntLike'</span>:</span><br><span class="line">            classLabelVector.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> listFromLine[<span class="number">-1</span>] == <span class="string">'smallDoses'</span>:</span><br><span class="line">            classLabelVector.append(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">elif</span> listFromLine[<span class="number">-1</span>] == <span class="string">'largeDoses'</span>:</span><br><span class="line">            classLabelVector.append(<span class="number">3</span>)</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 返回标签列向量以及特征矩阵</span></span><br><span class="line">    <span class="keyword">return</span> returnMat, classLabelVector</span><br></pre></td></tr></table></figure></li></ol><h3 id="分析数据，使用matplotlib进行画图"><a href="#分析数据，使用matplotlib进行画图" class="headerlink" title="分析数据，使用matplotlib进行画图"></a>分析数据，使用matplotlib进行画图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：可视化数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    datingDataMat - 特征矩阵</span></span><br><span class="line"><span class="string">    datingLabels - 分类Label</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-13</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showdatas</span><span class="params">(datingDataMat, datingLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 设置汉字格式为14号简体字</span></span><br><span class="line">    font = FontProperties(fname=<span class="string">r"C:\Windows\Fonts\simsun.ttc"</span>, size=<span class="number">14</span>)</span><br><span class="line">    <span class="comment"># 将fig画布分隔成1行1列，不共享x轴和y轴，fig画布的大小为（13，8）</span></span><br><span class="line">    <span class="comment"># 当nrows=2，ncols=2时，代表fig画布被分为4个区域，axs[0][0]代表第一行第一个区域</span></span><br><span class="line">    fig, axs = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, sharex=<span class="literal">False</span>, sharey=<span class="literal">False</span>, figsize=(<span class="number">13</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="comment"># 获取datingLabels的行数作为label的个数</span></span><br><span class="line">    <span class="comment"># numberOfLabels = len(datingLabels)</span></span><br><span class="line">    <span class="comment"># label的颜色配置矩阵</span></span><br><span class="line">    LabelsColors = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> datingLabels:</span><br><span class="line">        <span class="comment"># didntLike</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">            LabelsColors.append(<span class="string">'black'</span>)</span><br><span class="line">        <span class="comment"># smallDoses</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">2</span>:</span><br><span class="line">            LabelsColors.append(<span class="string">'orange'</span>)</span><br><span class="line">        <span class="comment"># largeDoses</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">            LabelsColors.append(<span class="string">'red'</span>)</span><br><span class="line">    <span class="comment"># 画出散点图，以datingDataMat矩阵第一列为x，第二列为y，散点大小为15, 透明度为0.5</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">0</span>].scatter(x=datingDataMat[:,<span class="number">0</span>], y=datingDataMat[:,<span class="number">1</span>], color=LabelsColors, s=<span class="number">15</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 设置标题，x轴label， y轴label</span></span><br><span class="line">    axs0_title_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_title(<span class="string">u'每年获得的飞行常客里程数与玩视频游戏所消耗时间占比'</span>, FontProperties=font)</span><br><span class="line">    axs0_xlabel_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_xlabel(<span class="string">u'每年获得的飞行常客里程数'</span>, FontProperties=font)</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_ylabel(<span class="string">u'玩视频游戏所消耗时间占比'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs0_title_text, size=<span class="number">9</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">    plt.setp(axs0_xlabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 画出散点图，以datingDataMat矩阵第一列为x，第三列为y，散点大小为15, 透明度为0.5</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">1</span>].scatter(x=datingDataMat[:,<span class="number">0</span>], y=datingDataMat[:,<span class="number">2</span>], color=LabelsColors, s=<span class="number">15</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 设置标题，x轴label， y轴label</span></span><br><span class="line">    axs1_title_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_title(<span class="string">u'每年获得的飞行常客里程数与每周消费的冰淇淋公升数'</span>, FontProperties=font)</span><br><span class="line">    axs1_xlabel_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_xlabel(<span class="string">u'每年获得的飞行常客里程数'</span>, FontProperties=font)</span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_ylabel(<span class="string">u'每周消费的冰淇淋公升数'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs1_title_text, size=<span class="number">9</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">    plt.setp(axs1_xlabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 画出散点图，以datingDataMat矩阵第二列为x，第三列为y，散点大小为15, 透明度为0.5</span></span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">0</span>].scatter(x=datingDataMat[:,<span class="number">1</span>], y=datingDataMat[:,<span class="number">2</span>], color=LabelsColors, s=<span class="number">15</span>, alpha=<span class="number">.5</span>)</span><br><span class="line">    <span class="comment"># 设置标题，x轴label， y轴label</span></span><br><span class="line">    axs2_title_text = axs[<span class="number">1</span>][<span class="number">0</span>].set_title(<span class="string">u'玩视频游戏所消耗时间占比与每周消费的冰淇淋公升数'</span>, FontProperties=font)</span><br><span class="line">    axs2_xlabel_text = axs[<span class="number">1</span>][<span class="number">0</span>].set_xlabel(<span class="string">u'玩视频游戏所消耗时间占比'</span>, FontProperties=font)</span><br><span class="line">    axs2_ylabel_text = axs[<span class="number">1</span>][<span class="number">0</span>].set_ylabel(<span class="string">u'每周消费的冰淇淋公升数'</span>, FontProperties=font)</span><br><span class="line">    plt.setp(axs2_title_text, size=<span class="number">9</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'red'</span>)</span><br><span class="line">    plt.setp(axs2_xlabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs2_ylabel_text, size=<span class="number">7</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment"># 设置图例</span></span><br><span class="line">    didntLike = mlines.Line2D([], [], color=<span class="string">'black'</span>, marker=<span class="string">'.'</span>, markersize=<span class="number">6</span>, label=<span class="string">'didntLike'</span>)</span><br><span class="line">    smallDoses = mlines.Line2D([], [], color=<span class="string">'orange'</span>, marker=<span class="string">'.'</span>, markersize=<span class="number">6</span>, label=<span class="string">'smallDoses'</span>)</span><br><span class="line">    largeDoses = mlines.Line2D([], [], color=<span class="string">'red'</span>, marker=<span class="string">'.'</span>, markersize=<span class="number">6</span>, label=<span class="string">'largeDoses'</span>)</span><br><span class="line">    <span class="comment"># 添加图例</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">0</span>].legend(handles=[didntLike, smallDoses, largeDoses])</span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">1</span>].legend(handles=[didntLike, smallDoses, largeDoses])</span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">0</span>].legend(handles=[didntLike, smallDoses, largeDoses])</span><br><span class="line">    <span class="comment"># 显示图片</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="准备数据，归一化"><a href="#准备数据，归一化" class="headerlink" title="准备数据，归一化"></a>准备数据，归一化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：对数据进行归一化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataSet - 特征矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    normDataSet - 归一化后的特征矩阵</span></span><br><span class="line"><span class="string">    ranges - 数据范围</span></span><br><span class="line"><span class="string">    minVals - 数据最小值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-13</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 获取数据的最小值</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 获取数据的最大值</span></span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 最大值和最小值的范围</span></span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    <span class="comment"># shape(dataSet)返回dataSet的矩阵行列数</span></span><br><span class="line">    normDataSet = np.zeros(np.shape(dataSet))</span><br><span class="line">    <span class="comment"># numpy函数shape[0]返回dataSet的行数</span></span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 原始值减去最小值（x-xmin）</span></span><br><span class="line">    normDataSet = dataSet - np.tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 差值处以最大值和最小值的差值（x-xmin）/（xmax-xmin）</span></span><br><span class="line">    normDataSet = normDataSet / np.tile(ranges, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 归一化数据结果，数据范围，最小值</span></span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure><h3 id="测试算法，作为完整程序验证分类器"><a href="#测试算法，作为完整程序验证分类器" class="headerlink" title="测试算法，作为完整程序验证分类器"></a>测试算法，作为完整程序验证分类器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：分类器测试函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    normDataSet - 归一化后的特征矩阵</span></span><br><span class="line"><span class="string">    ranges - 数据范围</span></span><br><span class="line"><span class="string">    minVals - 数据最小值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-13</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 打开文件名</span></span><br><span class="line">    filename = <span class="string">"datingTestSet.txt"</span></span><br><span class="line">    <span class="comment"># 将返回的特征矩阵和分类向量分别存储到datingDataMat和datingLabels中</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(filename)</span><br><span class="line">    <span class="comment"># 取所有数据的10% hoRatio越小，错误率越低</span></span><br><span class="line">    hoRatio = <span class="number">0.10</span></span><br><span class="line">    <span class="comment"># 数据归一化，返回归一化数据结果，数据范围，最小值</span></span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    <span class="comment"># 获取normMat的行数</span></span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 10%的测试数据的个数</span></span><br><span class="line">    numTestVecs = int(m * hoRatio)</span><br><span class="line">    <span class="comment"># 分类错误计数</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        <span class="comment"># 前numTestVecs个数据作为测试集，后m-numTestVecs个数据作为训练集</span></span><br><span class="line">        <span class="comment"># k选择label数+1（结果比较好）</span></span><br><span class="line">        classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:],\</span><br><span class="line">                                     datingLabels[numTestVecs:m], <span class="number">4</span>)</span><br><span class="line">        print(<span class="string">"分类结果:%d\t真实类别:%d"</span> % (classifierResult, datingLabels[i]))</span><br><span class="line">        <span class="keyword">if</span> classifierResult != datingLabels[i]:</span><br><span class="line">            errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"错误率:%f%%"</span> % (errorCount/float(numTestVecs)*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><h3 id="使用算法"><a href="#使用算法" class="headerlink" title="使用算法"></a>使用算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">函数说明：通过输入一个人的三围特征，进行分类输出</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2018-07-14</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    resultList = [<span class="string">'讨厌'</span>, <span class="string">'有些喜欢'</span>, <span class="string">'非常喜欢'</span>]</span><br><span class="line">    <span class="comment"># 三维特征用户输入</span></span><br><span class="line">    percentTats = float(input(<span class="string">"玩视频游戏所消耗时间百分比："</span>))</span><br><span class="line">    ffMiles = float(input(<span class="string">"每年获得的飞行常客里程数："</span>))</span><br><span class="line">    iceCream = float(input(<span class="string">"每周消费的冰淇淋公升数："</span>))</span><br><span class="line">    <span class="comment"># 打开的文件名</span></span><br><span class="line">    filename = <span class="string">"datingTestSet.txt"</span></span><br><span class="line">    <span class="comment"># 打开并处理数据</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(filename)</span><br><span class="line">    <span class="comment"># 训练集归一化</span></span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    <span class="comment"># 生成NumPy数组，测试集</span></span><br><span class="line">    inArr = np.array([percentTats, ffMiles, iceCream])</span><br><span class="line">    <span class="comment"># 测试集归一化</span></span><br><span class="line">    norminArr = (inArr - minVals) / ranges</span><br><span class="line">    <span class="comment"># 返回分类结果</span></span><br><span class="line">    classifierResult = classify0(norminArr, normMat, datingLabels, <span class="number">4</span>)</span><br><span class="line">    <span class="comment"># 打印结果</span></span><br><span class="line">    print(<span class="string">"你可能%s这个人"</span> % (resultList[classifierResult - <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》《西瓜书》笔记（一）</title>
      <link href="/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)/"/>
      <url>/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)/</url>
      
        <content type="html"><![CDATA[<h1 id="《机器学习实战》《西瓜书》笔记（一）"><a href="#《机器学习实战》《西瓜书》笔记（一）" class="headerlink" title="《机器学习实战》《西瓜书》笔记（一）"></a>《机器学习实战》《西瓜书》笔记（一）</h1><h2 id="机器学习的相关概念"><a href="#机器学习的相关概念" class="headerlink" title="机器学习的相关概念"></a>机器学习的相关概念</h2><p>我们要做的其实是让机器他有自己学习的能力，也就我们要做的应该<code>machine learning</code>的方向。讲的比较拟人化一点，所谓<code>machine learning</code>的方向，就是你就写段程序，然后让机器人变得了很聪明，他就能够有学习的能力。接下来，你就像教一个婴儿、教一个小孩一样的教他，你并不是写程序让他做到这件事，你是写程序让它具有学习的能力。然后接下来，你就可以用像教小孩的方式告诉它。假设你要叫他学会做语音辨识，你就告诉它这段声音是<code>“Hi”</code>，这段声音就是<code>“How are you”</code>，这段声音是<code>“Good bye”</code>。希望接下来它就学会了，你给它一个新的声音，它就可以帮你产生语音辨识的结果。<br><strong>用数学的语义去理解，机器需要一个函数对输入进行自主判断输出，以完成回归预测、分类、聚类等实际任务</strong></p><ul><li><strong>监督学习</strong><br>从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括<strong>回归分析和统计分类</strong>。</li><li><strong>无监督学习</strong><br>与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有<strong>生成对抗网络（GAN）、聚类</strong></li><li><strong>半监督学习</strong><br>介于监督学习与无监督学习之间</li><li><strong>增强学习机器</strong><br>为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的</li></ul><h2 id="开发机器学习应用程序的主要步骤"><a href="#开发机器学习应用程序的主要步骤" class="headerlink" title="开发机器学习应用程序的主要步骤"></a>开发机器学习应用程序的主要步骤</h2><ul><li>收集数据</li><li>准备输入数据（Python语言）</li><li>分析输入数据（数据处理、降维等方法）</li><li>训练算法（无监督学习不需要训练算法）</li><li>测试算法</li><li>执行算法</li></ul><h2 id="机器学习的基本术语"><a href="#机器学习的基本术语" class="headerlink" title="机器学习的基本术语"></a>机器学习的基本术语</h2><p>这组记录的集合称为一个 <strong>“数据集” (data set)</strong><br>其中每条记录是关于一个事件或对象(这里是一个西瓜)的描述，称为一个 <strong>“示例” (instance) 或”样本” (sample)</strong><br>反映事件或对象在某方面的表现或性质的事项，例如”色泽” “根蒂” “敲声”，称为”)副主” (attribute) 或 <strong>“特征”</strong>(feature)属性上的取值，例如”青绿” “乌黑”，称为”)副主值” (attribute va1ue)<br>属性张成的空间称为”属性空间” (attribute space)”样本空间” (samp1e space)或”输入空间”<br>例如我们把”色泽” “根蒂” “敲声”作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此我们也把…个示例称为一个 <strong>“特征向量” (feature vector)</strong>.<br><code>eg:</code><br>$D = {x_1,x_2,….x_m} $ 样本包含<code>m</code>个实例<br>$x_i = {x_{i1}…..x_{id}}$ 是<code>d</code>维样本空间的一个向量<br>((色泽:青绿;根蒂二蜷缩; 敲声=浊响)，好瓜)” .这里关于示例结果的信息，例如”好瓜”，称为 <strong>“标记” (labe1)</strong>; 拥有了标记信息的示例，则称为”样例” (examp1e).</p><h2 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h2><p>归纳是从特殊到一般的“泛化”过程，演绎是从一般到特殊的“特化”过程。<br>学习的目的是“泛化”，即通过对训练集中瓜的学习已获得对没见过的瓜进行判断的能力。<br>学习过程看作一个在所以假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确实，假设空间及其规模大小就却确定了。 </p><p>我们用 m 表示这 个假设.这样，若”色泽” “根蒂” “敲声”分别有3、 2、 2 种可能取值，则我 们面临的假设空间规模大小为 4 x 3 x 3 + 1 = 37.</p><h2 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h2><p>现在有三个与训练集一致的假设，但与他们对应的模型在面临新样本的时候，却会产生不同的输出。根据仅有的训练样本无法判断三个假设中哪个“更好”。对于一个具体的学习算法而言，它必须要产生一个模型，这时，学习算法本身的“偏好”起到关键左右。例如，若算法喜欢“尽可能特殊”的模型，则会有相应的模型产生。机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。<br>任何一个有效的机器学习算法必有其归纳偏好，否则产生的模型每次在进行预测时随机抽选训练集上的等效假设，学得模型结果不一，显然没有意义。</p><p>归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进 行选择的启发式或”价值观”那么，有没有一般性的原则来引导算法确立 “正确的”偏好呢? “奥卡姆剃刀” (Occam’s razor)是一种常用的、自然科学 研究中最基本的原则，即”若有多个假设与观察一致，则选最简单的那个”如 果采用这个原则，并且假设我们认为”更平滑”意味着”更简单” (例如曲线 A 更易于描述，其方程式是 $y = x2+ 6x + 1$ ，而曲线 B 则要复杂得多)，则在 图1.3 中我们会自然地偏好”平滑”的曲线 A.</p><h2 id="生成式模型与判别式模型"><a href="#生成式模型与判别式模型" class="headerlink" title="生成式模型与判别式模型"></a>生成式模型与判别式模型</h2><ul><li><p>产生式模型<br>从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。</p></li><li><p>判别式模型<br>寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p></li></ul><p><strong>区别：</strong></p><p>假设有样本输入值（或者观察值）x，类别标签（或者输出值）y</p><p>判别式模型评估对象是最大化条件概率p(y|x)并直接对其建模，</p><p>生成式模型评估对象是最大化联合概率p(x,y)并对其建模。</p><p>其实两者的评估目标都是要得到最终的类别标签Y， 而Y=argmax p(y|x)，不同的是判别式模型直接通过解在满足训练样本分布下的最优化问题得到模型参数，主要用到拉格朗日乘算法、梯度下降法，常见的判别式模型如最大熵模型、CRF、LR、SVM等；</p><p>而生成式模型先经过贝叶斯转换成Y = argmax p(y|x) = argmax p(x|y)*p(y)，然后分别学习p(y)和p(x|y)的概率分布，主要通过极大似然估计的方法学习参数，如NGram、HMM、Naive Bayes。</p><p><strong>优缺点：</strong></p><ul><li>生成模型：</li></ul><p>优点：<br>1）实际上带的信息要比判别模型丰富，研究单类问题比判别模型灵活性强<br>2）模型可以通过增量学习得到<br>3）生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。</p><p>缺点：<br>1）学习过程比较复杂。<br>2）实践中多数情况下判别模型效果更好。</p><ul><li>判别模型：</li></ul><p>优点：<br>1）分类边界更灵活，比使用纯概率方法或生产模型得到的更高级.<br>2）准确率往往较生成模型高。<br>3）不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。</p><p>缺点：<br>1）不能反映训练数据本身的特性。</p>]]></content>
      
      
      <categories>
          
          <category> Machine_Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>色彩搭配与设计</title>
      <link href="/2019/11/26/%E8%AE%BE%E8%AE%A1/%E5%85%B3%E4%BA%8E%E9%A2%9C%E8%89%B2/"/>
      <url>/2019/11/26/%E8%AE%BE%E8%AE%A1/%E5%85%B3%E4%BA%8E%E9%A2%9C%E8%89%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h1><h2 id="Material-design中的color"><a href="#Material-design中的color" class="headerlink" title="Material design中的color"></a>Material design中的color</h2><p><div><img src = "https://img.vim-cn.com/65/c21d40de1a5e93d9dbfd5f35e17f18d52b931f.webp"></div><br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--Material Colors--&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorRed"</span>&gt;</span>#f44336<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorPink"</span>&gt;</span>#e91e63<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorPurple"</span>&gt;</span>#9c27b0<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorDeepPurple"</span>&gt;</span>#673ab7<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorIndigo"</span>&gt;</span>#3f51b5<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorBlue"</span>&gt;</span>#2196f3<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorLightBlue"</span>&gt;</span>#03a9f4<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorCyan"</span>&gt;</span>#00bcd4<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorTeal"</span>&gt;</span>#009688<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorGreen"</span>&gt;</span>#4caf50<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorLightGreen"</span>&gt;</span>#8bc34a<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorLime"</span>&gt;</span>#cddc39<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorYellow"</span>&gt;</span>#FFeb3b<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorAmber"</span>&gt;</span>#FFc107<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorOrange"</span>&gt;</span>#FF9800<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorDeepOrange"</span>&gt;</span>#FF5722<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorBrown"</span>&gt;</span>#795548<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorGrey"</span>&gt;</span>#9e9e9e<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"materialColorBlueGrey"</span>&gt;</span>#607d8b<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="comment">&lt;!--Text Colors--&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"primaryText"</span>&gt;</span>#212121<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"secondaryText"</span>&gt;</span>#757575<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">color</span> <span class="attr">name</span>=<span class="string">"dividerColor"</span>&gt;</span>#bdbdbd<span class="tag">&lt;/<span class="name">color</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="色彩的巧妙搭配"><a href="#色彩的巧妙搭配" class="headerlink" title="色彩的巧妙搭配"></a>色彩的巧妙搭配</h2><p><div align = center><img src = "https://img.vim-cn.com/21/b362eb8232618c5eb6692fef2cb5f1cf24aa63.png"></div><br><br></p><p><strong>几个色彩网站推荐</strong></p><p>拼色网站1： <a href="https://colordrop.io/" target="_blank" rel="noopener">https://colordrop.io/</a><br>拼色网站2：<a href="http://www.peise.net/tools/web/#" target="_blank" rel="noopener">http://www.peise.net/tools/web/#</a><br>RGB色值对照表 ：<a href="https://tool.oschina.net/commons?type=3" target="_blank" rel="noopener">https://tool.oschina.net/commons?type=3</a></p><p><strong>一个免费图片网站</strong><br><a href="https://unsplash.com/" target="_blank" rel="noopener">https://unsplash.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> color </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础（5）</title>
      <link href="/2019/11/26/java/shyjava(5)/"/>
      <url>/2019/11/26/java/shyjava(5)/</url>
      
        <content type="html"><![CDATA[<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="方法的定义"><a href="#方法的定义" class="headerlink" title="方法的定义"></a>方法的定义</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">MethodDeclaration:</span><br><span class="line">MethodHeader MethodBody</span><br><span class="line">MethodHeader:</span><br><span class="line"><span class="function">Modifiersopt ResultType <span class="title">Identifier</span><span class="params">(FormalParameterListopt)</span> Throwsopt</span></span><br><span class="line"><span class="function">Modifiers:</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">protected</span> <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">abstract</span> <span class="keyword">final</span></span></span><br><span class="line"><span class="function"><span class="keyword">synchronized</span> <span class="keyword">native</span> <span class="keyword">strictfp</span></span></span><br><span class="line"><span class="function">ResultType:</span></span><br><span class="line"><span class="function">Type</span></span><br><span class="line"><span class="function"><span class="keyword">void</span></span></span><br><span class="line"><span class="function">MethodBody:</span></span><br><span class="line"><span class="function"></span>&#123; statements &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(num1 &gt; num2)</span><br><span class="line">        result = num1;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        result = num2;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>方法签名(Method Signature)指方法名称、参数类型、参数数量和返回类型。一个类中不能包含签名相同或仅返回类型不同的多个方法。</li><li>方法头中声明的变量称为形参(formal parameter)。当调用方法时，可向形参传递一个值，这个值称为实参(actual parameter / argument)。形参可以使用final进行修饰，表示方法内部不允许修改该参数。</li><li>形参不允许有默认值，最后一个可为变长参数（可用…或数组定义，参见第7章数组）。方法不允许static局部变量。</li><li>方法可以有一个返回值(return value)。如果方法没有返回值，返回值类型为void，但构造函数确实没有返回值。</li></ul><h2 id="方法的调用"><a href="#方法的调用" class="headerlink" title="方法的调用"></a>方法的调用</h2><ul><li>声明方法只给出方法的定义。要执行方法，必须调用(call/invoke)方法。</li><li>如果方法有返回值，通常将方法调用作为一个值来处理（可放在一个表达式里）。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> large = max(<span class="number">3</span>, <span class="number">4</span>) * <span class="number">2</span>;  </span><br><span class="line">System.out.println(max(<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">如果方法没有返回值，方法调用必须是一条语句。</span><br><span class="line">System.out.println(“Welcome to Java!”);</span><br></pre></td></tr></table></figure></li><li>当调用方法时，程序控制权转移至被调用的方法。当执行return语句或到达方法结尾时，程序控制权转移至调用者。</li><li>调用当前类中的静态方法：可直接用“方法名”，也可用”类名.方法名“；实例函数中也可用” 方法名“或”this.方法名“调用。</li><li>调用其它类中的静态方法：必须用”类名.方法名“或”对象.方法名“调用；子类实例函数也可用” super.方法名“调用父类方法。</li><li>所有静态方法提倡用”类名.方法名“调用。如<code>Math.sin(3.0)</code></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMax</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">int</span> k = max(i, j);</span><br><span class="line">System.out.println(<span class="string">"The maximum between "</span> + i + <span class="string">" and "</span> + j + <span class="string">" is "</span> + k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> result;</span><br><span class="line">result = (num1 &gt; num2) ?num1:num2;</span><br><span class="line"><span class="keyword">return</span> result ;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="调用程序栈"><a href="#调用程序栈" class="headerlink" title="调用程序栈"></a>调用程序栈</h2><p>每当调用一个方法时，系统将参数、局部变量存储在一个内存区域中，这个内存区域称为调用堆栈(call stack)。当方法结束返回到调用者时，系统自动释放相应的调用栈。</p><div align = center><img src = "https://img.vim-cn.com/9c/1276a008ebc0f1d635060c80096666064be1de.png"></div><h2 id="方法的参数传递"><a href="#方法的参数传递" class="headerlink" title="方法的参数传递"></a>方法的参数传递</h2><ul><li><p>如果方法声明中包含形参，调用方法时，必须提供实参。<br>实参的类型必须与形参的类型兼容：如父类形参可用子类实参。<br>实参顺序必须与形参的顺序一致。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">nPrintln</span><span class="params">(String message, <span class="keyword">int</span> n)</span> </span>&#123;  </span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    System.out.println(message);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">nPrintln(“Hello”, <span class="number">3</span>); <span class="comment">//正确</span></span><br><span class="line">nPrintln(<span class="number">3</span>, “Hello”); <span class="comment">//错误</span></span><br></pre></td></tr></table></figure></li><li><p>当调用方法时，基本数据类型的实参值的副本被传递给方法的形参。方法内部对形参的修改不影响实参值。(Call by value)<br>对象类型的参数是引用调用（Call by reference）</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestPassByValue</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> num1 = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> num2 = <span class="number">2</span>;</span><br><span class="line">System.out.println(<span class="string">"调用swap方法之前：num1 = "</span> + num1 + <span class="string">"，num2 = "</span> + num2);</span><br><span class="line"></span><br><span class="line">swap(num1, num2);</span><br><span class="line">System.out.println(<span class="string">"调用swap方法之后：num1 = "</span> + num1 + <span class="string">"，num2 = "</span> + num2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> n1, <span class="keyword">int</span> n2)</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"\t在swap方法内："</span>);</span><br><span class="line">System.out.println(<span class="string">"\t\t交换之前：n1 = "</span> + n1 + <span class="string">"，n2 = "</span> + n2);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> temp = n1;</span><br><span class="line">n1 = n2;</span><br><span class="line">n2 = temp;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"\t\t交换之后：n1 = "</span> + n1 + <span class="string">"，n2 = "</span> + n2);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法的重载"><a href="#方法的重载" class="headerlink" title="方法的重载"></a>方法的重载</h2><ul><li>方法重载(overloading)是指方法名称相同，但方法签名不同的方法，仅返回类型不同的方法不可重载。一个类中可以包含多个重载的方法。</li><li>当调用方法时，Java编译器会根据实参的个数和类型寻找最合适的方法进行调用。</li><li>调用时匹配成功的方法可能多于一个，则会产生编译二义性错误，称为歧义调用(ambiguous invocation）<br><strong>重载示例</strong><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMethodOverloading</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Return the max between two int values */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMethodOverloading</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/** Return the max between two int values */</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> (num1 &gt; num2) ？num1:num2; </span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Return the max between two double values */</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">max</span><span class="params">(<span class="keyword">double</span> num1, <span class="keyword">double</span> num2)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> (num1 &gt; num2) ？num1:num2;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/** Return the max among three double values */</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">max</span><span class="params">(<span class="keyword">double</span> num1, <span class="keyword">double</span> num2, <span class="keyword">double</span> num3)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> max(max(num1, num2), num3);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestMethodOverloading</span> </span>&#123;</span><br><span class="line"> <span class="comment">/** Main method */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[ ] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">// Invoke the max method with int parameters</span></span><br><span class="line">System.out.println(<span class="string">"The maximum between 3 and 4 is "</span></span><br><span class="line">+ max(<span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Invoke the max method with the double parameters</span></span><br><span class="line">System.out.println(<span class="string">"The maximum between 3.0 and 5.4 is "</span> </span><br><span class="line">+ max(<span class="number">3.0</span>, <span class="number">5.4</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Invoke the max method with three double parameters</span></span><br><span class="line">System.out.println(<span class="string">"The maximum between 3.0, 5.4, and 10.14 is "</span> </span><br><span class="line">+ max(<span class="number">3.0</span>, <span class="number">5.4</span>, <span class="number">10.14</span>));</span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><strong>有歧义的重载</strong><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AmbiguousOverloading</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[ ] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//System.out.println(max(1, 2));  //该调用产生歧义</span></span><br><span class="line">        &#125;        <span class="comment">//以下任一函数的参数都相容（都能自动转换），编译无法确定用哪个函数</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">double</span> num2)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (num1 &gt; num2)</span><br><span class="line"><span class="keyword">return</span> num1;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> num2;</span><br><span class="line">        &#125;</span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">max</span><span class="params">(<span class="keyword">double</span> num1, <span class="keyword">int</span> num2)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (num1 &gt; num2)</span><br><span class="line"><span class="keyword">return</span> num1;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> num2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="局部变量的作用域"><a href="#局部变量的作用域" class="headerlink" title="局部变量的作用域"></a>局部变量的作用域</h2><p>方法内部声明的变量称为局部变量(local variable)。<br>局部变量的作用域(scope)指程序中可以使用该变量的部分。局部变量的生命期和其作用域相同。<br>局部变量的作用域从它的声明开始，直到包含该变量的程序块结束。局部变量在使用前必须先赋值。<br>在方法中，可以在不同的非嵌套程序块中以相同的名称多次声明局部变量。但不能在嵌套的块中以相同的名称多次声明局部变量：无法访问外部块变量。<br>在for语句的初始动作部分声明的变量，作用域是整个循环。在for语句循环体中声明的变量，作用域从变量声明开始到循环体结束<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestLocalVariable</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">method1</span><span class="params">( )</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>; <span class="keyword">int</span> y = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">x += i;  </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">y += i;              <span class="comment">//正确：两个循环未嵌套，都可用i</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//错误，变量i在嵌套的语句块中声明</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">method2</span><span class="params">( )</span> </span>&#123;</span><br><span class="line"><span class="comment">//int i = 1;</span></span><br><span class="line"><span class="comment">//int sum = 0;</span></span><br><span class="line"><span class="comment">//for (int i = 1; i &lt; 10; i++) &#123;//java不允许函数的局部变量或参数的作用域被覆盖</span></span><br><span class="line"><span class="comment">//sum += i;          //无法访问外部块局部变量</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><strong>全局变量的声明与使用</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span> <span class="title">args</span> </span>&#123;  </span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">static</span> String username; <span class="comment">// 全局变量</span></span><br><span class="line">     <span class="keyword">public</span> <span class="keyword">static</span> String password; <span class="comment">//全局变量</span></span><br><span class="line">&#125;</span><br><span class="line">&gt;&gt; args.username</span><br><span class="line">&gt;&gt; args.password</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础（4）</title>
      <link href="/2019/11/23/java/shyjava(4)/"/>
      <url>/2019/11/23/java/shyjava(4)/</url>
      
        <content type="html"><![CDATA[<h1 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h1><h2 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (i &lt; <span class="number">100</span>) &#123;</span><br><span class="line">    System.out.println(“Welcome to Java!”);</span><br><span class="line">    i++;     <span class="comment">//必须有语句改变循环条件</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="do-while循环"><a href="#do-while循环" class="headerlink" title="do while循环"></a>do while循环</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">循环体至少执行一次</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">statement or block</span><br><span class="line"><span class="keyword">while</span> (loop-continuation-condition);</span><br></pre></td></tr></table></figure><h2 id="for-循环"><a href="#for-循环" class="headerlink" title="for 循环"></a>for 循环</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">    System.out.println(“Welcome to Java!”);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>循环头中的每个部分可以是零个或多个以逗句分隔的表达式。</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>; i + j &lt; <span class="number">10</span>; i++, j++) &#123;</span><br><span class="line">    System.out.println(“Welcome to Java!”);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如果<span class="keyword">for</span>循环中的loop-continuation-condition被省略，则隐含为真。</span><br><span class="line"><span class="keyword">for</span> (;;) &#123;                   <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">    <span class="comment">//do something  等价于        //do something</span></span><br><span class="line">&#125;                            &#125;</span><br></pre></td></tr></table></figure><h2 id="break-与-continue"><a href="#break-与-continue" class="headerlink" title="break 与 continue"></a>break 与 continue</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">break</span> <span class="comment">//跳出循环</span></span><br><span class="line"><span class="keyword">continue</span> <span class="comment">//跳出当前循环，继续循环</span></span><br></pre></td></tr></table></figure><h2 id="JDK1-5-增强的for循环"><a href="#JDK1-5-增强的for循环" class="headerlink" title="JDK1.5 增强的for循环"></a>JDK1.5 增强的for循环</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">JDK <span class="number">1.5</span>引入新的<span class="keyword">for</span>循环，可以不用下标就可以依次访问数组元素。语法：</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(elementType value : arrayRefVar) &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; myList.length; i++) &#123;</span><br><span class="line"> sum += myList[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">double</span> value : myList) &#123;</span><br><span class="line">sum += value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础（3）</title>
      <link href="/2019/11/22/java/shyjava(3)/"/>
      <url>/2019/11/22/java/shyjava(3)/</url>
      
        <content type="html"><![CDATA[<h1 id="Shy-Learnjava（3）基础"><a href="#Shy-Learnjava（3）基础" class="headerlink" title="Shy-Learnjava（3）基础"></a>Shy-Learnjava（3）基础</h1><h2 id="3-数学函数、字符与字符串"><a href="#3-数学函数、字符与字符串" class="headerlink" title="3 数学函数、字符与字符串"></a>3 数学函数、字符与字符串</h2><h3 id="3-1-数学函数"><a href="#3-1-数学函数" class="headerlink" title="3.1 数学函数"></a>3.1 数学函数</h3><p><strong>Math是final类：在java.lang.Math中，所有数学函数都是静态方法</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Math类中定义了常用的数学常量，如</span><br><span class="line">PI : <span class="number">3.14159265358979323846</span></span><br><span class="line">E : <span class="number">2.7182818284590452354</span></span><br><span class="line"># 方法:注意都是静态函数</span><br><span class="line"># 三角函数</span><br><span class="line">sin, cos, tan, asin, acos, atan,toRadians,toDigrees</span><br><span class="line"># 指数</span><br><span class="line">exp, log, log10，pow, sqrt</span><br><span class="line"># 取整</span><br><span class="line">ceil, floor, round</span><br><span class="line"># 其它</span><br><span class="line">min, max, abs, random（[<span class="number">0.0</span>,<span class="number">1.0</span>))</span><br></pre></td></tr></table></figure></p><p><strong>Math.random方法生成[0.0,1.0)之间的double类型的随机数</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如：</span><br><span class="line">（<span class="keyword">int</span>）(Math.random( )*<span class="number">10</span>);<span class="comment">//[0,10)</span></span><br><span class="line"><span class="number">50</span>+(<span class="keyword">int</span>)(Math.random( )*<span class="number">50</span>);<span class="comment">//[50,100)</span></span><br><span class="line">一般地</span><br><span class="line">a+(<span class="keyword">int</span>)(Math.random( )*b)              <span class="comment">//返回[a, a+b)</span></span><br><span class="line">a+(<span class="keyword">int</span>)(Math.random( )*（b+<span class="number">1</span>）)       <span class="comment">//返回[a, a+b]</span></span><br></pre></td></tr></table></figure><br><strong>编写生成随机字符的方法</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Java中每个字符对应一个Unicode编码从<span class="number">0000</span>到FFFF。  </span><br><span class="line">在生成一个随机字符，就是产生一个从<span class="number">0</span>到<span class="number">65535</span>之间的随机数。  </span><br><span class="line">所以, 计算表达式为：</span><br><span class="line"></span><br><span class="line">(<span class="keyword">int</span>)(Math.random( ) * (<span class="number">65535</span> + <span class="number">1</span>)) 。</span><br><span class="line"></span><br><span class="line">英文大、小写字母的Unicode是一串连续的整数，如</span><br><span class="line">‘a’的统一码是:   </span><br><span class="line">(<span class="keyword">int</span>)‘a’=<span class="number">97</span></span><br><span class="line">由于<span class="keyword">char</span>类型可自动地被转换为<span class="keyword">int</span>类型，所以我们可以对应使用如下整数值：</span><br><span class="line">‘a’=<span class="number">97</span>, ‘b’=<span class="number">98</span>， …, ‘z’=<span class="number">122</span></span><br><span class="line"></span><br><span class="line">因此，随机生成从‘a’-‘z’之间的字符就等于生成‘a’-‘z’之间的随机数，可用</span><br><span class="line">‘a’+（<span class="keyword">int</span>）(Math.Random( ) * (‘z’-’a’+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">将上面讨论一般化，按如下表达式，可以生成任意<span class="number">2</span>个字符ch1和ch2（ch1&lt;ch2）之间的随机字符</span><br><span class="line"></span><br><span class="line">(<span class="keyword">char</span>)(ch1+(<span class="keyword">int</span>)(Math.rabdom()*(ch2-ch1+<span class="number">1</span>)))</span><br></pre></td></tr></table></figure></p><h3 id="3-2-字符数据类型"><a href="#3-2-字符数据类型" class="headerlink" title="3.2 字符数据类型"></a>3.2 字符数据类型</h3><p><strong>Unicode和ASCII码</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Java对字符采用<span class="number">16</span>位Unicode编码，因此<span class="keyword">char</span>类型的大小为二个字节</span><br><span class="line"><span class="number">16</span>位的Unicode用以\u开头的<span class="number">4</span>位<span class="number">16</span>进制数表示，范围从’\u0000’到’\uffff’,不能少写位数</span><br><span class="line">Unicode包括ASCII码，从’\u0000’到’\u007f’对应<span class="number">128</span>个ASCII字符</span><br><span class="line">JAVA中的ASCII字符也可以用Unicode表示，例如</span><br><span class="line"><span class="keyword">char</span> letter = ‘A’；</span><br><span class="line"><span class="keyword">char</span> letter = ‘\u0041’；<span class="comment">//等价，\u后面必须写满4位16进制数</span></span><br><span class="line">++和--运算符也可以用在<span class="keyword">char</span>类型数据上，运算结果为该字符之后或之前的字符，例如下面的语句显示字符b</span><br><span class="line"><span class="keyword">char</span> ch = ‘a’;</span><br><span class="line">System.out.println(++ch);  <span class="comment">//显示b</span></span><br></pre></td></tr></table></figure><br><strong>特殊字符转义</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">和C++一样，采用反斜杠(\)后面加上一个字符或者一些数字位组成转义序列，一个转义序列被当做一个字符</span><br><span class="line">如\n  \t  \b  \r  \f  \\  \<span class="string">'  \"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">如果想打印带””的信息 He said “Java is fun “</span></span><br><span class="line"><span class="string">System.out.println(“He said \”Java is fun \””);</span></span><br></pre></td></tr></table></figure><br><strong>字符型和数据型的转换</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>类型数据可以转换成任意一种数值类型，反之亦然。将整数转换成<span class="keyword">char</span>类型数据时，只用到该数据的低<span class="number">16</span>位，其余被忽略。例如</span><br><span class="line"><span class="keyword">char</span> ch = （<span class="keyword">char</span>）<span class="number">0xAB0041</span>；</span><br><span class="line">       System.out.println(ch);<span class="comment">//显示A</span></span><br><span class="line">要将浮点数转成<span class="keyword">char</span>时，先把浮点数转成<span class="keyword">int</span>型，然后将整数转换成<span class="keyword">char</span></span><br><span class="line">       <span class="keyword">char</span> ch = （<span class="keyword">char</span>）<span class="number">65.25</span>；</span><br><span class="line">       System.out.println(ch);<span class="comment">//显示A</span></span><br><span class="line">当一个<span class="keyword">char</span>型转换成数值型时，这个字符的Unicode码就被转换成某种特定数据类型</span><br><span class="line">       <span class="keyword">int</span> i = （<span class="keyword">int</span>）‘A’；</span><br><span class="line">       System.out.println(i);<span class="comment">//显示65</span></span><br><span class="line"></span><br><span class="line">如果转换结果适用于目标变量（不会有精度损失），可以采用隐式转换；否则必须强制类型转换</span><br><span class="line"> <span class="keyword">int</span> i = ‘A’；</span><br><span class="line"> <span class="keyword">byte</span> b = （<span class="keyword">byte</span>）‘\uFFF4’;  <span class="comment">//取低8位二进制数</span></span><br><span class="line">所有数值运算符都可以用在<span class="keyword">char</span>型操作数上，  </span><br><span class="line">如果另一个操作数是数值，那么<span class="keyword">char</span>型操作数就自动转换为数值；  </span><br><span class="line">如果另外一个操作数是字符串，那么<span class="keyword">char</span>型操作数会自动转换成字符串再和另外一个操作数字符串相连</span><br><span class="line">      <span class="keyword">int</span> i = ‘<span class="number">2</span>’+ ‘<span class="number">3</span>’;</span><br><span class="line">      System.out.println（i）；  <span class="comment">// i为50+51=101</span></span><br><span class="line">      <span class="keyword">int</span> j = <span class="number">2</span> + ‘a’；       <span class="comment">//j = 99</span></span><br><span class="line">      System.out.println(j + “ is the Unicode of ”+ (<span class="keyword">char</span>)j);<span class="comment">//99 is the Unicode of  c</span></span><br></pre></td></tr></table></figure><br><strong>字符的比较测试</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">两个字符可以通过关系运算符进行比较，如同比较二个数值：通过字符的Unicode值进行比较</span><br><span class="line">Java为每个基本类型实现了对应的包装类，<span class="keyword">char</span>类型的包装类是Character类。注意包装类对象为引用类型，不是值类型</span><br><span class="line">Character类的作用</span><br><span class="line">将<span class="keyword">char</span>类型的数据封装成对象</span><br><span class="line">包含处理字符的方法和常量</span><br><span class="line">方法</span><br><span class="line">isDigit方法判断一个字符是否是数字</span><br><span class="line">isLetter方法判断一个字符是否是字母</span><br><span class="line">isLetterOrDigit方法判断一个字符是否是字母或数字</span><br><span class="line">isLowerCase方法判断一个字符是否是小写</span><br><span class="line">isUpperCase方法判断一个字符是否是大写</span><br><span class="line">toLowerCase方法将一个字符转换成小写</span><br><span class="line">toUpperCase方法将一个字符转换成大写</span><br></pre></td></tr></table></figure></p><h2 id="3-3-String-类，一个final类"><a href="#3-3-String-类，一个final类" class="headerlink" title="3.3 String 类，一个final类"></a>3.3 String 类，一个final类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">java.lang.String表示一个固定长度的字符序列，实例化后字符不能改。</span><br><span class="line">构造函数</span><br><span class="line">长度(length)</span><br><span class="line">获取字符(charAt)</span><br><span class="line">连接(concat)</span><br><span class="line">截取(substring)</span><br><span class="line">比较(equals, equalsIgnoreCase, compareTo, startWith),</span><br><span class="line">endWith, regionMatch)</span><br><span class="line">转换(toLowerCase, toUpperCase, trim, replace)</span><br><span class="line">查找(indexOf, lastIndexOf)</span><br><span class="line">字符串和数组间转换(getChars, toCharArray), getChars返回<span class="keyword">void</span>,超出长度就异常</span><br><span class="line">字符串和数字间转换(valueOf)</span><br></pre></td></tr></table></figure><p><strong>String类对象的构造</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">从字面值创建字符串</span><br><span class="line">String newString = <span class="keyword">new</span> String(stringLiteral);</span><br><span class="line">例如：</span><br><span class="line">String message = <span class="keyword">new</span> String(<span class="string">"Welcome to Java"</span>);</span><br><span class="line">由于字符串经常使用，java提供了创建字符串的简写形式。</span><br><span class="line">String newString = stringLiteral;</span><br><span class="line">例如：</span><br><span class="line">String m1 = “Welcome”;  <span class="comment">//m1和m2中的字符都是不可修改的</span></span><br><span class="line">String m2 = “Welcome”;  <span class="comment">//故m1和m2可以优化引用同一常量：m1==m2</span></span><br><span class="line">String m3 = <span class="string">"Wel"</span> +<span class="string">"come"</span>;<span class="comment">//m1==m2==m3  </span></span><br><span class="line">String m4 = <span class="string">"Wel"</span> +<span class="keyword">new</span> String(<span class="string">"come"</span>); <span class="comment">//m1!=m4</span></span><br><span class="line"></span><br><span class="line">字符串对象创建之后，其内容是不可修改的。</span><br><span class="line">String s = “java”;</span><br><span class="line">s = “HTML”;</span><br><span class="line">String t =s;</span><br></pre></td></tr></table></figure></p><p><strong>字符串的比较</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">equals方法用于比较两个字符串是否包含相同的内容（字符序列）:</span><br><span class="line">两个字符串内容相同，返回<span class="keyword">true</span></span><br><span class="line">两个字符串内容不同，返回<span class="keyword">false</span></span><br><span class="line">比较字符串内容不能直接比较二个引用变量，比较二个引用变量只是判断这二个引用变量是否指向同一个对象</span><br><span class="line">equalsIngnoeCase忽略大小写比较内容是否相同</span><br><span class="line">regionMatch比较部分内容是否相同</span><br><span class="line">startsWith判断是否以某个字符串开始</span><br><span class="line">endsWith判断是否以某个字符串结束</span><br><span class="line">compareTo方法用于比较两个字符串的大小，即第一个不同字符的差值。s1.compareTo(s2)的返回值:</span><br><span class="line">当两个字符串相同时，返回０</span><br><span class="line">当s1按字典排序在s2之前，返回小于０的值</span><br><span class="line">当s1按字典排序在s2之后，返回大于０的值</span><br><span class="line">String s0 = <span class="string">"Java"</span>;</span><br><span class="line">String s1 = <span class="string">"Welcome to "</span> + s0;</span><br><span class="line">String s2 = <span class="string">"Welcome to Java"</span>;</span><br><span class="line">String s3 = <span class="string">"welcome to java"</span>;</span><br><span class="line">String s6 = <span class="string">"Welcome to Java"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// equals用于比较两个字符串的内容是否相同</span></span><br><span class="line">System.out.println(<span class="string">"s1.equals(s2) is "</span> + s1.equals(s2)); <span class="comment">//true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// equalsIgnoreCase忽略大小写</span></span><br><span class="line">System.out.println(<span class="string">"s2.equals(s3) is "</span> + s2.equals(s3)); <span class="comment">//false</span></span><br><span class="line">System.out.println(<span class="string">"s2.equalsIgnoreCase(s3) is "</span> + s2.equalsIgnoreCase(s3)); <span class="comment">//true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// regionMatches比较部分字符串: 给定两个串的起始位置和长度</span></span><br><span class="line">System.out.println(<span class="string">"s2.regionMatches(11, s0, 0, 4) is "</span> + s2.regionMatches(<span class="number">11</span>, s0, <span class="number">0</span>, <span class="number">4</span>)); <span class="comment">//true</span></span><br><span class="line">System.out.println(<span class="string">"s3.regionMatches(11, s0, 0, 4) is "</span> + s3.regionMatches(<span class="number">11</span>, s0, <span class="number">0</span>, <span class="number">4</span>));<span class="comment">//false</span></span><br><span class="line">System.out.println(<span class="string">"s3.regionMatches(true, 11, s0, 0, 4) is "</span> + s3.regionMatches(<span class="keyword">true</span>, <span class="number">11</span>, s0, <span class="number">0</span>, <span class="number">4</span>));<span class="comment">//true,忽略大小写</span></span><br><span class="line">String s0 = <span class="string">"Java"</span>;</span><br><span class="line">String s1 = <span class="string">"Welcome to "</span> + s0;</span><br><span class="line">String s2 = <span class="string">"Welcome to Java"</span>;</span><br><span class="line">String s3 = <span class="string">"welcome to java"</span>;</span><br><span class="line">String s6 = <span class="string">"Welcome to Java"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// startsWith判断是否以某个字符串开始</span></span><br><span class="line"><span class="comment">// endsWith判断是否以某个字符串结束</span></span><br><span class="line">System.out.println(<span class="string">"s2.startsWith(s0) is "</span> + s2.startsWith(s0));<span class="comment">//false</span></span><br><span class="line">System.out.println(<span class="string">"s2.endsWith(s0) is "</span> + s2.endsWith(s0));  <span class="comment">//true</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">// compareTo根据字典排序比较两个字符串</span></span><br><span class="line">String s4 = <span class="string">"abc"</span>;</span><br><span class="line">String s5 = <span class="string">"abe"</span>;</span><br><span class="line">System.out.println(<span class="string">"s4.compareTo(s5) is "</span> + s4.compareTo(s5));<span class="comment">//-2</span></span><br></pre></td></tr></table></figure><br><strong>字符串方法</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">调用length( )方法可以获取字符串的长度。</span><br><span class="line">例如：</span><br><span class="line">message.length( )返回<span class="number">15</span></span><br><span class="line">charAt(index)方法可以获取指定位置的字符。index必须在<span class="number">0</span>到s.length()-<span class="number">1</span>之间。</span><br><span class="line">例如：</span><br><span class="line">message.charAt(<span class="number">0</span>)返回字符’W’</span><br><span class="line">concat方法用于连接两个字符串。例如：</span><br><span class="line">String s3 = s1.concat(s2);</span><br><span class="line">使用加号(+)连接两个字符串。例如：</span><br><span class="line">String s3 = s1 + s2;</span><br><span class="line">s1 + s2 + s3 等价于s1.concat(s2).concat(s3)</span><br><span class="line">连接操作返回一个新的字符串：因为String类型的实例不可修改。</span><br><span class="line">substring用于截取字符串的一部分，返回新字符串。</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">substring</span><span class="params">(<span class="keyword">int</span> beginIndex, <span class="keyword">int</span> endIndex)</span></span></span><br><span class="line"><span class="function">返回字符串的子串。子串从beginIndex开始，直到endIndex-1</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">substring</span><span class="params">(<span class="keyword">int</span> beginIndex)</span></span></span><br><span class="line"><span class="function">返回字符串的子串。子串从beginIndex开始，直到字符串的结尾。</span></span><br><span class="line"><span class="function">toLowerCase将字符串转换成小写形式，得到新串</span></span><br><span class="line"><span class="function">toUpperCase将字符串转换成大写形式，得到新串</span></span><br><span class="line"><span class="function">trim删除两端的空格，得到新串</span></span><br><span class="line"><span class="function">replace字符替换，得到新串</span></span><br><span class="line"><span class="function">String s0 </span>= <span class="string">"Java"</span>;</span><br><span class="line">String s1 = <span class="string">" Welcome to Java "</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// toLowerCase将字符串转换成小写形式</span></span><br><span class="line">System.out.println(<span class="string">"s1.toLowerCase() is "</span> + s1.toLowerCase());</span><br><span class="line">        </span><br><span class="line"><span class="comment">// toUpperCase将字符串转换成大写形式</span></span><br><span class="line">System.out.println(<span class="string">"s1.toUpperCase() is "</span> + s1.toUpperCase());</span><br><span class="line">        </span><br><span class="line"><span class="comment">// trim删除两端的空格</span></span><br><span class="line">System.out.println(<span class="string">"s1.trim() is "</span> + s1.trim( ));</span><br><span class="line">        </span><br><span class="line"><span class="comment">// replace字符替换</span></span><br><span class="line">System.out.println(“s1.replace(s0, \”HTML\“) is ” + s1.replace(s0, “HTML”)); <span class="comment">//Welcome to HTML</span></span><br><span class="line"><span class="comment">// indexOf返回字符串中字符或字符串匹配的位置，返回-1表示未找到。</span></span><br><span class="line"><span class="string">"Welcome to Java"</span>.indexOf(<span class="string">'W'</span>) returns <span class="number">0</span></span><br><span class="line"><span class="string">"Welcome to Java"</span>.indexOf(<span class="string">'x'</span>) returns -<span class="number">1</span></span><br><span class="line"><span class="string">"Welcome to Java"</span>.indexOf(<span class="string">'o‘,5) returns 9</span></span><br><span class="line"><span class="string">"Welcome to Java".indexOf("come") returns 3</span></span><br><span class="line"><span class="string">"Welcome to Java".indexOf("Java", 5) returns 11</span></span><br><span class="line"><span class="string">"Welcome to Java".indexOf("java", 5) returns -1</span></span><br><span class="line"><span class="string">"Welcome to Java".lastIndexOf('</span>a<span class="string">') returns 14</span></span><br></pre></td></tr></table></figure><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">toCharArray将字符串转换成字符数组</span><br><span class="line">String s = “Java”;</span><br><span class="line"><span class="keyword">char</span>[ ] charArray = s.toCharArray( );<span class="comment">// charArray.length=4</span></span><br><span class="line">将字符数组转换成字符串</span><br><span class="line">使用String的构造函数，可同时初始化</span><br><span class="line"><span class="keyword">new</span> String(<span class="keyword">new</span> <span class="keyword">char</span>[ ] &#123;‘J’,‘a’,‘v’,‘a’&#125; );</span><br><span class="line">使用valueOf方法</span><br><span class="line">String.valueOf(<span class="keyword">new</span> <span class="keyword">char</span>[ ] &#123;‘J’,‘a’,‘v’,‘a’&#125;);</span><br><span class="line">String.valueOf(<span class="number">2.34</span>);</span><br><span class="line">valueOf方法将基本数据类型转换为字符串。例如</span><br><span class="line">String s1 = String.valueOf(<span class="number">1.0</span>);  <span class="comment">//“１.0”</span></span><br><span class="line">String s2 = String.valueOf(<span class="keyword">true</span>); <span class="comment">//“true”</span></span><br><span class="line">字符串转换为基本类型</span><br><span class="line">Double.parseDouble(str)</span><br><span class="line">Integer.parseInt(str)</span><br><span class="line">Boolean.parseBoolean(str)</span><br><span class="line">回文是指顺读和倒读都一样的词语。例如“mom”,  “dad”, ”noon”都是回文。编写程序，判断一个字符串是否是回文。</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPalindrome</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="comment">// The index of the first character in the string</span></span><br><span class="line"><span class="keyword">int</span> low = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// The index of the last character in the string</span></span><br><span class="line"><span class="keyword">int</span> high = s.length( ) - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (low &lt; high) &#123;</span><br><span class="line"><span class="keyword">if</span> (s.charAt(low) != s.charAt(high)) <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// Not a palindrome</span></span><br><span class="line">low++;</span><br><span class="line">high--;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>; <span class="comment">// The string is a palindrome</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPalindrome</span> </span>&#123;</span><br><span class="line"><span class="comment">/** Main method */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">// Prompt the user to enter a string</span></span><br><span class="line">String s = JOptionPane.showInputDialog(<span class="string">"Enter a string:"</span>);</span><br><span class="line">String output = <span class="string">""</span>;</span><br><span class="line"><span class="keyword">if</span> (isPalindrome(s))</span><br><span class="line">output = s + <span class="string">" is a palindrome"</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">output = s + <span class="string">" is not a palindrome"</span>;</span><br><span class="line"><span class="comment">// Display the result</span></span><br><span class="line">JOptionPane.showMessageDialog(<span class="keyword">null</span>, output);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><strong>StringBuilder与StringBuffer</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">String类一旦初始化完成，字符串就是不可修改的。</span><br><span class="line">StringBuilder与StringBuffer(<span class="keyword">final</span>类）初始化后还可以修改字符串。</span><br><span class="line">StringBuffer修改缓冲区的方法是同步的，更适合多任务环境。</span><br><span class="line">StringBuilder在单任务模式下与StringBuffer工作机制类似。</span><br><span class="line">由于可修改字符串， StringBuilder 与StringBuffer 增加了String类没有的一些函数，例如：append、insert、delete、replace、reverse、setCharAt等。</span><br><span class="line">仅以StringBuilder为例：</span><br><span class="line">StringBuilder  stringMy=<span class="keyword">new</span> StringBuilder( );</span><br><span class="line">StringMy.append(“Welcome to”);</span><br><span class="line">      StringMy.append(“ Java”);</span><br><span class="line">StringBuffer用于处理可变内容的字符串。</span><br><span class="line">append方法在字符串的结尾追加数据</span><br><span class="line">insert方法在指定位置上插入数据</span><br><span class="line">reverse方法翻转字符串</span><br><span class="line">replace方法替换字符</span><br><span class="line">toString方法返回String对象</span><br><span class="line">capacity方法返回缓冲区的容量</span><br><span class="line">length方法返回缓冲区中字符的个数</span><br><span class="line">setLength方法设置缓冲区的长度</span><br><span class="line">charAt方法返回指定位置的字符</span><br><span class="line">setCharAt方法设置指定位置的字符</span><br><span class="line">所有对StringBuffer对象内容进行修改的方法，都返回指向相同StringBuffer对象的引用</span><br><span class="line">StringBuffer bf = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">StringBuffer bf1 = bf.append(<span class="string">"Welcome "</span>); </span><br><span class="line">StringBuffer bf2 = bf.append(<span class="string">"to "</span>);</span><br><span class="line">StringBuffer bf3 = bf.append(<span class="string">"Java"</span>);</span><br><span class="line"><span class="keyword">assert</span>(bf==bf1 &amp;&amp; bf==bf2 &amp;&amp; bf == bf3);</span><br><span class="line">因此以上语句可以直接写成：</span><br><span class="line">bf.append(<span class="string">"Welcome "</span>).append(<span class="string">"to "</span>).append(<span class="string">"Java"</span>);</span><br><span class="line"><span class="comment">// 追加</span></span><br><span class="line">StringBuffer bf = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">bf.append(“Welcome”);</span><br><span class="line">bf.append(‘ ‘);</span><br><span class="line">bf.append(“to ”);</span><br><span class="line">bf.append(“Java”);</span><br><span class="line">System.out.println(bf.toString()); <span class="comment">//Welcome to Java</span></span><br><span class="line"><span class="comment">//插入</span></span><br><span class="line">bf.insert(<span class="number">11</span>,”HTML and ”) <span class="comment">//Welcome to HTML and JAVA</span></span><br><span class="line"><span class="comment">//删除</span></span><br><span class="line">bf.delete(<span class="number">8</span>,<span class="number">11</span>); <span class="comment">//Welcome Java</span></span><br><span class="line">bf.deleteCharAt(<span class="number">8</span>);<span class="comment">//Welcome o Java</span></span><br><span class="line">bf.reverse(); <span class="comment">//avaJ ot emocleW</span></span><br><span class="line">bf.replace（<span class="number">11</span>，<span class="number">15</span>，“HTML”）;<span class="comment">//Welcome to HTML</span></span><br><span class="line">bf.setCharAt(<span class="number">0</span>,’w’);<span class="comment">//welcome to java</span></span><br><span class="line"></span><br><span class="line">toString(): 从缓冲区返回字符串</span><br><span class="line">capacity()：返回缓冲区容量。length &lt;= capacity</span><br><span class="line">    当字符串长度超过缓冲区容量，capacity会自动增加</span><br><span class="line">length()：返回缓冲区中字符数量</span><br><span class="line">setLength(newLength)：设置缓冲区长度</span><br><span class="line">charAt(index)：返回下标为index的字符</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编写程序，检查回文，并忽略不是字母和数字的字符。</span></span><br><span class="line">解决方案</span><br><span class="line">创建一个新的StringBuffer，将字符串的字母和数字添加到StringBuffer中，返回过滤后的String对象。</span><br><span class="line">翻转过滤后的字符串，并与过滤后的字符串进行比较，如果内容相同则是回文。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a new string that is the reversal of s</span></span><br><span class="line">String s2 = reverse(s);</span><br><span class="line"><span class="comment">// Compare if the reversal is the same as the original stringreturn s2.equals(s);</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">reverse</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">StringBuffer strBuf = <span class="keyword">new</span> StringBuffer(s);</span><br><span class="line">strBuf.reverse();</span><br><span class="line"><span class="keyword">return</span> strBuf.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="3-4-格式化控制台输入输出"><a href="#3-4-格式化控制台输入输出" class="headerlink" title="3.4 格式化控制台输入输出"></a>3.4 格式化控制台输入输出</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">JDK1<span class="number">.5</span>提供了格式化控制台输出方法</span><br><span class="line">System.out.printf(format, item1, item2, …);</span><br><span class="line">格式化字符串</span><br><span class="line">String.format(format, item1, item2, …);</span><br><span class="line">格式描述符</span><br><span class="line">%b 布尔值</span><br><span class="line">%c 字符</span><br><span class="line">%d 十进制整数</span><br><span class="line">%f 浮点数</span><br><span class="line">%e 科学计数法</span><br><span class="line">%s 字符串</span><br><span class="line">String.format(“格式$：%<span class="number">1</span>$d,%<span class="number">2</span>$s”, <span class="number">99</span>,“abc”); <span class="comment">//结果”格式$：99，abc“</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestPrintf</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.printf(<span class="string">"boolean : %6b\n"</span>, <span class="keyword">false</span>);</span><br><span class="line">        System.out.printf(<span class="string">"boolean : %6b\n"</span>, <span class="keyword">true</span>);</span><br><span class="line">        System.out.printf(<span class="string">"character : %4c\n"</span>, <span class="string">'a'</span>);</span><br><span class="line">        System.out.printf(<span class="string">"integer : %6d, %6d\n"</span>, <span class="number">100</span>, <span class="number">200</span>);</span><br><span class="line">        System.out.printf(<span class="string">"double : %7.2f\n"</span>, <span class="number">12.345</span>);</span><br><span class="line">        System.out.printf(<span class="string">"String : %7s\n"</span>, <span class="string">"hello"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础（2）</title>
      <link href="/2019/11/21/java/shyjava(2)/"/>
      <url>/2019/11/21/java/shyjava(2)/</url>
      
        <content type="html"><![CDATA[<h1 id="Shy-Learnjava（2）基础"><a href="#Shy-Learnjava（2）基础" class="headerlink" title="Shy-Learnjava（2）基础"></a>Shy-Learnjava（2）基础</h1><h2 id="2-选择"><a href="#2-选择" class="headerlink" title="2 选择"></a>2 选择</h2><h3 id="2-1-布尔类型和逻辑运算符"><a href="#2-1-布尔类型和逻辑运算符" class="headerlink" title="2.1 布尔类型和逻辑运算符"></a>2.1 布尔类型和逻辑运算符</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">boolean</span>类型的值有真(<span class="keyword">true</span>)或假(<span class="keyword">false</span>)。</span><br><span class="line">关系运算符: &lt;, &lt;=, &gt;, &gt;=, ==, !=</span><br><span class="line">关系运算符的计算结果是<span class="keyword">boolean</span>类型</span><br><span class="line"><span class="keyword">boolean</span>类型不能与其它数据类型混合运算</span><br><span class="line">布尔运算符: !, &amp;&amp;, ||, ^, &amp;, | </span><br><span class="line">&amp;&amp; , ||为条件逻辑运算符: (x&gt;<span class="number">0</span>) &amp;&amp; (x&lt;<span class="number">9</span>)</span><br><span class="line">&amp;，|为无条件逻辑运算符，同时也是位操作符</span><br><span class="line">^ 异或</span><br></pre></td></tr></table></figure><div align = center><img src = "https://img.vim-cn.com/80/4f17375d3236c4af37f9a0803b9272b11b99c5.png"></div><h3 id="2-2-if语句"><a href="#2-2-if语句" class="headerlink" title="2.2 if语句"></a>2.2 if语句</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (radius &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">  area = radius * radius * PI;</span><br><span class="line">  System.out.println(<span class="string">"The area for the circle of radius "</span> </span><br><span class="line">    + radius + <span class="string">" is "</span> + area);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (radius &gt;= <span class="number">0</span>) &#123;   </span><br><span class="line">  area = radius * radius * <span class="number">3.14159</span>;</span><br><span class="line">  System.out.println(<span class="string">"The area for the circle of radius "</span> </span><br><span class="line">    + radius + <span class="string">" is "</span> + area);</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">else</span> &#123;  System.out.println(<span class="string">"Negative input"</span>);   &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (score &gt; <span class="number">90.0</span>)</span><br><span class="line">    grade = ‘A’;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (score &gt;= <span class="number">80.0</span>)</span><br><span class="line">    grade = ‘B’;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (scroe &gt;= <span class="number">70.0</span>)</span><br><span class="line">    grade = ‘C’;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (score &gt;= <span class="number">60.0</span>)</span><br><span class="line">    grade = ‘D’;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    grade = ‘F’</span><br><span class="line"></span><br><span class="line"><span class="comment">// 高手的if</span></span><br><span class="line"><span class="keyword">if</span> (number % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">    even = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    even = <span class="keyword">false</span>;<span class="comment">//新手</span></span><br><span class="line">等价于</span><br><span class="line">even = (number % <span class="number">2</span> == <span class="number">0</span>);<span class="comment">//高手</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (even == <span class="keyword">true</span>)</span><br><span class="line">    System.out.println(“It is even.”);</span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">if</span> (even)</span><br><span class="line">    System.out.println(“It is even.”);</span><br></pre></td></tr></table></figure><h3 id="2-3-条件语句"><a href="#2-3-条件语句" class="headerlink" title="2.3 条件语句"></a>2.3 条件语句</h3><ul><li><p>swith语句</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(expression) &#123;</span><br><span class="line">    <span class="keyword">case</span> value1 : </span><br><span class="line">        statement(s)</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> value2 : </span><br><span class="line">        statement(s)</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    …</span><br><span class="line">    <span class="keyword">default</span> : </span><br><span class="line">        statement(s)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>条件表达式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">max = (num1 &gt; num2) ? num1 : num2;</span><br></pre></td></tr></table></figure><h3 id="2-4-操作符的优先级和表达式规则"><a href="#2-4-操作符的优先级和表达式规则" class="headerlink" title="2.4 操作符的优先级和表达式规则"></a>2.4 操作符的优先级和表达式规则</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">括号优先级最高，如果括号有嵌套，内部括号优先执行。</span><br><span class="line">如果没有括号，则根据操作符的优先级和结合规则确定执行顺序。</span><br><span class="line">如果相邻的操作符有相同的优先级，则根据结合规则确定执行顺序。</span><br><span class="line">除赋值运算符之外的二元运算符都是左结合的。</span><br><span class="line">赋值运算符和?:运算符是右结合的。</span><br><span class="line">例如：</span><br><span class="line">a+b-c+d  等价于 ((a+b)-c)+d</span><br><span class="line">a=b+=c=<span class="number">5</span> 等价于 a=(b+=(c=<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">操作符的优先级和结合规则只规定了操作符的执行顺序。操作数从左至右进行运算。</span><br><span class="line">二元操作符左边的操作数比右边的操作数优先运算。例如：</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> x = a + (++a);</span><br><span class="line">x的结果为<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> x = (++a) + a;</span><br><span class="line">x的结果为<span class="number">2</span></span><br></pre></td></tr></table></figure></li><li>表达式规则<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">规则</span><br><span class="line">可能的情况下，从左向右计算所有子表达式</span><br><span class="line">根据运算符的优先级进行运算</span><br><span class="line">优先级相同的运算符，根据结合方向进行运算</span><br><span class="line"><span class="number">3</span> + <span class="number">4</span> * <span class="number">4</span> &gt; <span class="number">5</span> * (<span class="number">4</span> + <span class="number">3</span>) - <span class="number">1</span> 的执行顺序为：</span><br><span class="line"> <span class="number">3</span> + <span class="number">4</span> * <span class="number">4</span> &gt; <span class="number">5</span> * (<span class="number">4</span> + <span class="number">3</span>) - <span class="number">1</span> </span><br><span class="line"> <span class="number">3</span> + <span class="number">4</span> * <span class="number">4</span> &gt; <span class="number">5</span> * <span class="number">7</span> – <span class="number">1</span></span><br><span class="line"> <span class="number">3</span> + <span class="number">16</span> &gt; <span class="number">5</span> * <span class="number">7</span> – <span class="number">1</span></span><br><span class="line"> <span class="number">3</span> + <span class="number">16</span> &gt; <span class="number">35</span> – <span class="number">1</span></span><br><span class="line"> <span class="number">19</span> &gt; <span class="number">35</span> – <span class="number">1</span></span><br><span class="line"> <span class="number">19</span> &gt; <span class="number">34</span></span><br><span class="line"> <span class="keyword">false</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础（1）</title>
      <link href="/2019/11/20/java/shyjava(1)/"/>
      <url>/2019/11/20/java/shyjava(1)/</url>
      
        <content type="html"><![CDATA[<p><strong>从今天开始复习java</strong></p><h1 id="Shy-Learnjava（1）基础"><a href="#Shy-Learnjava（1）基础" class="headerlink" title="Shy-Learnjava（1）基础"></a>Shy-Learnjava（1）基础</h1><h2 id="0-编程风格"><a href="#0-编程风格" class="headerlink" title="0 编程风格"></a>0 编程风格</h2><ul><li><strong>注释</strong><ol><li>类和方法前使用文档注释</li><li>方法步骤前使用行注释。</li></ol></li><li><strong>命名</strong><ol><li>变量和方法名使用小写，如果有多个单词，第一个单词首字母小写，其它单词首字母大写。</li><li>类名的每个单词的首字母大写。</li><li>常量使用大写，单词间以下划线分隔。</li><li>缩进、空格、块样式（在eclipse中使用ctrl+shift+f）</li></ol></li></ul><h2 id="1-基本程序设计"><a href="#1-基本程序设计" class="headerlink" title="1 基本程序设计"></a>1 基本程序设计</h2><ul><li>编写一个程序<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ComputeArea</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">double</span> raidus;</span><br><span class="line">        <span class="keyword">double</span> area;</span><br><span class="line">        area = radius * raidus * <span class="number">3.14</span></span><br><span class="line">        System.out.println(<span class="string">"The area of the circle of raius"</span> + radius + <span class="string">"is"</span> + area);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-1-标准输入与输出"><a href="#1-1-标准输入与输出" class="headerlink" title="1.1 标准输入与输出"></a>1.1 标准输入与输出</h3></li><li>标准输入<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">System.out <span class="comment">//标准输出流类OutputStrem的对象</span></span><br><span class="line">System.in <span class="comment">//标准输入流类InputStrem的对象</span></span><br></pre></td></tr></table></figure></li><li>Scanner类<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Scanner</span><br><span class="line">Scanner input = <span class="keyword">new</span> Scanner(System.in)</span><br><span class="line"><span class="keyword">double</span> d = input.nextDouble();</span><br><span class="line"><span class="comment">// 方法有</span></span><br><span class="line">nextByte()</span><br><span class="line">nextShort()</span><br><span class="line">nextInt()</span><br><span class="line">nextLong()</span><br><span class="line">nextFloat()</span><br><span class="line">nextDouble()</span><br><span class="line">next()  <span class="comment">// 读入一个字符串</span></span><br></pre></td></tr></table></figure><h3 id="1-2-标识符、常量与变量"><a href="#1-2-标识符、常量与变量" class="headerlink" title="1.2 标识符、常量与变量"></a>1.2 标识符、常量与变量</h3></li><li>标识符命名规则<ul><li>标识符是由字母、数字、下划线(_)、美元符号($)组成的字符序列。</li><li>标识符必须以字母、下划线(_)、美元符号($)开头。不能以数字开头。标识符不能是保留字。</li><li>标识符不能为true、false或null等事实上的保留字（参见英文维基网）</li><li>标识符可以为任意长度，但编译通常只接受前128字符</li><li>例如：$2, area, radius, showMessageDialog是合法的标识符；2A, d+4是非法的标识符</li></ul></li><li>java保留字<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span>       <span class="keyword">default</span> <span class="keyword">if</span>        <span class="keyword">package</span>       <span class="keyword">this</span>   </span><br><span class="line"><span class="keyword">assert</span>               <span class="keyword">do</span>              goto            <span class="keyword">private</span>         <span class="keyword">throw</span>   </span><br><span class="line"><span class="keyword">boolean</span>        <span class="keyword">double</span>          implements<span class="keyword">protected</span> <span class="keyword">throws</span>        </span><br><span class="line"><span class="function"><span class="keyword">break</span>         <span class="keyword">else</span>        <span class="keyword">import</span>        <span class="keyword">public</span>        <span class="title">transient</span><span class="params">(非序列化)</span></span></span><br><span class="line"><span class="function"><span class="keyword">byte</span>        <span class="keyword">enum</span><span class="keyword">instanceof</span>return        <span class="keyword">true</span></span></span><br><span class="line"><span class="function"><span class="keyword">case</span>        extends<span class="keyword">int</span>        <span class="keyword">short</span>        <span class="keyword">try</span></span></span><br><span class="line"><span class="function"><span class="keyword">catch</span>        <span class="keyword">false</span>        interface       <span class="keyword">static</span>        <span class="keyword">void</span></span></span><br><span class="line"><span class="function"><span class="keyword">char</span>        <span class="keyword">final</span>           <span class="keyword">long</span>            <span class="title">strictfp</span><span class="params">(严格浮点)</span>   <span class="keyword">volatile</span>          </span></span><br><span class="line"><span class="function">class                 <span class="keyword">finally</span><span class="title">native</span><span class="params">(本地方法)</span>     <span class="keyword">super</span>        <span class="keyword">while</span></span></span><br><span class="line"><span class="function"><span class="keyword">const</span>        <span class="keyword">float</span>        new     <span class="keyword">switch</span>        </span></span><br><span class="line"><span class="function"><span class="keyword">continue</span>        <span class="keyword">for</span>                     <span class="keyword">null</span>         <span class="keyword">synchronized</span></span></span><br></pre></td></tr></table></figure></li><li>java常量<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> datatype CONSTANT_NAME = value;</span><br><span class="line"><span class="comment">//注意常量的声明和初始化必须同时完成</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">double</span> PI = <span class="number">3.14159</span>;</span><br><span class="line"><span class="comment">// 避免重复输入</span></span><br><span class="line"><span class="comment">// 便于程序修改</span></span><br><span class="line"><span class="comment">// 便于程序阅读</span></span><br></pre></td></tr></table></figure><h3 id="1-3-赋值语句与基本表达式"><a href="#1-3-赋值语句与基本表达式" class="headerlink" title="1.3 赋值语句与基本表达式"></a>1.3 赋值语句与基本表达式</h3></li><li><p>赋值语句</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 赋值语句右读</span></span><br><span class="line">i = j = k = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//不要认为i, j, k的值不变，volatile类型的变量值可变</span></span><br><span class="line">k = <span class="number">1</span>;</span><br><span class="line">j = k;  </span><br><span class="line">i = j;</span><br><span class="line"></span><br><span class="line"><span class="comment">//语法</span></span><br><span class="line">datatype variable = expression;</span><br><span class="line"><span class="comment">//例如：</span></span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;   <span class="comment">//某些变量在申明时必须同时初始化：final int m=0;</span></span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>, y = <span class="number">2</span>;</span><br><span class="line"><span class="comment">//局部变量在使用前必须赋值。</span></span><br><span class="line"><span class="keyword">int</span> x, y;     <span class="comment">//若是成员变量，x, y有默认值=0</span></span><br><span class="line">y = x + <span class="number">1</span>; <span class="comment">//局部变量无默认值则错error</span></span><br></pre></td></tr></table></figure><h3 id="1-4-java-数据类型"><a href="#1-4-java-数据类型" class="headerlink" title="1.4 java 数据类型"></a>1.4 java 数据类型</h3><div align=center><img src="https://img.vim-cn.com/bc/a351f0120d3312145a6fe46f6a96aa7a61273e.png"></div></li><li><p>数值数据类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">整数</span><br><span class="line"><span class="keyword">byte</span><span class="number">8</span>位带符号整数(-<span class="number">128</span> 到 <span class="number">127</span>)</span><br><span class="line"><span class="keyword">short</span><span class="number">16</span>位带符号整数(-<span class="number">32768</span> 到 <span class="number">32767</span>)</span><br><span class="line"><span class="keyword">int</span><span class="number">32</span>位带符号整数(-<span class="number">2147483648</span> 到 <span class="number">2147483647</span>)</span><br><span class="line"><span class="keyword">long</span><span class="number">64</span>位带符号整数(-<span class="number">9223372036854775808</span> 到<span class="number">9223372036854775807</span>)</span><br><span class="line"></span><br><span class="line">浮点数</span><br><span class="line"><span class="keyword">float</span><span class="number">32</span>位浮点数(负数  -<span class="number">3.4</span>×<span class="number">1038</span>到-<span class="number">1.4</span>×<span class="number">10</span>-<span class="number">45</span> </span><br><span class="line">                  正数  <span class="number">1.4</span>×<span class="number">10</span>-<span class="number">45</span>到<span class="number">3.4</span>×<span class="number">1038</span> )</span><br><span class="line"><span class="keyword">double</span><span class="number">64</span>位浮点数(负数  -<span class="number">1.8</span>×<span class="number">10308</span>到-<span class="number">4.9</span>×<span class="number">10</span>-<span class="number">324</span></span><br><span class="line">                 正数  <span class="number">4.9</span>×<span class="number">10</span>-<span class="number">324</span>到<span class="number">1.8</span>×<span class="number">10308</span>)</span><br><span class="line"></span><br><span class="line">加(+)、减(-)、乘(*)、除(/)、求余(%)：注意+，-的优先级较低</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">34</span> + <span class="number">1</span>;<span class="comment">// 35</span></span><br><span class="line"><span class="keyword">double</span> b = <span class="number">34.0</span> – <span class="number">0.1</span>;<span class="comment">// 33.9</span></span><br><span class="line"><span class="keyword">long</span> c = <span class="number">300</span> * <span class="number">30</span>;            <span class="comment">// 9000</span></span><br><span class="line"><span class="keyword">double</span> d = <span class="number">1.0</span> / <span class="number">2.0</span>;<span class="comment">// 0.5: 此处为浮点除</span></span><br><span class="line"><span class="keyword">int</span> e = <span class="number">1</span> / <span class="number">2</span>;<span class="comment">// 0: 此处为整除</span></span><br><span class="line"><span class="keyword">byte</span> f = <span class="number">20</span> % <span class="number">3</span>;<span class="comment">// 2: 取余数</span></span><br><span class="line">整数相除的结果还是整数，省略小数部分。</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">5</span> / <span class="number">2</span><span class="comment">// 2</span></span><br><span class="line"><span class="keyword">int</span> j = -<span class="number">5</span> / <span class="number">2</span> <span class="comment">// -2</span></span><br><span class="line"></span><br><span class="line">字面值是直接出现在程序中的常量值。</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">34</span>;</span><br><span class="line"><span class="keyword">long</span> k = <span class="number">100000L</span>; </span><br><span class="line">整数字面值</span><br><span class="line">以<span class="number">0</span>开头表示八进制，如<span class="number">035</span>；以<span class="number">0</span>x或<span class="number">0</span>X开头表示十六进制，如<span class="number">0x1D</span>,<span class="number">0X1d</span>；以<span class="number">1</span>-<span class="number">9</span>开头表示十进制，如<span class="number">29</span></span><br><span class="line">后缀字母：以l或L结尾表示<span class="keyword">long</span>类型，如<span class="number">29L</span>；其它表示<span class="keyword">int</span>类型。</span><br><span class="line">浮点数字面值</span><br><span class="line">浮点数是包含小数点的十进制数，后跟可选的指数部分。如</span><br><span class="line"><span class="number">18</span>.  <span class="number">1.8e1</span> .<span class="number">18E2</span></span><br><span class="line">后缀字母：以d或D结尾或者无后缀表示<span class="keyword">double</span>类型；以f或F结尾表示<span class="keyword">float</span>类型</span><br></pre></td></tr></table></figure></li><li><p>操作运算符</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">常用简洁操作符, 结果均为右值。</span><br><span class="line">操作符举例等价于</span><br><span class="line">+=i += <span class="number">8</span>i = i + <span class="number">8</span></span><br><span class="line">-=f -= <span class="number">8.0</span>f = f - <span class="number">8.0</span></span><br><span class="line">*=i *= <span class="number">8</span>i = i * <span class="number">8</span></span><br><span class="line">/=i /= <span class="number">8</span>i = i / <span class="number">8</span></span><br><span class="line">%=i %= <span class="number">8</span>i = i % <span class="number">8</span></span><br><span class="line">递增和递减运算符：++, --。结果均为右值。</span><br><span class="line">前缀表示先加(减)<span class="number">1</span>后使用</span><br><span class="line">后缀表示先使用后加(减) <span class="number">1</span></span><br><span class="line">    <span class="keyword">int</span> i =<span class="number">10</span>;             <span class="comment">//i=++i + ++i; 结果为23</span></span><br><span class="line">    <span class="keyword">int</span> newNum= <span class="number">10</span> * i++; <span class="comment">//newNum = 100, i = 11</span></span><br><span class="line">    <span class="keyword">int</span> newNum= <span class="number">10</span> * ++i; <span class="comment">//newNum = 110, i = 11</span></span><br></pre></td></tr></table></figure></li><li><p>数值类型转换</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">如果二元操作符的两个操作数的数据类型不同，那么根据下面的规则对操作数进行转换：</span><br><span class="line">如果有一个操作数是<span class="keyword">double</span>类型，另一个操作数转换为<span class="keyword">double</span>类型。</span><br><span class="line">否则，如果有一个操作数是<span class="keyword">float</span>类型，另一个操作数转换为<span class="keyword">float</span>类型。</span><br><span class="line">否则，如果有一个操作数是<span class="keyword">long</span>类型，另一个操作数转换为<span class="keyword">long</span>类型。</span><br><span class="line">否则，两个操作数都转换为<span class="keyword">int</span>类型。</span><br><span class="line">数据转换总是向较大范围的数据类型转换，避免精度损失</span><br><span class="line"><span class="keyword">long</span> k = i * <span class="number">3</span> + <span class="number">4</span>; <span class="comment">//i变成int参与右边表达式计算，计算结果转long</span></span><br><span class="line"><span class="keyword">double</span> d = i * <span class="number">3.1</span> + k / <span class="number">2</span>; <span class="comment">//i转double， k/2转double</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">将值赋值给较大取值范围的变量时，自动进行类型转换。</span><br><span class="line"><span class="keyword">byte</span> → <span class="keyword">char</span>→ <span class="keyword">short</span> → <span class="keyword">int</span> → <span class="keyword">long</span> → <span class="keyword">float</span> → <span class="keyword">double</span> </span><br><span class="line">将值赋值给较小取值范围的变量时，必须使用强制类型转换(type casting)。语法：</span><br><span class="line">(datatype)variableName</span><br><span class="line">例如：</span><br><span class="line"><span class="keyword">float</span> f = (<span class="keyword">float</span>)<span class="number">10.1</span>;<span class="comment">// 10.1是double类型</span></span><br><span class="line"><span class="keyword">int</span> i = (<span class="keyword">int</span>)f;<span class="comment">// 10</span></span><br><span class="line"><span class="keyword">int</span> j = (<span class="keyword">int</span>)-f;<span class="comment">// -10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">``</span><br><span class="line"></span><br><span class="line">- 字符数据类型</span><br><span class="line">```java</span><br><span class="line"><span class="keyword">char</span>表示<span class="number">16</span>位的单个Unicode字符。</span><br><span class="line"><span class="keyword">char</span>类型的字面值</span><br><span class="line">以两个单引号界定的单个Unicode字符。如:<span class="string">'男'</span>,<span class="string">'女'</span></span><br><span class="line">可以用\uxxxx形式表示， xxxx为十六进制。如:<span class="string">'\u7537'</span>, <span class="string">'\u5973'</span></span><br><span class="line">转义字符表示：\n   \t  \b  \r   \f   \\   \<span class="string">'   \"</span></span><br><span class="line"><span class="string">例如：</span></span><br><span class="line"><span class="string">char letter = '</span>A<span class="string">';</span></span><br><span class="line"><span class="string">char numChar = '</span><span class="number">4</span><span class="string">';</span></span><br><span class="line"><span class="string">如果想打印带””的信息 He said “Java is fun “</span></span><br><span class="line"><span class="string">      System.out.println(“He said \”Java is fun \””); </span></span><br><span class="line"><span class="string">String表示一个字符序列，注意字符串是String类实现的，是引用类型</span></span><br><span class="line"><span class="string">字符串的字面值是由双引号界定的零个或多个字符。</span></span><br><span class="line"><span class="string">"Welcom to java!"                 ""</span></span><br><span class="line"><span class="string">连接运算：+, +=</span></span><br><span class="line"><span class="string">加号用于连接两个字符串。如果其中一个不是字符串，则先将该操作数转换成字符串，再执行连接操作。</span></span><br><span class="line"><span class="string">String message = "Welcome " + "to " + "java";  // Welcome to Java</span></span><br><span class="line"><span class="string">String s = “Chapter” + 2;           // Chapter2：不能都是数值</span></span><br><span class="line"><span class="string">String s1 += "Supplement" + '</span>B<span class="string">';           // SupplementB  </span></span><br><span class="line"><span class="string">message += " and Java is fun";  // Welcome to Java and Java is fun</span></span><br><span class="line"><span class="string">int i = 1;</span></span><br><span class="line"><span class="string">int j = 2;</span></span><br><span class="line"><span class="string">System.out.println("i + j = "  + i + j);          // i+j=12</span></span><br><span class="line"><span class="string">System.out.println("i + j = "  + (i + j));          // i+j = 3</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux</title>
      <link href="/2019/11/01/Linux/linux/"/>
      <url>/2019/11/01/Linux/linux/</url>
      
        <content type="html"><![CDATA[<h2 id="1-linux-的基础系统命令"><a href="#1-linux-的基础系统命令" class="headerlink" title="1. linux 的基础系统命令"></a>1. linux 的基础系统命令</h2><hr><p>在linux中<strong>系统命令</strong>通常是如下的格式：<br><code>命令名称  【命令参数】 【命令对象】</code>  </p><ol><li>获取登录信息<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># w</span></span><br><span class="line"><span class="comment"># who </span></span><br><span class="line"><span class="comment"># who am i</span></span><br></pre></td></tr></table></figure></li><li>查看自己使用的shell<code>ps</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ps</span></span><br></pre></td></tr></table></figure></li><li>查看命令的说明<code>whatis</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># what is ps</span></span><br><span class="line"><span class="comment"># what is python</span></span><br></pre></td></tr></table></figure></li><li>查看命令的位置<code>which、whereis</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># where is ps</span></span><br><span class="line"><span class="comment"># where is python</span></span><br><span class="line"><span class="comment"># which ps</span></span><br><span class="line"><span class="comment"># which python</span></span><br></pre></td></tr></table></figure></li><li>查看帮助文档<code>man、help、apropos</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># man ps</span></span><br><span class="line"><span class="comment"># info ps</span></span><br></pre></td></tr></table></figure></li><li>切换用户<code>su</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># su hellokitty</span></span><br></pre></td></tr></table></figure></li><li>以管理员身份执行命令<code>sudo</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls /root(fail)</span></span><br><span class="line"><span class="comment"># sudo ls /root(success)</span></span><br></pre></td></tr></table></figure></li><li>登入和登出相关<code>logout、exit、adduser、userdel、passwd、ssh</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># adduser hellokitty</span></span><br><span class="line"><span class="comment"># passwd hellokitty (change password)</span></span><br><span class="line"><span class="comment"># ssh@hellokitty@1.2.3.4</span></span><br><span class="line"><span class="comment"># logout</span></span><br></pre></td></tr></table></figure></li><li>查看系统和主机名<code>unname、hostname</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unname</span></span><br><span class="line"><span class="comment"># hostname</span></span><br></pre></td></tr></table></figure></li><li>重启和关机<code>rebot、init 6、shutdown、init 0</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rebot</span></span><br></pre></td></tr></table></figure></li><li>查看历史命令<code>history</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># history</span></span><br></pre></td></tr></table></figure></li></ol><hr><h2 id="2-linux常用的实用命令"><a href="#2-linux常用的实用命令" class="headerlink" title="2. linux常用的实用命令"></a>2. linux常用的实用命令</h2><ol><li>创建和删除目录<code>mkdir</code>、<code>rmdir</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir abc</span></span><br><span class="line"><span class="comment"># mkdir -p abc</span></span><br><span class="line"><span class="comment"># rmdir abc</span></span><br></pre></td></tr></table></figure></li><li>创建和删除文件<code>touch</code>、<code>rm</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># touch readme.md</span></span><br><span class="line"><span class="comment"># rm readme.md</span></span><br><span class="line"><span class="comment"># rm -rf xyz  </span></span><br><span class="line">```bash</span><br><span class="line">&gt; `touch`命令用于创建空白文件或者修改文件时间。在Linux系统中有三种文件时间：</span><br><span class="line">- 更改文件内容的时间：`mtime`</span><br><span class="line">- 更改权限的时间: `ctime`</span><br><span class="line">- 最后访问时间: `atime`</span><br><span class="line">&gt; rm有几个重要的参数如下：</span><br><span class="line">- `-i` :交互式删除，每个删除项都要询问</span><br><span class="line">- `-r` :删除目录并且递归删除目录及文件</span><br><span class="line">- `-f` :强制删除，忽略存在的文件，没有任何提示</span><br><span class="line"></span><br><span class="line">3. 切换和查看当前的工作目录`<span class="built_in">cd</span>`、`<span class="built_in">pwd</span>`</span><br><span class="line">4. 查看目录内容`ls`</span><br><span class="line">* `-l` :以长格式查看文件和目录</span><br><span class="line">* `-a` ：显示以点开头的文件和目录</span><br><span class="line">* `-R` ：遇到目录，递归展开</span><br><span class="line">* `-d` : 只列出目录，不列出内容</span><br><span class="line">* `-s  -t` : 按照大小和时间进行排序</span><br><span class="line">5. 查看文件内容`cat`、`head`、`tail`、`more`、`less`</span><br><span class="line">```bash</span><br><span class="line"><span class="comment"># cat readme.md</span></span><br><span class="line"><span class="comment"># head -10 sohu.html</span></span><br></pre></td></tr></table></figure></li><li>拷贝和移动文件<code>cp</code>、<code>mv</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp sohu.html  backup/</span></span><br><span class="line"><span class="comment"># cd backup</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mv sohu.html sohu_index.html</span></span><br></pre></td></tr></table></figure></li><li>查找文件和查找内容<code>find</code>、<code>grep</code></li></ol><ul><li>grep 在搜索字符串时可以直接使用正则表达式，如果需要使用正则表达式，则可以用<code>grep -E</code></li></ul><ol start="8"><li>链接<code>ln</code></li></ol><ul><li>链接可以分为硬链接和软链接，硬链接可以认为是一个指向文件数据的指针。我们平常删除数据时，并没有删除硬盘上的文件，我们删除的是一个指针。而软链接类似于windows里面的快捷方式。   </li></ul><ol start="9"><li>压缩\解压缩\归档\解归档<code>gzip</code>、<code>gunzip</code>、<code>xz</code>、<code>tar</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gunzip redis-4.0.10.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xvf redis-4.0.10.tar</span></span><br></pre></td></tr></table></figure></li><li>其它工具<code>sort</code>、<code>uniq</code>、<code>diff</code>、<code>tr</code>、<code>cut</code>、<code>paste</code>、<code>file</code>、<code>wc</code></li><li>管道和重定向  </li></ol><ul><li>管道的使用：<code>|</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find ./ | wc -1   % 查找当前目录下的文件个数 </span></span><br><span class="line"><span class="comment"># ls | cat - n    % 列出当前路径下的文件加，给每一项加一个编号</span></span><br><span class="line"><span class="comment"># cat record.log | grep AAA | grep - v BBB | wc - 1 % 查找record.log 中的AAA，但不包含BBB的个数</span></span><br></pre></td></tr></table></figure></li><li>输出重定向和错误重定向：<code>- &gt; / &gt;&gt;  / 2 &gt;</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># cat readme.txt</span></span><br><span class="line">banana</span><br><span class="line">apple</span><br><span class="line">grape</span><br><span class="line">apple</span><br><span class="line">grape</span><br><span class="line">watermelon</span><br><span class="line">pear</span><br><span class="line">pitaya</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># cat readme.txt | sort | uniq &gt; result.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># cat result.txt</span></span><br><span class="line">apple</span><br><span class="line">banana</span><br><span class="line">grape</span><br><span class="line">pear</span><br><span class="line">pitaya</span><br><span class="line">watermelon</span><br></pre></td></tr></table></figure></li><li>输入重定向：<code>- &lt;</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># echo 'hello, world!' &gt; hello.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># wall &lt; hello.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment">#</span></span><br><span class="line">Broadcast message from root@iZwz97tbgo9lkabnat2lo8Z (Wed Jun 20 19:43:05 2018):</span><br><span class="line">hello, world!</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># echo 'I will show you some code.' &gt;&gt; hello.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># wall &lt; hello.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment">#</span></span><br><span class="line">Broadcast message from root@iZwz97tbgo9lkabnat2lo8Z (Wed Jun 20 19:43:55 2018):</span><br><span class="line">hello, world!</span><br><span class="line">I will show you some code.</span><br></pre></td></tr></table></figure></li></ul><ol start="12"><li>别名</li></ol><ul><li><em>alias</em><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># alias ll='ls -l'</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># alias frm='rm -rf'</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ll</span></span><br><span class="line">...</span><br><span class="line">drwxr-xr-x  2 root       root   4096 Jun 20 12:52 abc</span><br><span class="line">...bash</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># frm abc</span></span><br></pre></td></tr></table></figure></li><li><em>unlias</em><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># unalias frm</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># frm sohu.html</span></span><br><span class="line">-bash: frm: <span class="built_in">command</span> not found</span><br></pre></td></tr></table></figure></li></ul><ol start="13"><li>其它程序</li></ol><ul><li>时间和日期 <code>date / cal</code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># date</span></span><br><span class="line">Wed Jun 20 12:53:19 CST 2018</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># cal</span></span><br><span class="line">      June 2018</span><br><span class="line">Su Mo Tu We Th Fr Sa</span><br><span class="line">                1  2</span><br><span class="line"> 3  4  5  6  7  8  9</span><br><span class="line">10 11 12 13 14 15 16</span><br><span class="line">17 18 19 20 21 22 23</span><br><span class="line">24 25 26 27 28 29 30</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># cal 5 2017</span></span><br><span class="line">      May 2017</span><br><span class="line">Su Mo Tu We Th Fr Sa</span><br><span class="line">    1  2  3  4  5  6</span><br><span class="line"> 7  8  9 10 11 12 13</span><br><span class="line">14 15 16 17 18 19 20</span><br><span class="line">21 22 23 24 25 26 27</span><br><span class="line">28 29 30 31</span><br></pre></td></tr></table></figure></li><li>录制操作脚本<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">script</span><br></pre></td></tr></table></figure></li><li>给用户发送消息<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mesg / write / wall / mail</span><br></pre></td></tr></table></figure><h2 id="3-文件系统"><a href="#3-文件系统" class="headerlink" title="3. 文件系统"></a>3. 文件系统</h2><h3 id="文件和路径"><a href="#文件和路径" class="headerlink" title="文件和路径"></a>文件和路径</h3></li></ul><ol><li>命名规则：文件名的最大长度与文件系统类型有关，一般情况下，文件名不应该超过255个字符，虽然绝大多数的字符都可以用于文件名，但是最好使用英文大小写字母、数字、下划线、点这样的符号。文件名中虽然可以使用空格，但应该尽可能避免使用空格，否则在输入文件名时需要用将文件名放在双引号中或者通过\对空格进行转义。</li><li>扩展名：在Linux系统下文件的扩展名是可选的，但是使用扩展名有助于对文件内容的理解。有些应用程序要通过扩展名来识别文件，但是更多的应用程序并不依赖文件的扩展名，就像file命令在识别文件时并不是依据扩展名来判定文件的类型</li><li>隐藏文件：以点开头的文件在Linux系统中是隐藏文件（不可见文件）。<h3 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1.  */bin* - 基本命令的二进制文件。</span><br><span class="line">2.  */boot* - 引导加载程序的静态文件。</span><br><span class="line">3.  */dev* - 设备文件。</span><br><span class="line">4.  */etc* - 配置文件。</span><br><span class="line">5.  */home* - 普通用户主目录的父目录。</span><br><span class="line">6.  */lib* - 共享库文件。</span><br><span class="line">7.  */lib64* - 共享64位库文件。</span><br><span class="line">8.  */lost+found* - 存放未链接文件。</span><br><span class="line">9.  */media* - 自动识别设备的挂载目录。</span><br><span class="line">10.  */mnt* - 临时挂载文件系统的挂载点。</span><br><span class="line">11.  */opt* - 可选插件软件包安装位置。</span><br><span class="line">12.  */proc* - 内核和进程信息。</span><br><span class="line">13.  */root* - 超级管理员用户主目录。</span><br><span class="line">14.  */run* - 存放系统运行时需要的东西。</span><br><span class="line">15.  */sbin* - 超级用户的二进制文件。</span><br><span class="line">16.  */sys* - 设备的伪文件系统。</span><br><span class="line">17.  */tmp* - 临时文件夹。</span><br><span class="line">18.  */usr* - 用户应用目录。</span><br><span class="line">19.  */var* - 变量数据目录。</span><br></pre></td></tr></table></figure><h3 id="访问权限"><a href="#访问权限" class="headerlink" title="访问权限"></a>访问权限</h3></li><li><em>chmod</em> 改变文件模式比特<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ls -l</span></span><br><span class="line">...</span><br><span class="line">-rw-r--r--  1 root       root 211878 Jun 19 16:06 sohu.html</span><br><span class="line">...</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># chmod g+w,o+w sohu.html</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ls -l</span></span><br><span class="line">...</span><br><span class="line">-rw-rw-rw-  1 root       root 211878 Jun 19 16:06 sohu.html</span><br><span class="line">...</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># chmod 644 sohu.html</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ls -l</span></span><br><span class="line">...</span><br><span class="line">-rw-r--r--  1 root       root 211878 Jun 19 16:06 sohu.html</span><br><span class="line">说明：通过上面的例子可以看出，用chmod改变文件模式比特有两种方式：一种是字符设定法，另一种是数字设定法。  </span><br><span class="line">除了chmod之外，可以通过<span class="built_in">umask</span>来设定哪些权限将在新文件的默认权限中被删除。</span><br><span class="line">...</span><br></pre></td></tr></table></figure></li><li><em>chown</em> - 改变文件所有者。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ls -l</span></span><br><span class="line">...</span><br><span class="line">-rw-r--r--  1 root root     54 Jun 20 10:06 readme.txt</span><br><span class="line">...</span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># chown hellokitty readme.txt</span></span><br><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># ls -l</span></span><br><span class="line">...bash</span><br><span class="line">-rw-r--r--  1 hellokitty root     54 Jun 20 10:06 readme.txt</span><br></pre></td></tr></table></figure><h3 id="磁盘管理"><a href="#磁盘管理" class="headerlink" title="磁盘管理"></a>磁盘管理</h3></li><li>列出文件系统的磁盘使用状况: <em>df</em><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        40G  5.0G   33G  14% /</span><br><span class="line">devtmpfs        486M     0  486M   0% /dev</span><br><span class="line">tmpfs           497M     0  497M   0% /dev/shm</span><br><span class="line">tmpfs           497M  356K  496M   1% /run</span><br><span class="line">tmpfs           497M     0  497M   0% /sys/fs/cgroup</span><br><span class="line">tmpfs           100M     0  100M   0% /run/user/0</span><br></pre></td></tr></table></figure></li><li>磁盘分区操作: <em>fdisk</em><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># fdisk -l</span></span><br><span class="line">Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk label <span class="built_in">type</span>: dos</span><br><span class="line">Disk identifier: 0x000a42f4</span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *        2048    83884031    41940992   83  Linux</span><br><span class="line">Disk /dev/vdb: 21.5 GB, 21474836480 bytes, 41943040 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br></pre></td></tr></table></figure></li><li>格式化文件系统 <em>mkfs</em></li><li>文件系统检查 <em>fsck</em></li><li>挂载/卸载： <em>mount / umount</em></li></ol><h2 id="4-编辑器-vim"><a href="#4-编辑器-vim" class="headerlink" title="4 编辑器 vim"></a>4 编辑器 <strong>vim</strong></h2><ol><li><p>启动<code>vim</code>。可以通过<code>vi</code>或<code>vim</code>命令来启动<code>vim</code>，启动时可以指定文件名来打开一个文件，如果没有指定文件名，也可以在保存的时候指定文件名。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># vim guess.py</span></span><br></pre></td></tr></table></figure></li><li><p>命令模式、编辑模式和末行模式：启动<code>vim</code>进入的是命令模式（也称为<code>Normal</code>模式），在命令模式下输入英文字母i会进入编辑模式（<code>Insert</code>模式），屏幕下方出现– <code>INSERT</code> –提示；在编辑模式下按下<code>Esc</code>会回到命令模式，此时如果输入英文:会进入末行模式，在末行模式下输入<code>q!</code>可以在不保存当前工作的情况下强行退出<code>vim</code>；在命令模式下输入<code>v</code>会进入可视模式（<code>Visual</code>模式），可以用光标选择一个区域再完成对应的操作。</p></li><li><p>保存和退出<code>vim</code>：在命令模式下输入<code>:</code>进入末行模式，输入<code>wq</code>可以实现保存退出；如果想放弃编辑的内容输入<code>q!</code>强行退出，这一点刚才已经提到过了；在命令模式下也可以直接输入<code>ZZ</code>实现保存退出。如果只想保存文件不退出，那么可以在末行模式下输入<code>w</code>；可以在<code>w</code>后面输入空格再指定要保存的文件名。</p></li><li><p>光标操作：</p></li></ol><ul><li>在命令模式下可以通过<code>h、j、k、l</code>来控制光标向左、下、上、右的方向移动，可以在字母前输入数字来表示移动的距离，例如：<code>10h</code>表示向左移动<code>10</code>个字符。</li><li>在命令模式下可以通过<code>Ctrl+y</code>和<code>Ctrl+e</code>来实现向上、向下滚动一行文本的操作，可以通过<code>Ctrl+f</code>和<code>Ctrl+b</code>来实现向前和向后翻页的操作。</li><li>在命令模式下可以通过输入英文字母<code>G</code>将光标移到文件的末尾，可以通过<code>gg</code>将光标移到文件的开始，也可以通过在<code>G</code>前输入数字来将光标移动到指定的行。</li></ul><ol start="5"><li>文本操作</li></ol><ul><li>删除：在命令模式下可以用<code>dd</code>来删除整行；可以在dd前加数字来指定删除的行数；可以用<code>d$</code>来实现删除从光标处删到行尾的操作，也可以通过<code>d0</code>来实现从光标处删到行首的操作；如果想删除一个单词，可以使用<code>dw</code>；如果要删除全文，可以在输入<code>:%d</code>（其中:用来从命令模式进入末行模式）。</li><li>复制和粘贴：在命令模式下可以用<code>yy</code>来复制整行；可以在<code>yy</code>前加数字来指定复制的行数；可以通过<code>p</code>将复制的内容粘贴到光标所在的地方。</li><li>撤销和恢复：在命令模式下输入<code>u</code>可以撤销之前的操作；通过<code>Ctrl+r</code>可以恢复被撤销的操作。</li><li>对内容进行排序：在命令模式下输入<code>%!sort</code>。</li></ul><ol start="6"><li>查找和替换</li></ol><ul><li>查找操作需要输入/进入末行模式并提供正则表达式来匹配与之对应的内容，例如：<code>/doc.*\.</code>，输入n来向前搜索，也可以输入N来向后搜索。</li><li>替换操作需要输入:进入末行模式并指定搜索的范围、正则表达式以及替换后的内容和匹配选项，例如：<code>:1,$s/doc.*/hello/gice</code>，其中：<ul><li><code>g</code> - <code>global</code>：全局匹配。</li><li><code>i</code> - <code>ignore case</code>：忽略大小写匹配。</li><li><code>c</code> - <code>confirm</code>：替换时需要确认。</li><li><code>e</code> - <code>error</code>：忽略错误。</li></ul></li></ul><ol start="7"><li>参数设定在输入:进入末行模式后可以对vim进行设定。</li></ol><ul><li>设置<code>Tab</code>键的空格数：<code>set ts=4</code></li><li>置显示/不显示行号：<code>set nu / set nonu</code></li><li>设置启用/关闭高亮语法：<code>syntax on / syntax off</code></li><li>设置显示标尺（光标所在的行和列）： <code>set ruler</code></li><li>设置启用/关闭搜索结果高亮：<code>set hls / set nohls</code></li></ul><ul><li>说明：如果希望上面的这些设定在每次启动vim时都能生效，需要将这些设定写到用户主目录下的<code>.vimrc</code>文件中。</li></ul><ol start="8"><li>高级技巧</li></ol><ul><li>比较多个文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># vim -d foo.txt bar.txt</span></span><br></pre></td></tr></table></figure></li><li>打开多个文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@iZwz97tbgo9lkabnat2lo8Z ~]<span class="comment"># vim foo.txt bar.txt hello.txt</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>启动<code>vim</code>后只有一个窗口显示的是<code>foo.txt</code>，可以在末行模式中输入ls查看到打开的三个文件，也可以在末行模式中输入<code>b &lt;num&gt;</code>来显示另一个文件，例如可以用<code>:b 2</code>将<code>bar.txt</code>显示出来，可以用<code>:b 3</code>将<code>hello.txt</code>显示出来。</p></li><li><p>拆分和切换窗口：<br>可以在末行模式中输入<code>sp</code>或<code>vs</code>来实现对窗口的水平或垂直拆分，这样我们就可以同时打开多个编辑窗口，通过按两次<code>Ctrl</code>+w就可以实现编辑窗口的切换，在一个窗口中执行退出操作只会关闭对应的窗口，其他的窗口继续保留。</p></li><li><p>映射快捷键：在<code>vim</code>下可以将一些常用操作映射为快捷键来提升工作效率。</p><ul><li><p>例子1：在命令模式下输入<code>F4</code>执行从第一行开始删除10000行代码的操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:map &lt;F4&gt; gg10000dd。</span><br></pre></td></tr></table></figure></li><li><p>例子2：在编辑模式下输入__main直接补全为:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">:inoremap __main <span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p>说明：上面例子<code>2</code>的<code>inoremap</code>中的<code>i</code>表示映射的键在编辑模式使用，<code>nore</code>表示不要递归，这一点非常重要，否则如果键对应的内容中又出现键本身，就会引发递归（相当于进入了死循环）。如果希望映射的快捷键每次启动<code>vim</code>时都能生效，需要将映射写到用户主目录下的<code>.vimrc</code>文件中。</p></li><li><p>录制宏：</p><ul><li><p>在命令模式下输入<code>qa</code>开始录制宏（其中a是寄存器的名字，也可以是其他英文字母或<code>0-9</code>的数字）。</p></li><li><p>执行你的操作（光标操作、编辑操作等），这些操作都会被录制下来。</p></li><li><p>如果录制的操作已经完成了，按<code>q</code>结束录制。</p></li><li><p>通过<code>@a</code>（<code>a</code>是刚才使用的寄存器的名字）播放宏，如果要多次执行宏可以在前面加数字，例如<code>100@a</code>表示将宏播放<code>100</code>次。</p></li><li><p>可以试一试下面的例子来体验录制宏的操作，该例子来源于<code>Harttle Land</code>网站，该网站上提供了很多关于<code>vim</code>的使用技巧，有兴趣的可以去了解一下。 </p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理论计算基础</title>
      <link href="/2019/10/20/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/%E7%90%86%E8%AE%BA%E5%82%AC%E5%8C%96%E8%AE%A1%E7%AE%97(%E4%B8%80)/"/>
      <url>/2019/10/20/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/%E7%90%86%E8%AE%BA%E5%82%AC%E5%8C%96%E8%AE%A1%E7%AE%97(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<h1 id="（一）准备工作"><a href="#（一）准备工作" class="headerlink" title="（一）准备工作"></a>（一）准备工作</h1><h2 id="1-系统与软件部分"><a href="#1-系统与软件部分" class="headerlink" title="1 系统与软件部分"></a>1 系统与软件部分</h2><ul><li><code>Linux与windows</code>系统</li><li>编辑器：<code>windows</code>用<code>notepad++</code>编辑器，<code>linux</code>用<code>vim</code>编辑器</li><li>相关程序：</li></ul><ol><li><code>Materials studio</code> (用来建模)</li><li><code>Vesta</code>(用来进行可视化与文件转换)</li><li><code>VASP与CP2k</code>:用来做第一性原理计算的软件,CP2k是从头算分析动力学模拟，是表面催化计算的大杀器，资料少，学习困难</li><li><a href="http://www,psvasp.at" target="_blank" rel="noopener">p4vasp</a>:vasp计算结果的后处理程序</li><li><a href="http://www.ks.uiuc.edu/Research/vmd/" target="_blank" rel="noopener">VMD</a>:分子动力学的可视化程序，作图的玩着，自由度很高，使用复杂</li></ol><h2 id="2-理论知识部分"><a href="#2-理论知识部分" class="headerlink" title="2 理论知识部分"></a>2 理论知识部分</h2><h3 id="2-1-催化化学与量子化学"><a href="#2-1-催化化学与量子化学" class="headerlink" title="2.1 催化化学与量子化学"></a>2.1 催化化学与量子化学</h3><p>这里推荐两本书，<br><img src="https://ss0.baidu.com/73F1bjeh1BF3odCf/it/u=668807871,692209756&amp;fm=85&amp;s=C940E8110E375A88742D76C50300D0A0" alt="电催化，孙世刚院士著"><img src="https://ss0.baidu.com/73F1bjeh1BF3odCf/it/u=2760061665,532098802&amp;fm=85&amp;s=B22BF604505753CC0292E9CC030050BA" alt="量子化学"></p><h3 id="2-2-密度泛函理论"><a href="#2-2-密度泛函理论" class="headerlink" title="2.2 密度泛函理论"></a>2.2 密度泛函理论</h3><p>这里也推荐本书：<br><img src="https://img.vim-cn.com/1d/caf343d9bf941549911857e38766ae912b5f66.jpg" alt=""></p><h1 id="（二）催化模型构建"><a href="#（二）催化模型构建" class="headerlink" title="（二）催化模型构建"></a>（二）催化模型构建</h1><h2 id="3-晶体结构数据库的使用"><a href="#3-晶体结构数据库的使用" class="headerlink" title="3 晶体结构数据库的使用"></a>3 晶体结构数据库的使用</h2><h3 id="3-1-相关说明"><a href="#3-1-相关说明" class="headerlink" title="3.1 相关说明"></a>3.1 相关说明</h3><p>第一篇单原子催化文章：<em>Nat. Chem., 2011, 3,634-641</em><br>研究晶体结构的文献都会给出结构的详细参数：比如，<em>J. Am. Chem. Soc. 136, 20, 7221-7224</em><br>注意：不要用<code>MS</code>里<code>build–crystals–build crystals</code>对着文献输入参数。费了半天劲还容易搞错，在晶体数据库里可以直接找到cif结构文件。</p><h3 id="3-2-晶体结构与数据库"><a href="#3-2-晶体结构与数据库" class="headerlink" title="3.2 晶体结构与数据库"></a>3.2 晶体结构与数据库</h3><ul><li>问题：找晶体结构到底在找什么？<br>答：<code>CIF</code>文件，后缀名为<code>.cif</code>，内部含有结构信息</li><li>常用的晶体结构数据库：<ul><li><code>ICSD – the Inorganic Crystal Structure Database</code> 无机晶体数据库。<a href="http://www2.fiz-karlsruhe.de/icsd_home.html" target="_blank" rel="noopener">http://www2.fiz-karlsruhe.de/icsd_home.html</a></li><li><code>CCDC – The Cambridge Crystallographic Data Centre</code><a href="https://www.ccdc.cam.ac.uk/" target="_blank" rel="noopener">https://www.ccdc.cam.ac.uk/</a></li><li><code>Materials studio</code>自带晶体数据库</li><li><code>Materials Project</code>（强烈推荐）：<a href="https://materialsproject.org/" target="_blank" rel="noopener">https://materialsproject.org/</a><br>特色，不但有实验结构参数，还有理论计算数据，比如磁矩，形成能，密度，带隙，空间群，点群，晶系，能带结构，弹性张量， 压电张量等数据。截至2018年9月11日，收录83989种无机化合物， 52179个能带结构</li><li>AMCSD – American Mineralogist Crystal Structure Database：<a href="http://rruff.geo.arizona.edu/AMS/amcsd.php" target="_blank" rel="noopener">http://rruff.geo.arizona.edu/AMS/amcsd.php</a></li><li>google search （ex： Al2O3 filetype:cif ）<h3 id="3-3-CCDC实战训练（一）-从文章中找到晶体结构"><a href="#3-3-CCDC实战训练（一）-从文章中找到晶体结构" class="headerlink" title="3.3 CCDC实战训练（一）-从文章中找到晶体结构"></a>3.3 CCDC实战训练（一）-从文章中找到晶体结构</h3></li></ul></li><li>如何从文章找到晶体结构？<br>答： 直接到文章末尾去找<code>CCDC</code>编码，但有时晶体结构也会出现在文章中或者SI里面。</li><li>如何从晶体数据库中获得结构文件？<ul><li>登录数据库查找<code>CCDC</code>编码</li></ul><ul><li>下载<code>CIF</code>文件</li></ul></li></ul><h3 id="3-4-ISDC-实战训练（二）-得到各种AL2O3模型"><a href="#3-4-ISDC-实战训练（二）-得到各种AL2O3模型" class="headerlink" title="3.4 ISDC 实战训练（二）-得到各种AL2O3模型"></a>3.4 ISDC 实战训练（二）-得到各种AL2O3模型</h3><p><code>Materials studio</code> 只有一种<code>Al2O3</code> 模型，是<code>a</code>型的<code>trigonal</code>晶系，也称作<code>corundum</code>刚玉。如果想要得到不同晶型的<code>Al2O3</code>就要去晶体数据库上找。</p><ul><li>总结：科研中碰到一个晶体，应该怎么找对应的结构文件。</li><li>依次尝试下列方法：</li></ul><ol><li>在<code>materials project</code>中直接输入对应元素和原子个数。如果<br>搞不清该化合物的晶型，提前<code>Google</code>该晶体所属晶系，点<br>群和空间群。</li><li>在<code>ICSD chemistry</code>中输入对应元素和原子个数。（<code>ICSD</code>是<br>最全的无机晶体数据库，如果这都找不到结构，应该回头<br>看那里搞错了）</li><li>在文献中找结构，然后去ICSD搜索该文献。</li><li>直接<code>google</code>，例如， <code>black phosphorus CIF</code><br>想偷懒可以在<code>MS</code>里， <code>File-input-structures</code>里找结构， <code>MS</code>里只有非常少数的常见结构。</li></ol><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>非常感谢研之成理和清华化学系刘锦程博士，微信搜索研之成理就可以<code>get</code>一个非常非常优质的公众号了！！</p>]]></content>
      
      
      <categories>
          
          <category> 量子计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 量子计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo + GithubPages 搭建个人博客与网站！</title>
      <link href="/2019/10/15/%E9%85%8D%E7%BD%AE/Hexo-GithubPages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/"/>
      <url>/2019/10/15/%E9%85%8D%E7%BD%AE/Hexo-GithubPages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-GithubPages-搭建个人博客与网站！"><a href="#Hexo-GithubPages-搭建个人博客与网站！" class="headerlink" title="Hexo + GithubPages 搭建个人博客与网站！"></a>Hexo + GithubPages 搭建个人博客与网站！</h1><h2 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h2><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><ol><li><p><a href="https://www.simon96.online/2018/11/10/hexo-env/" target="_blank" rel="noopener">node.js 下载</a>，并安装。</p></li><li><p><a href="https://www.simon96.online/2018/11/10/hexo-env/" target="_blank" rel="noopener">Git 下载</a>，并安装。</p></li><li><p>安装<code>Hexo</code>，在命令行（即<code>Git Bash</code>）运行以下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>初始化<code>hexo</code>,在命令行依次运行以下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init folder</span><br><span class="line">$ <span class="built_in">cd</span> folder</span><br><span class="line">$ npm install hexo server</span><br></pre></td></tr></table></figure><blockquote><p>新建完成后，会在路径下产生下列文件和文件夹：  </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── _config.yml  % 站点配置文件</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts  % 文章发布文件夹</span><br><span class="line"> themes  % 主题配置文件夹，可folk别人的模板</span><br></pre></td></tr></table></figure></li><li><p>启动服务，在命令行输入:  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></li><li><p>浏览器访问网址,至此，<code>hexo</code>博客已经搭建到本地了</p></li></ol><h3 id="Github实时搭建"><a href="#Github实时搭建" class="headerlink" title="Github实时搭建"></a><code>Github</code>实时搭建</h3><ol><li>在github官网创建账号</li><li>创建仓库：<code>&lt;账号名称&gt;github.io</code></li><li>将本地博客推送到<code>githubPages</code>。</li></ol><ul><li><p>安装<code>hexo-deplyer-git</code>插件。<code>bash</code>命令行运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li><li><p>添加<code>github</code>的<code>SSH-KEY</code>,创建一个<code>SSH-KEY</code>,在命令行输入:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"邮箱地址"</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><p><code>C:\Users\Administrator\.ssh\id_rsa.pub</code></p></blockquote><ul><li>修改<code>_config.yml</code>,文件末尾修改为:<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span>   <span class="string">%</span> <span class="string">注意冒号后面有一个空格</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:shyshy903/shyshy903.github.io</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure></li><li>推送到<code>githubPages</code>中。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><blockquote><p>之后在浏览器输入：<a href="http://shyshy903.github.io">http://shyshy903.github.io</a> 就可以访问个人博客了  </p></blockquote></li></ul><h3 id="添加域名，创建个人网站"><a href="#添加域名，创建个人网站" class="headerlink" title="添加域名，创建个人网站"></a>添加域名，创建个人网站</h3><ol><li>到万网或者阿里云买一个域名，进行<code>DNS</code>解析</li><li><code>DNS</code>解析：<br>类型选择为 <code>CNAME</code>;<br>机记录即域名前缀，填写为<code>www</code>;<br>录值填写为<code>&lt;Github账号名称&gt;.github.io</code>;<br>解析线路，<code>TTL</code> 默认即可</li><li>仓库设置  </li><li><ol><li>打开博客仓库设置：<code>https://github.com/&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io/settings</code>  </li></ol></li><li><ol start="2"><li>在<code>Custom domain</code>下，填写自定义域名，点击<code>save</code>； </li></ol></li><li><ol start="3"><li>在站点目录的<code>source</code>文件夹下，创建并打开<code>CNAME.txt</code>，写入你的域名（如<code>www.simon96.online</code>），保存，并重命名为<code>CNAME</code><blockquote><p>完成以上步骤，就可以通过域名<code>www.shyshy903.top</code>来访问个人网站和博客了。</p></blockquote></li></ol></li></ol><h3 id="安装必要的插件"><a href="#安装必要的插件" class="headerlink" title="安装必要的插件"></a>安装必要的插件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i -S hexo-generator-search hexo-generator-json-content hexo-renderer-less</span><br></pre></td></tr></table></figure><h3 id="站点配置-hexo根目录下config-yml文件"><a href="#站点配置-hexo根目录下config-yml文件" class="headerlink" title="站点配置-hexo根目录下config.yml文件"></a>站点配置-<code>hexo</code>根目录下<code>config.yml</code>文件</h3><ul><li>多语言支持<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">language:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">en</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">zh-HK</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">zh-TW</span></span><br></pre></td></tr></table></figure></li><li>搜索框的配置<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>站点文件添加下面的代码块：<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">    <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">    <span class="attr">format:</span> <span class="string">html</span></span><br><span class="line">    <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure><h3 id="主题优化之自定样式-cdn的使用"><a href="#主题优化之自定样式-cdn的使用" class="headerlink" title="主题优化之自定样式 cdn的使用"></a>主题优化之自定样式 <code>cdn</code>的使用</h3><code>cdn + github</code>的博客可以参考别人的文章<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">############################### 基本信息 ###############################</span></span><br><span class="line"><span class="attr">info:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">Material</span> <span class="string">X</span></span><br><span class="line">  <span class="attr">docs:</span> <span class="string">https://xaoxuu.com/wiki/material-x/</span></span><br><span class="line">  <span class="attr">cdn:</span> <span class="comment"># 把对应的那一行注释掉就使用本地的文件</span></span><br><span class="line">    <span class="attr">css:</span></span><br><span class="line">      <span class="comment"># style: https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9.9/css/style.css</span></span><br><span class="line">    <span class="attr">js:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9/js/app.js</span></span><br><span class="line">      <span class="attr">search:</span> <span class="string">https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9/js/search.js</span></span><br><span class="line">      <span class="attr">volantis:</span> <span class="string">https://cdn.jsdelivr.net/gh/xaoxuu/volantis@1.0.5/js/volantis.min.js</span></span><br></pre></td></tr></table></figure><h3 id="主题优化之评论系统-valine的使用"><a href="#主题优化之评论系统-valine的使用" class="headerlink" title="主题优化之评论系统 valine的使用"></a>主题优化之评论系统 <code>valine</code>的使用</h3></li><li>站点配置<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud:</span></span><br><span class="line">  <span class="attr">app_id:</span> <span class="string">你的appId</span>   <span class="comment"># 从leancloud官网获得：https://www.avoscloud.com/dashboard/</span></span><br><span class="line">  <span class="attr">app_key:</span> <span class="string">你的appKey</span></span><br></pre></td></tr></table></figure></li><li>主题配置<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span> <span class="comment"># 如果你想用Valine评论系统，请设置enable为true</span></span><br><span class="line">  <span class="attr">volantis:</span> <span class="literal">true</span> <span class="comment"># 是否启用volantis版本（禁止匿名，增加若干贴吧、QQ表情）</span></span><br><span class="line">  <span class="comment"># 还需要在根目录配置文件中添加下面这三行内容</span></span><br><span class="line">  <span class="comment"># leancloud:</span></span><br><span class="line">  <span class="comment">#   app_id: 你的appId</span></span><br><span class="line">  <span class="comment">#   app_key: 你的appKey</span></span><br><span class="line">  <span class="attr">guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment">#valine comment header info</span></span><br><span class="line">  <span class="attr">placeholder:</span> <span class="string">快来评论吧~</span> <span class="comment"># valine comment input placeholder(like: Please leave your footprints )</span></span><br><span class="line">  <span class="attr">avatar:</span> <span class="string">mp</span> <span class="comment"># gravatar style https://valine.js.org/avatar</span></span><br><span class="line">  <span class="attr">pageSize:</span> <span class="number">20</span> <span class="comment"># comment list page size</span></span><br><span class="line">  <span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># valine verify code (true/false)</span></span><br><span class="line">  <span class="attr">notify:</span> <span class="literal">false</span> <span class="comment"># valine mail notify (true/false)</span></span><br><span class="line">  <span class="attr">lang:</span> <span class="string">zh-cn</span></span><br><span class="line">  <span class="attr">highlight:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="主题优化之加入萌萌哒表情"><a href="#主题优化之加入萌萌哒表情" class="headerlink" title="主题优化之加入萌萌哒表情"></a>主题优化之加入萌萌哒表情</h3></li></ul><ol><li><p>这里需要安装一个插件哦:  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-helper-live2d --save</span><br></pre></td></tr></table></figure></li><li><p>复制你喜欢的名字，如<code>z16</code>。</p></li><li><p>在<code>hexo</code>文件夹中建立一个文件夹<code>live2d_models</code>。  </p></li><li><p>1 在<code>live2d_models</code>中建立文件夹<code>z16</code>。  </p></li><li><p>2 在文件夹啊<code>z16</code>中创建<code>json</code>文件：<code>z16.model.json</code>。</p></li><li><p>将以下代码添加到主题配置文件中去：  </p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">live2d:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">scriptFrom:</span> <span class="string">local</span></span><br><span class="line">  <span class="attr">pluginRootPath:</span> <span class="string">live2dw/</span></span><br><span class="line">  <span class="attr">pluginJsPath:</span> <span class="string">lib/</span></span><br><span class="line">  <span class="attr">pluginModelPath:</span> <span class="string">assets/</span></span><br><span class="line">  <span class="attr">tagMode:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">log:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">model:</span></span><br><span class="line">    <span class="attr">use:</span> <span class="string">live2d-widget-model-z16</span></span><br><span class="line">  <span class="attr">display:</span></span><br><span class="line">    <span class="attr">position:</span> <span class="string">right</span></span><br><span class="line">    <span class="attr">width:</span> <span class="number">150</span></span><br><span class="line">    <span class="attr">height:</span> <span class="number">300</span></span><br><span class="line">  <span class="attr">mobile:</span></span><br><span class="line">    <span class="attr">show:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>安装模型:  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install liv2d_models-widget-z16 --save</span><br></pre></td></tr></table></figure></li><li><p>在命令行中运行命令，既可以在<code>&quot;http://localhost:4000&quot;</code>中预览自己的博客网页  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure></li><li><p>如果预览的博客符合自己的要求，就可以进行更新,大公告成啦：  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d -g</span><br></pre></td></tr></table></figure></li><li><p>如果需要调整插入模型的透明度，可以在第4步中的代码中插入：  </p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">display:</span></span><br><span class="line">    <span class="attr">position:</span> <span class="string">right</span></span><br><span class="line">    <span class="attr">width:</span> <span class="number">300</span></span><br><span class="line">    <span class="attr">height:</span> <span class="number">600</span></span><br><span class="line">    <span class="attr">opacity:</span> <span class="number">0.4</span></span><br><span class="line">  <span class="attr">mobile:</span></span><br><span class="line">    <span class="attr">show:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">react:</span></span><br><span class="line">    <span class="attr">opacity:</span> <span class="number">0.4</span></span><br><span class="line">    <span class="attr">opacityOnHover:</span> <span class="number">0.7</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/10/15/hello-world/"/>
      <url>/2019/10/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
