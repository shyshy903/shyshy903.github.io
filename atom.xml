<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>what hurts more, the pain of hard work or the pain of regret?</title>
  
  <subtitle>just do it</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shyshy903.github.io/"/>
  <updated>2021-01-31T16:00:00.000Z</updated>
  <id>https://shyshy903.github.io/</id>
  
  <author>
    <name>Haiyang Song</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>epoll</title>
    <link href="https://shyshy903.github.io/2021/02/01/Linux/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E7%90%86%E8%A7%A3epoll/"/>
    <id>https://shyshy903.github.io/2021/02/01/Linux/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E7%90%86%E8%A7%A3epoll/</id>
    <published>2021-01-31T16:00:00.000Z</published>
    <updated>2021-01-31T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="手动实现与理解epoll"><a href="#手动实现与理解epoll" class="headerlink" title="手动实现与理解epoll"></a>手动实现与理解epoll</h1><h2 id="进程阻塞为什么不占用CPU资源？"><a href="#进程阻塞为什么不占用CPU资源？" class="headerlink" title="进程阻塞为什么不占用CPU资源？"></a>进程阻塞为什么不占用CPU资源？</h2><p>程序的运行状态有等待和执行等几种状态，其中等待状态就是阻塞状态。</p><p>等待队列是一种重要的数据结构，当进程创建socket语句是，会创建一个socket对象，socket对象包含了发送缓冲区，接收缓冲区，等待队列等成员。</p><p>当程序执行到recv()语句时，操作系统就会将进程A移动到socket的等待队列中去，继续转而去执行进程B\C，因此此时进程A被阻塞，并没有占用CPU资源。</p><p>当socket接收到数据后，等待队列上的进程就会被唤醒，重新放回工作队列，进程变为运行状态。相当于socket的接收缓冲区已经有了数据。</p><h2 id="操作系统如何知道网络数据对应于哪个socket-如何同时监听多个socket数据？"><a href="#操作系统如何知道网络数据对应于哪个socket-如何同时监听多个socket数据？" class="headerlink" title="操作系统如何知道网络数据对应于哪个socket, 如何同时监听多个socket数据？"></a>操作系统如何知道网络数据对应于哪个socket, 如何同时监听多个socket数据？</h2><p>网络数据包包含了ip和端口号信息，一个socket对应着一个端口号。内核可以通过端口号，找到对应的socket。操作i系统会维护端口号到socket的索引的数据结构。</p><p>而epoll的要义就是高效的监听多个socket。</p><p>首先看看select<br>在如下的代码中，先准备一个数组（下面代码中的fds），让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个（也可以是多个）socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD_ISSET判断具体哪个socket收到数据，然后做出处理</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> s = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);  </span><br><span class="line">bind(s, ...)</span><br><span class="line">listen(s, ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> fds[] =  存放需要监听的socket</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">    <span class="keyword">int</span> n = select(..., fds, ...)</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; fds.count; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(FD_ISSET(fds[i], ...))&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//fds[i]的数据处理</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="select-有什么缺点："><a href="#select-有什么缺点：" class="headerlink" title="select 有什么缺点："></a>select 有什么缺点：</h2><p>其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历（遍历进程A关心的所有socket，需要注意的是添加从等待队列头部添加，删除通过回调直接实现，所以每个socket的等待队列不用遍历），而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。<br>其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次（这一次遍历是在应用层）。<br>那么，有没有减少遍历的方法？有没有保存就绪socket的方法？这两个问题便是epoll技术要解决的。<br>当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞</p><h2 id="epoll高效的原因"><a href="#epoll高效的原因" class="headerlink" title="epoll高效的原因"></a>epoll高效的原因</h2><ol><li>功能分离<br>select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用select都需要这两步操作，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程（解耦）。显而易见的，效率就能得到提升</li><li>就绪列表<br>select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据。</li></ol><p>不重复传递socket句柄给内核，通过内核中红黑树存储要监控的句柄，通过双链表存储准备就绪的事件，并结合回调机制，造就了epoll的高效</p><h2 id="epoll使用了哪些数据结构？"><a href="#epoll使用了哪些数据结构？" class="headerlink" title="epoll使用了哪些数据结构？"></a>epoll使用了哪些数据结构？</h2><p>epool_creat()返回一个句柄，会创建一个event_poll结构体<br>红黑树:存储需要监听的事件<br>就绪列表:当红黑树的事件发生，调用epoll_ctl_callback将改事件添加到rdlist中去。</p><p>eptim对应一个事件</p><p><img src="&quot;https://s1.51cto.com/images/20180325/1521963216966024.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&quot;" alt="avatar"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;手动实现与理解epoll&quot;&gt;&lt;a href=&quot;#手动实现与理解epoll&quot; class=&quot;headerlink&quot; title=&quot;手动实现与理解epoll&quot;&gt;&lt;/a&gt;手动实现与理解epoll&lt;/h1&gt;&lt;h2 id=&quot;进程阻塞为什么不占用CPU资源？&quot;&gt;&lt;a href
      
    
    </summary>
    
    
      <category term="Linux" scheme="https://shyshy903.github.io//categories/Linux/"/>
    
    
      <category term="linux" scheme="https://shyshy903.github.io//tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>LRU缓存算法</title>
    <link href="https://shyshy903.github.io/2021/01/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/"/>
    <id>https://shyshy903.github.io/2021/01/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/</id>
    <published>2021-01-29T16:00:00.000Z</published>
    <updated>2021-01-29T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>LRU缓存算法是双向链表与散列表的合体结构</p><p>需要考虑的问题</p><ol><li><p>当新数据在缓存节点中，找到缓存节点的位置，删除该节点，插入到链表的头节点中</p></li><li><p>当新数据不在缓存列表中，直接插入头部</p></li><li><p>缓存列表满了，删除尾部节点，插入头部节点。</p></li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *双链表结点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> val,key;</span><br><span class="line">    node *prev,*next;</span><br><span class="line">    node():prev(<span class="literal">NULL</span>),next(<span class="literal">NULL</span>)&#123;&#125;</span><br><span class="line">    node(<span class="keyword">int</span> k,<span class="keyword">int</span> v):key(k),val(v),prev(<span class="literal">NULL</span>),next(<span class="literal">NULL</span>)&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//重载 == 号</span></span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> == (<span class="keyword">const</span> node &amp;p) <span class="keyword">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> val==p.val&amp;&amp;key==p.key;</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *双链表</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">DoubleList</span>&#123;</span></span><br><span class="line">     </span><br><span class="line">     <span class="keyword">public</span>:</span><br><span class="line">        node *first;</span><br><span class="line">        node *end;</span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">        DoubleList();</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(node*)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(node*)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">removeLast</span><span class="params">()</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line"> &#125;;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *构造函数，新建首尾节点，相连</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line">DoubleList::DoubleList()&#123;</span><br><span class="line">    n=<span class="number">0</span>;</span><br><span class="line">    first = <span class="keyword">new</span> node();</span><br><span class="line">    end = <span class="keyword">new</span> node();</span><br><span class="line">    first-&gt;next = end;</span><br><span class="line">    end-&gt;prev = first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *在第一位添加一个节点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DoubleList::addFirst</span><span class="params">(node *nd)</span></span>&#123;</span><br><span class="line">    n++;</span><br><span class="line">    <span class="comment">//node *tmp = new node(nd-&gt;key,nd&gt;val);</span></span><br><span class="line">    node *t = first-&gt;next;</span><br><span class="line">    nd-&gt;next = t;</span><br><span class="line">    first-&gt;next = nd;</span><br><span class="line">    nd-&gt;prev = first;</span><br><span class="line">    t-&gt;prev = nd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *删除一个肯定存在的节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DoubleList::remove</span><span class="params">(node *nd)</span></span>&#123;</span><br><span class="line">    n--;</span><br><span class="line"></span><br><span class="line">    node *p = first;</span><br><span class="line">    <span class="keyword">while</span>(p-&gt;key!=nd-&gt;key)&#123;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    node *pt = p-&gt;prev;</span><br><span class="line">    node *nt = p-&gt;next;</span><br><span class="line">    pt-&gt;next = nt;</span><br><span class="line">    nt-&gt;prev = pt;</span><br><span class="line">    <span class="keyword">delete</span> p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *删除最后一个节点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DoubleList::removeLast</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        node *tmp = end-&gt;prev;</span><br><span class="line">        node *pt = tmp-&gt;prev;</span><br><span class="line">    </span><br><span class="line">        pt-&gt;next = end;</span><br><span class="line">        end-&gt;prev = pt;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> t = tmp-&gt;key;</span><br><span class="line">        <span class="keyword">delete</span> tmp;</span><br><span class="line">        n--;</span><br><span class="line">        <span class="keyword">return</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DoubleList::size</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LRUCache</span>&#123;</span></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,node&gt; <span class="built_in">map</span>;</span><br><span class="line">        DoubleList *lru;    </span><br><span class="line">        <span class="keyword">int</span> maxSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        LRUCache()&#123;&#125;;</span><br><span class="line">        LRUCache(<span class="keyword">int</span> ms);</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> key)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span> val)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">show</span><span class="params">()</span></span>;</span><br><span class="line"> &#125;;</span><br><span class="line"></span><br><span class="line">LRUCache::LRUCache(<span class="keyword">int</span> ms)&#123;</span><br><span class="line">    maxSize = ms;</span><br><span class="line">    lru = <span class="keyword">new</span> DoubleList();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *查找该节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">LRUCache::get</span><span class="params">(<span class="keyword">int</span> key)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>.count(key)==<span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val = <span class="built_in">map</span>.find(key)-&gt;second.val;</span><br><span class="line">        put(key,val);</span><br><span class="line">        <span class="keyword">return</span> val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*将节点提前</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LRUCache::put</span><span class="params">(<span class="keyword">int</span> key,<span class="keyword">int</span> value)</span></span>&#123;</span><br><span class="line">    node *nd = <span class="keyword">new</span> node(key,value);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">map</span>.count(nd-&gt;key)==<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="comment">//移到前面</span></span><br><span class="line">        lru-&gt;remove(nd);</span><br><span class="line">        lru-&gt;addFirst(nd);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(lru-&gt;n==maxSize)&#123;</span><br><span class="line">            <span class="keyword">int</span> k = lru-&gt;removeLast();</span><br><span class="line">            <span class="built_in">map</span>.erase(k);</span><br><span class="line">            lru-&gt;addFirst(nd);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            lru-&gt;addFirst(nd);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">map</span>[key] = *nd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">LRUCache::show</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(lru-&gt;n==<span class="number">0</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"empty task"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        node *p = lru-&gt;first-&gt;next;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"当前一共有"</span>&lt;&lt;lru-&gt;n&lt;&lt;<span class="string">"个任务: "</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;lru-&gt;n;i++)&#123;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">"第"</span>&lt;&lt;i+<span class="number">1</span>&lt;&lt;<span class="string">"个任务: "</span>&lt;&lt;p-&gt;val&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">            p=p-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    LRUCache *l = <span class="keyword">new</span> LRUCache(<span class="number">3</span>);</span><br><span class="line">    l-&gt;put(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">    l-&gt;put(<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">    l-&gt;put(<span class="number">3</span>,<span class="number">4</span>);</span><br><span class="line">    l-&gt;put(<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line">    l-&gt;show();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;LRU缓存算法是双向链表与散列表的合体结构&lt;/p&gt;
&lt;p&gt;需要考虑的问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;当新数据在缓存节点中，找到缓存节点的位置，删除该节点，插入到链表的头节点中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当新数据不在缓存列表中，直接插入头部&lt;/p&gt;
&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>传输层</title>
    <link href="https://shyshy903.github.io/2020/12/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/"/>
    <id>https://shyshy903.github.io/2020/12/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E4%BC%A0%E8%BE%93%E5%B1%82/</id>
    <published>2020-12-27T16:00:00.000Z</published>
    <updated>2020-12-27T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h1><p><strong>传输层（英语：Transport Layer）</strong>在计算机网络中是互联网协议包与开放系统互连（OSI）网络堆栈中协议的分层结构中的方法的一个概念划分。该层的协议为应用进程提供端到端的通信服务。[1] 它提供面向连接的数据流支持、可靠性、流量控制、多路复用等服务。</p><p>从通信和信息处理的角度看，传输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层<br>互联网与一般性网络的开放系统互连OSI模型的基础，TCP/IP模型的传输层的具体实现和含义（RFC 1122）[2]是不同的。在OSI模型中传输层最常被称作第4层或L4，而TCP/IP中不常给网络层编号。<br>IP 协议能够将 IP 数据报由源主机交付至目的主机，那么，为什么还需要传输层呢？这是因为，从 IP 层来说，通信的两端是两个主机。IP 数据报的首部明确地标志了这两个主机的 IP 地址。但是 “两个主机之间的通信” 这种说法还不够清楚，这是因为，真正进行通信的实体是在主机中的进程，是这个主机的一个进程和另外一个主机中的一个进程在交换数据。因此，严格来讲，两个主机进行通信就是两个主机中的应用进程互相通信</p><p>IP 协议虽然将分组送到目的主机，但是这个分组还停留在主机的网络层而没有交付主机中的应用进程。从传输层的角度看，通信的真正端点并不是主机而是主机中的进程。也就是说，端到端的通信是应用进程之间的通信</p><p>传输层为相互通信的应用进程提供逻辑通信，即所谓的 “端“到” 端“通信。并负责对收到的报文进行差错检验，消除网络间不可靠性，提供从源端主机到目的端主机的可靠的、与实际使用的网络无关的信息传输</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FFhSC4.jpg#shadow"></img></div><p>最著名的TCP/IP传输协议是传输控制协议（TCP）, 它的名称借用自整个包的名称。它用于面向连接的传输，而无连接的用户数据报协议（UDP）用于简单消息传输。TCP是更复杂的协议，因为它的状态性设计结合了可靠传输和数据流服务。这个协议组中其他重要协议有数据拥塞控制协议（DCCP）与流控制传输协议（SCTP）。</p><h2 id="复用与分用"><a href="#复用与分用" class="headerlink" title="复用与分用"></a>复用与分用</h2><ul><li>复用是指发送方不同的应用进程都可以使用同一个传输层协议传送数据（需要加适当首部）</li><li>分用是指接收方的传输层在剥去报文的首部后，可以将这些数据正确交付目的应用进程</li></ul><h2 id="TCP与UDP"><a href="#TCP与UDP" class="headerlink" title="TCP与UDP"></a>TCP与UDP</h2><p>TCP/IP 的传输层有两个不同的协议：  </p><p>(1) 用户数据报协议 UDP (User Datagram Protocol)  </p><p>(2) 传输控制协议 TCP (Transmission Control Protocol)</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FF4OtU.jpg#shadow"></img></div><p>UDP 在传送数据之前不需要先建立连接。对方的运输层在收到 UDP 报文后，不需要给出任何确认。UDP 不提供可靠交付</p><p>TCP 则提供面向连接的服务。TCP 不提供广播或多播服务。由于 TCP 要提供可靠的、面向连接的运输服务，因此不可避免地增加了许多的开销。这不仅使协议数据单元的首部增大很多，也占用许多的处理机资源</p><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><p>传输控制协议（英语：Transmission Control Protocol，缩写：TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。在简化的计算机网络OSI模型中，它完成第四层传输层所指定的功能。用户数据报协议（UDP）是同一层内另一个重要的传输协议。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Tcp_state_diagram_fixed_new.svg/375px-Tcp_state_diagram_fixed_new.svg.png"></img></div><p>在因特网协议族（Internet protocol suite）中，TCP层是位于IP层之上，应用层之下的中间层。不同主机的应用层之间经常需要可靠的、像管道一样的连接，但是IP层不提供这样的流机制，而是提供不可靠的包交换。</p><p>应用层向TCP层发送用于网间传输的、用8位字节表示的数据流，然后TCP把数据流分割成适当长度的报文段（通常受该计算机连接的网络的数据链路层的最大传输单元（MTU）的限制）。之后TCP把结果包传给IP层，由它来透过网络将包传送给接收端实体的TCP层。TCP为了保证不发生丢包，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的包发回一个相应的确认信息（ACK）；如果发送端实体在合理的往返时延（RTT）内未收到确认，那么对应的数据包就被假设为已丢失并进行重传。TCP用一个校验和函数来检验数据是否有错误，在发送和接收时都要计算校验和。</p><h3 id="运作方式"><a href="#运作方式" class="headerlink" title="运作方式"></a>运作方式</h3><p>数据在TCP层称为流（Stream），数据分组称为分段（Segment）。作为比较，数据在IP层称为Datagram，数据分组称为分片（Fragment）。 UDP 中分组称为Message。</p><p>TCP协议的运行可划分为三个阶段：连接创建(connection establishment)、数据传送（data transfer）和连接终止（connection termination）。操作系统将TCP连接抽象为套接字表示的本地端点（local end-point），作为编程接口给程序使用。在TCP连接的生命期内，本地端点要经历一系列的状态改变。</p><h3 id="创建通路"><a href="#创建通路" class="headerlink" title="创建通路"></a>创建通路</h3><p>TCP用三次握手（或称三路握手，three-way handshake）过程创建一个连接。在连接创建过程中，很多参数要被初始化，例如序号被初始化以保证按序传输和连接的强壮性。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Connection_TCP.png/330px-Connection_TCP.png"></img></div>一对终端同时初始化一个它们之间的连接是可能的。但通常是由一端打开一个套接字（socket）然后监听来自另一方的连接，这就是通常所指的被动打开（passive open）。服务器端被被动打开以后，用户端就能开始创建主动打开（active open）。1. 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序号设定为随机数A。2. 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK的确认码应为A+1，SYN/ACK包本身又有一个随机产生的序号B。3. 最后，客户端再发送一个ACK。此时包的序号被设定为A+1，而ACK的确认码则为B+1。当服务端收到这个ACK的时候，就完成了三次握手，并进入了连接创建状态。如果服务器端接到了客户端发的SYN后回了SYN-ACK后客户端掉线了，服务器端没有收到客户端回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，服务器端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会断开这个连接。使用三个TCP参数来调整行为：tcp_synack_retries 减少重试次数；tcp_max_syn_backlog，增大SYN连接数；tcp_abort_on_overflow决定超出能力时的行为。### 资源使用主机收到一个TCP包时，用两端的IP地址与端口号来标识这个TCP包属于哪个session。使用一张表来存储所有的session，表中的每条称作Transmission Control Block（TCB），tcb结构的定义包括连接使用的源端口、目的端口、目的ip、序号、应答序号、对方窗口大小、己方窗口大小、tcp状态、tcp输入/输出队列、应用层输出队列、tcp的重传有关变量等。服务器端的连接数量是无限的，只受内存的限制。客户端的连接数量，过去由于在发送第一个SYN到服务器之前需要先分配一个随机空闲的端口，这限制了客户端IP地址的对外发出连接的数量上限。从Linux 4.2开始，有了socket选项IP_BIND_ADDRESS_NO_PORT，它通知Linux内核不保留usingbind使用端口号为0时内部使用的临时端口（ephemeral port），在connect时会自动选择端口以组成独一无二的四元组（同一个客户端端口可用于连接不同的服务器套接字；同一个服务器端口可用于接受不同客户端套接字的连接）。对于不能确认的包、接收但还没读取的数据，都会占用操作系统的资源### 数据传输在TCP的数据传送状态，很多重要的机制保证了TCP的可靠性和强壮性。它们包括：使用序号，对收到的TCP报文段进行排序以及检测重复的数据；使用校验和检测报文段的错误，即无错传输[3]；使用确认和计时器来检测和纠正丢包或延时；流控制（Flow control）；拥塞控制（Congestion control）；丢失包的重传。<div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Tcp_transport_example.gif/525px-Tcp_transport_example.gif"></img></div><ol><li>发送方首先发送第一个包含序列号为1（可变化）和1460字节数据的TCP报文段给接收方。接收方以一个没有数据的TCP报文段来回复（只含报头），用确认号1461来表示已完全收到并请求下一个报文段。</li><li>发送方然后发送第二个包含序列号为1461，长度为1460字节的数据的TCP报文段给接收方。正常情况下，接收方以一个没有数据的TCP报文段来回复，用确认号2921（1461+1460）来表示已完全收到并请求下一个报文段。发送接收这样继续下去。</li><li>然而当这些数据包都是相连的情况下，接收方没有必要每一次都回应。比如，他收到第1到5条TCP报文段，只需回应第五条就行了。在例子中第3条TCP报文段被丢失了，所以尽管他收到了第4和5条，然而他只能回应第2条。</li><li>发送方在发送了第三条以后，没能收到回应，因此当时钟（timer）过时（expire）时，他重发第三条。（每次发送者发送一条TCP报文段后，都会再次启动一次时钟：RTT）。</li><li>这次第三条被成功接收，接收方可以直接确认第5条，因为4，5两条已收到。</li></ol><h3 id="校验和"><a href="#校验和" class="headerlink" title="校验和"></a>校验和</h3><p>TCP的16位的校验和（checksum）的计算和检验过程如下：发送者将TCP报文段的头部和数据部分的和计算出来，再对其求反码（一的补码），就得到了校验和，然后将结果装入报文中传输。（这里用反码和的原因是这种方法的循环进位使校验和可以在16位、32位、64位等情况下的计算结果再叠加后相同）接收者在收到报文后再按相同的算法计算一次校验和。这里使用的反码使得接收者不用再将校验和字段保存起来后清零，而可以直接将报文段连同校验加总。如果计算结果是全部为一，那么就表示了报文的完整性和正确性。</p><p>注意：TCP校验和也包括了96位的伪头部，其中有源地址、目的地址、协议以及TCP的长度。这可以避免报文被错误地路由。</p><p>按现在的标准，TCP的校验和是一个比较脆弱的校验。出错概率高的数据链路层需要更高的能力来探测和纠正连接错误。TCP如果是在今天设计的，它很可能有一个32位的CRC校验来纠错，而不是使用校验和。但是通过在第二层使用通常的CRC校验或更完全一点的校验可以部分地弥补这种脆弱的校验。第二层是在TCP层和IP层之下的，比如PPP或以太网，它们使用了这些校验。但是这也并不意味着TCP的16位校验和是冗余的，对于因特网传输的观察，表明在受CRC校验保护的各跳之间，软件和硬件的错误通常也会在报文中引入错误，而端到端的TCP校验能够捕捉到大部分简单的错误。这就是应用中的端到端原则。</p><h3 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h3><p>流量控制用来避免主机分组发送得过快而使接收方来不及完全收下，一般由接收方通告给发送方进行调控。</p><p>TCP使用滑动窗口协议实现流量控制。接收方在“接收窗口”域指出还可接收的字节数量。发送方在没有新的确认包的情况下至多发送“接收窗口”允许的字节数量。接收方可修改“接收窗口”的值。</p><p>TCP包的序号与接收窗口的行为很像时钟。<br>当接收方宣布接收窗口的值为0，发送方停止进一步发送数据，开始了“保持定时器”（persist timer），以避免因随后的修改接收窗口的数据包丢失使连接的双侧进入死锁，发送方无法发出数据直至收到接收方修改窗口的指示。当“保持定时器”到期时，TCP发送方尝试恢复发送一个小的ZWP包（Zero Window Probe），期待接收方回复一个带着新的接收窗口大小的确认包。一般ZWP包会设置成3次，如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。</p><p>如果接收方以很小的增量来处理到来的数据，它会发布一系列小的接收窗口。这被称作愚蠢窗口综合症，因为它在TCP的数据包中发送很少的一些字节，相对于TCP包头是很大的开销。解决这个问题，就要避免对小的window size做出响应，直到有足够大的window size再响应：</p><ul><li>接收端使用David D Clark算法：如果收到的数据导致window size小于某个值，可以直接ack把window给关闭了，阻止了发送端再发数据。等到接收端处理了一些数据后windows size大于等于了MSS，或者接收端buffer有一半为空，就可以把window打开让发送端再发数据过来。</li><li>发送端使用Nagle算法来延时处理，条件一：Window Size&gt;=MSS 或是 Data Size &gt;=MSS；条件二：等待时间或是超时200ms，这两个条件有一个满足，才会发数据，否则就是在积累数据。Nagle算法默认是打开的，所以对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性程序，需要关闭这个算法。可以在Socket设置TCP_NODELAY选项来关闭这个算法。</li></ul><h3 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h3><p>拥塞控制是发送方根据网络的承载情况控制分组的发送量，以获取高性能又能避免拥塞崩溃（congestion collapse，网络性能下降几个数量级）。这在网络流之间产生近似最大最小公平分配。</p><p>发送方与接收方根据确认包或者包丢失的情况，以及定时器，估计网络拥塞情况，从而修改数据流的行为，这称为拥塞控制或网络拥塞避免。</p><p>TCP的现代实现包含四种相互影响的拥塞控制算法：慢开始、拥塞避免、快速重传、快速恢复。</p><p>此外，发送方采取“超时重传”（retransmission timeout，RTO），这是估计出来回通信延迟 (RTT) 以及RTT的方差。</p><h3 id="终结通路"><a href="#终结通路" class="headerlink" title="终结通路"></a>终结通路</h3><p>连接终止使用了四路握手过程（或称四次握手，four-way handshake），在这个过程中连接的每一侧都独立地被终止。当一个端点要停止它这一侧的连接，就向对侧发送FIN，对侧回复ACK表示确认。因此，拆掉一侧的连接过程需要一对FIN和ACK，分别由两侧端点发出。</p><div align='center'><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Deconnection_TCP.png/330px-Deconnection_TCP.png"></img></div><div align='center'>TCP的终止连接</div><p>首先发出FIN的一侧，如果给对侧的FIN响应了ACK，那么就会超时等待2*MSL时间，然后关闭连接。在这段超时等待时间内，本地的端口不能被新连接使用；避免延时的包的到达与随后的新连接相混淆。RFC793定义了MSL为2分钟，Linux设置成了30s。参数tcp_max_tw_buckets控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的TIME_WAIT状态的连接给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow）</p><p>连接可以工作在TCP半开状态。即一侧关闭了连接，不再发送数据；但另一侧没有关闭连接，仍可以发送数据。已关闭的一侧仍然应接收数据，直至对侧也关闭了连接。</p><p>也可以通过测三次握手关闭连接。主机A发出FIN，主机B回复FIN &amp; ACK，然后主机A回复ACK.</p><p>一些主机（如Linux或HP-UX）的TCP栈能实现半双工关闭序列。这种主机如果主动关闭一个连接但还没有读完从这个连接已经收到的数据，该主机发送RST代替FIN。这使得一个TCP应用程序能确认远程应用程序已经读了所有已发送数据，并等待远程侧发出的FIN。但是远程的TCP栈不能区分Connection Aborting RST与Data Loss RST，两种原因都会导致远程的TCP栈失去所有的收到数据。</p><p>一些应用协议使用TCP open/close handshaking，因为应用协议的TCP open/close handshaking可以发现主动关闭的RST问题。例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = connect(remote);</span><br><span class="line">send(s, data);</span><br><span class="line">close(s);</span><br></pre></td></tr></table></figure></p><h3 id="状态编码"><a href="#状态编码" class="headerlink" title="状态编码"></a>状态编码</h3><p>下表为TCP状态码列表，以S指代服务器，C指代客户端，S&amp;C表示两者，S/C表示两者之一：[15]</p><ul><li>LISTEN S<br>服务器等待从任意远程TCP端口的连接请求。侦听状态。</li><li>SYN-SENT C<br>客户在发送连接请求后等待匹配的连接请求。通过connect()函数向服务器发出一个同步（SYNC）信号后进入此状态。</li><li>SYN-RECEIVED S<br>服务器已经收到并发送同步（SYNC）信号之后等待确认（ACK）请求。</li><li>ESTABLISHED S&amp;C<br>服务器与客户的连接已经打开，收到的数据可以发送给用户。数据传输步骤的正常情况。此时连接两端是平等的。这称作全连接。</li><li>FIN-WAIT-1 S&amp;C<br>（服务器或客户）主动关闭端调用close（）函数发出FIN请求包，表示本方的数据发送全部结束，等待TCP连接另一端的ACK确认包或FIN&amp;ACK请求包。</li><li>FIN-WAIT-2 S&amp;C<br>主动关闭端在FIN-WAIT-1状态下收到ACK确认包，进入等待远程TCP的连接终止请求的半关闭状态。这时可以接收数据，但不再发送数据。</li><li>CLOSE-WAIT S&amp;C<br>被动关闭端接到FIN后，就发出ACK以回应FIN请求，并进入等待本地用户的连接终止请求的半关闭状态。这时可以发送数据，但不再接收数据。</li><li>CLOSING S&amp;C<br>在发出FIN后，又收到对方发来的FIN后，进入等待对方对己方的连接终止（FIN）的确认（ACK）的状态。少见。</li><li>LAST-ACK S&amp;C<br>被动关闭端全部数据发送完成之后，向主动关闭端发送FIN，进入等待确认包的状态。</li><li>TIME-WAIT S/C<br>主动关闭端接收到FIN后，就发送ACK包，等待足够时间以确保被动关闭端收到了终止请求的确认包。【按照RFC 793，一个连接可以在TIME-WAIT保证最大四分钟，即最大分段寿命（maximum segment lifetime）的2倍】</li><li>CLOSED S&amp;C<br>完全没有连接。</li></ul><h3 id="端口"><a href="#端口" class="headerlink" title="端口"></a>端口</h3><p>TCP使用了通信端口（Port number）的概念来标识发送方和接收方的应用层。对每个TCP连接的一端都有一个相关的16位的无符号端口号分配给它们。端口被分为三类：众所周知的、注册的和动态/私有的。众所周知的端口号是由因特网赋号管理局（IANA）来分配的，并且通常被用于系统一级或根进程。众所周知的应用程序作为服务器程序来运行，并被动地侦听经常使用这些端口的连接。例如：FTP、TELNET、SMTP、HTTP等。注册的端口号通常被用来作为终端用户连接服务器时短暂地使用的源端口号，但它们也可以用来标识已被第三方注册了的、被命名的服务。动态/私有的端口号在任何特定的TCP连接外不具有任何意义。可能的、被正式承认的端口号有65535个。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;传输层&quot;&gt;&lt;a href=&quot;#传输层&quot; class=&quot;headerlink&quot; title=&quot;传输层&quot;&gt;&lt;/a&gt;传输层&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;传输层（英语：Transport Layer）&lt;/strong&gt;在计算机网络中是互联网协议包与开放系统互连（OSI）
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>应用层</title>
    <link href="https://shyshy903.github.io/2020/11/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/"/>
    <id>https://shyshy903.github.io/2020/11/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%BA%94%E7%94%A8%E5%B1%82/</id>
    <published>2020-11-25T16:00:00.000Z</published>
    <updated>2020-11-25T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h1><p>应用层（英语：Application layer）位于OSI模型的第七层。应用层直接和应用程序接口结合，并提供常见的网络应用服务。应用层也向第六层表示层发出请求。</p><h2 id="域名系统（DNS"><a href="#域名系统（DNS" class="headerlink" title="域名系统（DNS)"></a>域名系统（DNS)</h2><p>域名通俗来讲就是网络上每个地址的名称，全球唯一（难道有两个 www.baidu.com 吗）</p><div align='center'><img src="https://s1.ax1x.com/2018/11/24/FF7HFP.png#shadow"></img></div><h3 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h3><p>先输入nslookup查看本机 DNS，在输入要解析的域名www.baidu.com，返回该域名的 IP 地址</p><h2 id="DHCP服务器"><a href="#DHCP服务器" class="headerlink" title="DHCP服务器"></a>DHCP服务器</h2><p>动态主机配置协议 DHCP，负责给互联网上的计算机提供动态的 IP 地址</p><p>IP 地址获取方式有两种，一种静态 IP，一种动态 IP。静态 IP 是人工自己指定的，一般公司自己组建的局域网、学校机房的固定的计算机、机房服务器、互联网上的大型服务器，凡是位置固定不动的，都用静态 IP 地址。动态 IP 地址是用 DHCP 服务器来分配的地址，适用于计算机位置不固定、家庭拨号上网等情况。可以避免产生 IP 地址冲突。</p><p>DHCP 客户端请求 IP 地址的过程（逆 arp 协议）</p><p>需要地址的客户机先在网上发广播包请求地址，DHCP 服务器收到广播包后在自己的地址池里选一个地址（包括配套的子网掩码和网关），租给该客户机，该客户机再给 DHCP 服务器一个确认。（DHCP 服务器本身必须是静态地址！！！）</p><h2 id="FTP服务器"><a href="#FTP服务器" class="headerlink" title="FTP服务器"></a>FTP服务器</h2><p>FTP 连接方式：</p><ul><li>控制连接：标准端口为 21，用于发送 FTP 命令信息</li><li>数据连接：标准端口为 20，用于上传、下载数据</li><li><p>客户端选择数据连接的建立类型</p></li><li><p>主动模式：FTP 客户端告诉 FTP 服务器使用什么端口，FTP 服务器就主动用自己的 20 端口和 FTP 客户端的这个端口建立连接</p></li><li><p>被动模式：服务端在指定范围内打开一个新的端口，被动等待客户端发起连接<br>FTP 传输模式：</p></li><li><p>文本模式：ASCII 模式，以文本序列传输数据</p></li><li>二进制模式：Binary 模式，以二进制序列传输数据</li></ul><h2 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h2><ul><li>www：万维网简称，分布式超媒体系统。URL 标记这个大系统中的资源位置，HTTP 进行资源传输，HTML 用以显示页面，搜索引擎提供寻找资源的方法</li><li>url 的语法：协议名:// 主机域名或 ip 地址: 端口 / 资源的路径? 参数 1 = 值 1# 参数 2 = 值 2</li><li>作为服务器，要时刻监听 80 端口，一旦有访问，就建立一条 tcp 链接，在此基础上传输 http 协议的数据。解析协议数据，获取访问的 html 文档。由于采用了 tcp 请求，就有一个请求和确认，收到确认的过程。http 协议中，同样有一个请求链接时间，一个传输报文的时间。我们会设置一个超时最大的值，超过即放弃请求。</li><li>代理服务器：即高速缓存，解决了路由器 ip 地址被自己的客户机使用崩溃的问题。</li><li>HTTP 请求报文中的请求方法：get读取信息；POST给服务器添加信息；option请求一些选项的信息；head请求 html 页面里的 headers 标签；put在指定的 url 下存一些文档；DELETE删除 url 对应的文件；connect用于代理服务器；trace环回测试的请求报文</li><li>Cookie 是个服务器跟踪客户的工具</li><li>动态文档：动态文档是指在客户端请求到服务器时，服务器首先安排一个应用程序处理客户端的请求数据，并把处理后的数据封装成 http 报文里面，传输给客户端。这样，主要是有一些数据是实时可变的，不可能将数据放在静态的 html 里面，那样讲无法同步更新。但是这种动态指的是每次刷新都获取当前值，而不刷新的话，数据不会自己变化呈现。</li><li>活动 web 文档：服务器推送：在这种情况下，服务器和客户端之间建立 http 长链接，服务器不断地把数据推送给客户端。动画呈现，时间更新等等。太占带宽，链接不断，占用服务器资源，端口，带宽，时间资源。 活动文档：服务器直接把一个活动的程序，脚本之类的返回给客户端，其实链接已经断开了，但是程序运行呈现在页面上，看起来是连续不断的数据</li><li>搜索技术</li><li>加密技术 https：http 明文在网上传播不安全，对于有安全需求的消息，服务器和客户端之间首先建立连接，之后，服务器给客户端提供一个加密算法，一把秘钥，客户端程序使用算法和秘钥对数据加密，传输给服务器，服务器再用另一把不同的秘钥将数据解密出来</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;应用层&quot;&gt;&lt;a href=&quot;#应用层&quot; class=&quot;headerlink&quot; title=&quot;应用层&quot;&gt;&lt;/a&gt;应用层&lt;/h1&gt;&lt;p&gt;应用层（英语：Application layer）位于OSI模型的第七层。应用层直接和应用程序接口结合，并提供常见的网络应用服务。应
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>网络层</title>
    <link href="https://shyshy903.github.io/2020/11/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
    <id>https://shyshy903.github.io/2020/11/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E5%B1%82/</id>
    <published>2020-10-31T16:00:00.000Z</published>
    <updated>2020-10-31T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h1><p>网络层（Network Layer）是OSI模型中的第三层（TCP/IP模型中的网际层），提供路由和寻址的功能，使两终端系统能够互连且决定最佳路径，并具有一定的拥塞控制和流量控制的能力。相当于发送邮件时需要地址一般重要。由于TCP/IP协议体系中的网络层功能由IP协议规定和实现，故又称IP层。</p><p>在网络层也能为主机之间提供无连接和有链接的服务。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/17/iz69CF.png#shadow"></img></div><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><h3 id="寻址"><a href="#寻址" class="headerlink" title="寻址"></a>寻址</h3><p>对网络层而言使用IP地址来唯一标识互联网上的设备，网络层依靠IP地址进行相互通信（类似于数据链路层的MAC地址），详细的编址方案参见IPv4和IPv6。</p><h3 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h3><p>在同一个网络中的内部通信并不需要网络层设备，仅仅靠数据链路层就可以完成相互通信，对于不同的网络之间相互通信则必须借助路由器等三层设备。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>在网络层中这些服务（无论是有链接还是无连接）都是提供主机到主机的服务，在传输层中提供的则是提供应用层进程之间的服务。</li><li>在至今为止的所有的主要计算机网络结构体系中（因特网、ATM、帧中继等），网络层提供了主机到主机无连接或者有连接服务，而不同时提供两种服务。仅提供无连接的的网络称为数据报网络(Datagram Network)，仅提供有连接的网络称为虚电路网络（Virtual-Circuit，VC）。</li></ul><h2 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h2><p>网际协议（英语：Internet Protocol，缩写：IP；也称互联网协议）是用于分组交换数据网络的一种协议。</p><p>IP是在TCP/IP协议族中网络层的主要协议，任务仅仅是根据源主机和目的主机的地址来传送数据。为此目的，IP定义了寻址方法和数据报的封装结构。第一个架构的主要版本为IPv4，目前仍然是广泛使用的互联网协议，尽管世界各地正在积极部署IPv6。</p><ul><li>IP （V4 V6）</li><li>IPX</li><li>X.25</li><li>RARP</li><li>ICMP（V4、V6）</li><li>IGMP</li><li>IPsec</li><li>RIP</li></ul><h2 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h2><ol><li>网络地址</li></ol><p>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p><ol><li><p>广播地址<br>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p></li><li><p>地址划分</p></li></ol><div align='center'><img src="https://s1.ax1x.com/2018/11/18/izbBMq.jpg#shadow"></img></div><ul><li>A 类地址以 0 开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255</li><li>B 类地址以 10 开头，前两个字节作为网络号，地址范围是：127.0.0.0~191.255.255.255</li><li>C 类地址以 110 开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255</li><li>D 类地址以 1110 开头，地址范围是 224.0.0.0~239.255.255.255，D 类地址作为组播地址（一对多的通信）</li><li>E 类地址以 1111 开头，地址范围是 240.0.0.0~255.255.255.255，E 类地址为保留地址，供以后使用</li></ul><p>注：只有 A、B、C 由网络号和主机号之分，D、E 没有划分网络号和主机号</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/izzIat.png#shadow"></img></div><h2 id="特殊地址"><a href="#特殊地址" class="headerlink" title="特殊地址"></a>特殊地址</h2><ul><li>255.255.255.255</li></ul><p>该 IP 地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，首先广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的的地址的分组；一般广播地址既可以在本地广播，也可跨网段广播。例如：主机 192.168.1.1/30 直接广播数据包后，另一个网段 192.168.1.15/30 也能直接收到该数据包；若发送受限广播数据包则不能收到</p><p>注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址则不能通过路由器</p><ul><li><p>0.0.0.0<br>常用于寻找自己的 IP 地址，例如在 RARP，BOOTP 和 DHCP 协议中，若某个位置 IP 地址的无盘机想要知道自己的 IP 地址，他就以 255.255.255.255 为目的地址，像本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送 IP 请求分组</p></li><li><p>回环地址<br>127.0.0.0/8 被用作回环地址，会换地址表示本机的地址，常用于对本机的测试，用得最多的是 127.0.0.1</p></li><li><p>A、B、C 类私有地址<br>私有地址（Private Address）也叫专用地址，他们不会在全球使用，只具有本地意义</p></li></ul><ul><li>A 类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255</li><li>B 类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255</li><li>C 类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255</li></ul><h2 id="子网掩码及网络划分"><a href="#子网掩码及网络划分" class="headerlink" title="子网掩码及网络划分"></a>子网掩码及网络划分</h2><p>子网掩码是标志两个 IP 地址是否属于一个子网的，也是 32 位二进制地址，其每一个 1 代表该位是网络位，0 代表主机位。它和 IP 地址一样也是使用点式十进制来表示的。如果两个 IP 地址在子网掩码的按位与计算所得结果相同，即表明它们共属于同一个子网中。</p><p>注：在计算子网掩码时，要注意 IP 地址中的保留地址，即 “0” 地址和广播地址，他们是指主机地址或网络全为 “0” 或 “1” 时的 IP 地址，他们代表着本网络地址和广播地址，一般是不能被计算在内的。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSCYw9.png#shadow"></img></div><div align='center'>利用子网掩码解析IP地址</div><ul><li>求子网掩码前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数</li></ul><p>（1）将子网数转化为二进制表示：如欲将 B 类 IP 地址 168.195.0.0 划分成 27 个子网：27=11011;</p><p>（2）取得该二进制的位数，为 N：该二进制为 5 位数，N=5</p><p>（3）取得该 IP 地址的类子网掩码，将其主机地址部分的前 N 位置 1 即得出该 IP 地址划分子网的子网掩码</p><p>该 B 类地址的子网掩码 255.255.0.0 的主机地址前 5 位置 1，得到 255.255.248.0.</p><ul><li>利用主机数来计算<br>如欲将 B 类 IP 地址 168.195.0.0 划分成若干子网，每个子网内由主机 700 台：</li></ul><p>（1）将主机数转化为二进制表示：700=1010111100;</p><p>（2）如果主机数小于或等于 254，则取得该主机的二进制位数，为 N。如果大于 254，也就是说主机地址将占据不止 8 位，该二进制为十位数，N=10</p><p>（3）使用 255.255.255.255 来将该类 IP 地址的主机地址位数全部置 1，然后从后向前将 N 位全部置 0，即为子网掩码值。将该 B 类地址的子网掩码 255.255.0.0 的主机地址全部置 1，得到 255.255.255.255，然后再从后向前将十位置 0，即为：11111111.11111111.11111100.00000000，即 255.255.252.0. 这就是将划分成主机为 700 台 B 类 IP 地址 168.195.0.0 的子网掩码。</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSCdW6.png#shadow"></img></div>同一个网段的中的计算机子网掩码相同，计算机的网关就就是到其他网段的出口，也就是路由器接口地址。路由器接口使用的地址可以是本网段中任何一个地址，不过通常使用该网段的第一个可用的地址或最后一个可用的地址，这是为了尽可能避免和网络中的计算机地址冲突<div align='center'><img src="https://s1.ax1x.com/2018/11/18/izvjSg.jpg#shadow"></img></div><p>结合上面内容及 IP 地址特点，我们应该注意到：</p><ul><li>在同一个局域网上的主机或路由器的 IP 地址中的网络号必须是一样的。图中的网络号就是 IP 地址中的 网络号字段的值</li><li>路由器总是具有两个或两个以上的 IP 地址。路由器的每一个接口都有一个不同网络号的 IP 地址</li><li>两个路由器直接相连的接口处，可指明也可不指明 IP 地址。如指明 IP 地址，则这一段连线就构成了一种只包含一段线路的特殊 “网络”（如图中 N1、N2、N3）。之所以称之为网络，因为他们有 IP 地址、但为了节省 IP 地址，对于这种仅由一段连线构成的特殊 “网络”，现在常不指明 IP 地址。称为无编号网络或者无名网络</li><li>用网桥（它只在数据链路层工作）互连的网段仍然是一个局域网，只能有一个网络号</li></ul><h3 id="划分子网"><a href="#划分子网" class="headerlink" title="划分子网"></a>划分子网</h3><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSehpd.jpg#shadow"></img></div><p>现在把上图的网络划分为三个子网（下图）。这里假定子网号占用 8 位，因此在增加了子网号后，主机号只有 8 位。所划分的三个子网分别是：145.13.3.0、145.13.7.0 和 145.13.21.0。在划分完子网后，整个网络对外部仍然表现为一个网络，其网络地址仍为 145.13.0.0。网络 145.13.0.0 上的路由器 R1 在收到外来的数据报后，再根据数据报的目的地址把它转发到相应的子网</p><div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSuhp4.jpg#shadow"></img></div><h3 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h3><ul><li>默认子网掩码<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSQv3F.jpg#shadow"></img></div><div align='center'>默认子网掩码</div></li><li>获取子网地址<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSKejs.jpg#shadow"></img></div><div align='center'>获取子网地址</div></li><li>判断弟子所属网段<div align='center'><img src="https://s1.ax1x.com/2018/11/18/FSNDc6.png#shadow"></img></div><div align='center'>判断子网地址网段</div></li></ul><h2 id="路由转发算法"><a href="#路由转发算法" class="headerlink" title="路由转发算法"></a>路由转发算法</h2><p>当划分子网后，路由表必须包含以下三项内容：目的网络地址、子网掩码和下一跳地址。在划分分组情况下，路由器转发分组的算法如下：</p><ol><li>从收到的分组的首部提取目的 IP 地址 D</li><li>先用各网络的子网掩码和 D 逐位相 “与”，看是否和相应的网络地址匹配。若匹配，则将分组直接交付。否则就是间接交付，执行 3</li><li>若路由表中有目的地址为 D 的特定主机路由，则将 分组传送给指明的下一跳路由器；否则，执行 4</li><li>对路由表中的每一行的子网掩码和 D 逐位相 “与”，若其结果与该行的目的网络地址匹配，则将分组传送 给该行指明的下一跳路由器；否则，执行 5</li><li>若路由表中有一个默认路由，则将分组传送给路由表中所指明的默认路由器；否则，执行 6</li><li>报告转发分组出错</li></ol><h2 id="网络层的功能设备"><a href="#网络层的功能设备" class="headerlink" title="网络层的功能设备"></a>网络层的功能设备</h2><ul><li>路由器</li><li>三层交换机</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;网络层&quot;&gt;&lt;a href=&quot;#网络层&quot; class=&quot;headerlink&quot; title=&quot;网络层&quot;&gt;&lt;/a&gt;网络层&lt;/h1&gt;&lt;p&gt;网络层（Network Layer）是OSI模型中的第三层（TCP/IP模型中的网际层），提供路由和寻址的功能，使两终端系统能够互连
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>物理层</title>
    <link href="https://shyshy903.github.io/2020/10/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%A9%E7%90%86%E5%B1%82/"/>
    <id>https://shyshy903.github.io/2020/10/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%A9%E7%90%86%E5%B1%82/</id>
    <published>2020-10-20T16:00:00.000Z</published>
    <updated>2020-10-20T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h1><p>物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体</p><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf15d2286ea.png#shadow"></img></div><h2 id="局域网通信模型"><a href="#局域网通信模型" class="headerlink" title="局域网通信模型"></a>局域网通信模型</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf149f057e5.png#shadow"></img></div><h2 id="广域网通信模型"><a href="#广域网通信模型" class="headerlink" title="广域网通信模型"></a>广域网通信模型</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf15fc5d58d.png#shadow"></img></div><h2 id="模拟信号与数字信号"><a href="#模拟信号与数字信号" class="headerlink" title="模拟信号与数字信号"></a>模拟信号与数字信号</h2><div align='center'><img src="https://i.loli.net/2018/10/23/5bcf167d7ac2c.png#shadow"></img></div>模拟信号与数字信号转换<div align='center'><img src="https://i.loli.net/2018/10/23/5bcf17ca2d7d7.png#shadow"></img></div><h2 id="信道"><a href="#信道" class="headerlink" title="信道"></a>信道</h2><p>信道（Channel）是信息传输的通道，即信息进行传输时所经过的一条通路，信道的一端是发送端，另一端是接收端。一条传输介质上可以有多条信道（多路复用）</p><ul><li>单向信道<br>又称为单工通信，即信号只能向一个方向传输，任何时候都不能改变信号的传送方向。无线电广播或有线电视广播就是单工通信，信号只能是广播电台发送，收音机接收。</li><li>双向交替信道<br>又称半双工通信，信号可以双向传送，但是必须是交替进行，一个时间只能向一个方向传。有些对讲机就是用半双工通信，A 端说话 B 端接听，B 端说话 A 端接听，不能同时说和听。</li><li>双向同时信道<br>又称全双工通信，即信号可以同时双向传送。比如我们手机打电话，听和说可以同时进行。</li></ul><h2 id="奈氏准则"><a href="#奈氏准则" class="headerlink" title="奈氏准则"></a>奈氏准则</h2><p>在任何信道中，码元传输的速率是有上限的，否则就会出现码间串扰的问题，使接收端对码元的判决（即识别）成为不可能。举个例子，假如把一个人说话的声音录下来，然后进行倍速播放，当倍速达到一定程度时就听不清楚了。</p><p>如果信道的频带越宽，也就是能够通过的信号高频分量越多，那么就可以使用更高速率传递码元而不出现码间串扰</p><p>理想低通信道的最高码元传输速率 = 2WBaud</p><p>W 是理想低通信道的带宽，单位为 HZ<br>Baud 是波特，是码元传输速率的单位<br>使用奈氏准则给出的公式，可以根据信道的带宽，计算出码元的最高传输速率</p><h2 id="信道复用技术"><a href="#信道复用技术" class="headerlink" title="信道复用技术"></a>信道复用技术</h2><h3 id="频分复用"><a href="#频分复用" class="headerlink" title="频分复用"></a>频分复用</h3><div align='center'><img src="https://i.loli.net/2018/10/24/5bcffbba16928.png#shadow"></img></div>### 时分复用时分复用采用同一物理连接的不同时段来传输不同的信号，将时间划分为一段段等长的时分复用帧（TDM 帧）<div align='center'><img src="https://i.loli.net/2018/10/24/5bcffc6f83469.png#shadow"></img></div><h3 id="波分复用"><a href="#波分复用" class="headerlink" title="波分复用"></a>波分复用</h3><p>提高光纤的传输信号的速率，也可以进行频分复用，由于光载波的频率很高，因此习惯上用波长而不用频率来表示所使用的光载波。这样就得出了波分复用这一名词</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;物理层&quot;&gt;&lt;a href=&quot;#物理层&quot; class=&quot;headerlink&quot; title=&quot;物理层&quot;&gt;&lt;/a&gt;物理层&lt;/h1&gt;&lt;p&gt;物理层解决如何在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体&lt;/p&gt;
&lt;div align=&#39;center&#39;&gt;

      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>数据链路层</title>
    <link href="https://shyshy903.github.io/2020/10/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
    <id>https://shyshy903.github.io/2020/10/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</id>
    <published>2020-10-10T16:00:00.000Z</published>
    <updated>2020-10-10T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h1><p>数据链路层（Data Link Layer）是OSI参考模型第二层，位于物理层与网络层之间。在广播式多路访问链路中（局域网），由于可能存在介质争用，它还可以细分成介质访问控制（MAC）子层和逻辑链路控制（LLC）子层，介质访问控制（MAC）子层专职处理介质访问的争用与冲突问题。</p><p>主要功能为在两个网络实体之间提供数据链路连接的创建、维持和释放管理。构成数据链路数据单元（frame：数据帧或帧），并对帧定界、同步、收发顺序的控制。传输过程中的网络流量控制、差错检测和差错控制等方面。</p><p>局域网与广域网皆属第一、二层。</p><h2 id="数据链路与帧"><a href="#数据链路与帧" class="headerlink" title="数据链路与帧"></a>数据链路与帧</h2><p><strong>链路（Link)</strong>是指的从一个节点到相邻节点的一段物理线路（有线或无线），而中间没有任何其他的交换节点。</p><p><strong>数据链路（Data Link）</strong>则是另一个概念，这是因为当需要在一条线路上传送数据时，除了必须有一条物理线路外，还必须有一些必要的通信协议来控制这些数据的传输。</p><p>数据链路层把网络层交下来的数据封装成帧发送到链路上，以及把接收到的帧中的数据取出并上交给网络层。在因特网中，网络层协议数据单元就是 IP 数据报（或简称为数据报、分组或包）。数据链路层封装的帧，在物理层变成数字信号在链路上传输。</p><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66b985a2b3.png#shadow"></img></div><h2 id="封装成帧"><a href="#封装成帧" class="headerlink" title="封装成帧"></a>封装成帧</h2><p>封装成帧，将网络层的 IP 数据报的前后分别添加首部和尾部，就构成了一个帧。</p><p>每一种数据链路层协议都规定了所能够传送的帧的数据部分长度的上限 — 即最大传输单元 MTU（Maximum Transfer Unit），以太网的 MTU 为 1500 个字节。</p><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66fbf448ae.png#shadow"></img></div><h2 id="差错控制"><a href="#差错控制" class="headerlink" title="差错控制"></a>差错控制</h2><p>比特在传输过程中可能会产生差错：1 可能会变成 0，而 0 也可能变成 1，这就叫做比特差错。</p><p>为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施。目前在数据链路层广泛使用了循环冗余检验 CRC(Cyclic Redundancy Check）的差错检验技术。</p><h2 id="PPP协议"><a href="#PPP协议" class="headerlink" title="PPP协议"></a>PPP协议</h2><p>有3个组成部分</p><div align='center'><img src="https://s1.ax1x.com/2018/10/30/i2OjUA.png#shadow"></img></div><ul><li>异步传输使用字节填充<br>在异步传输的链路上，数据传输以字节为单位，PPP 帧的转义符定义为 0x7D，并使用字节填充</li></ul><p>把信息字段中出现的每一个 0x7E 字节转变成为 2 字节序列（0x7D，0x5E）</p><p>若信息字段中出现一个 0x7D 的字节（即出现了和转义字符一样的比特组合），则把 0x7D 转变成为 2 字节序列（0x7D，0x5D）</p><ul><li>同步使用零比特填充<br>在同步传输的链路上，数据传输以帧为单位，PPP 协议采用零比特填充方法来实现透明传输。如果把 PPP 协议帧界定符 0x7E 写成二进制 01111110，可以看到中间有连续的 6 个 1, 只要想办法在数据部分不要出现连续的 6 个 1, 就肯定不会出现这界定符。具体办法就是 “零比特填充法”。</li></ul><h2 id="以太网帧格式"><a href="#以太网帧格式" class="headerlink" title="以太网帧格式"></a>以太网帧格式</h2><p>常用的以太网 MAC 帧格式有两种标准，一种是 EthernetV2 标准（即以太网 V2 标准），另一种是 IEEE 的 802.3 标准。使用得最多的是以太网 V2 的 MAC 帧格式。</p><p>信道利用率</p><p>设帧长为 $L(bit)$，数据发送率为 $C(bit/s)$，所以帧的发送时间为$ L/C=T_0(s)$<br>利用率是指的发送数据的时间占整个时间的比例。如下图所示，平均发送一帧所需要的时间经历了 n 倍争用期 $2\tau，T_0 $为发送该帧所需时间，$\tau$ 为该帧传播时延。</p><p>有冲突时信道利用率为</p><script type="math/tex; mode=display">S=\frac{T_0}{n2\tau +T_0+\tau}</script><p>从公式可以看出，要想提高信道利用率最好是 n=0，这就意味着以太网上的各个计算机发送数据不会产生碰撞（这显然已经不是 CSMA/CD，而需要一种特殊的调度方法），并且能够非常有效的利用网络的传输资源，即总线一旦空闲就有一个站立即发送数据。这种情况算出来的信道利用率是极限信道利用率。</p><script type="math/tex; mode=display">S_{max}=\frac{T_0}{T_0+\tau}=\frac{1}{1+\frac{\tau}{T_0}}</script><p>要想提高极限信道利用率就要降低公式中的 $\frac{\tau}{T_0}$<br>降低上面的分式有两种办法，分子足够小或者分母足够大，首先说分子，$\tau $值和以太网连线的长度有关，这就意味着$ \tau $值要小，以太网网线的长度就不能太长。其次是分母，带宽一定的情况下 $T_0 $和帧的长度有关，这就意味着，以太网的帧不能太短。</p><h2 id="MAC地址"><a href="#MAC地址" class="headerlink" title="MAC地址"></a>MAC地址</h2><p>这种 6 字节的 MAC 地址已被固化在网卡的 ROM 中。因此，MAC 地址也叫作硬件地址（hardware address）或物理地址。当这块网卡插入（或嵌入）到某台计算机后，网卡上的 MAC 地址就成为这台计算机的 MAC 地址了。</p><ul><li>单播（unicast）帧（一对一），即收到的帧的 MAC 地址与本站的硬件地址相同</li><li>广播（broadcast）帧（一对全体），即发送给本局域网上所有站点的帧（全 1 地址）</li><li>多播（multicast）帧（一对多），即发送给本局域网上一部分站点的帧</li></ul><h2 id="数据链路层的设备"><a href="#数据链路层的设备" class="headerlink" title="数据链路层的设备"></a>数据链路层的设备</h2><ul><li>交换机是本层设备。而集线器是物理层设备，不是数据链路层设备。</li><li>桥接器</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据链路层&quot;&gt;&lt;a href=&quot;#数据链路层&quot; class=&quot;headerlink&quot; title=&quot;数据链路层&quot;&gt;&lt;/a&gt;数据链路层&lt;/h1&gt;&lt;p&gt;数据链路层（Data Link Layer）是OSI参考模型第二层，位于物理层与网络层之间。在广播式多路访问链路中（
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络基础</title>
    <link href="https://shyshy903.github.io/2020/10/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>https://shyshy903.github.io/2020/10/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</id>
    <published>2020-09-30T16:00:00.000Z</published>
    <updated>2020-09-30T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算机网络基础"><a href="#计算机网络基础" class="headerlink" title="计算机网络基础"></a>计算机网络基础</h1><div align='center'><img src="https://i.loli.net/2018/10/29/5bd66b001571e.png#shadow"></img></div><h2 id="网络层次划分"><a href="#网络层次划分" class="headerlink" title="网络层次划分"></a>网络层次划分</h2><p>为了使不同计算机厂家生产的计算机能相互通信，在更大范围内建立计算机网络，国际标准化组织（ISO）在 1978 年提出了 “开放系统互联参考模型”，即著名的 OSI/RM 模型（Open System Interconnection/Reference Model）。它将计算机网络体系结构的通信协议划分为七层，自下而上依次为：<strong>物理层（Physics Layer）、数据链路层（Data Link Layer）、网络层（Network Layer）、传输层（Transport Layer）、会话层（Session Layer）、表示层（Presentation Layer）、应用层（Application Layer）</strong>。</p><div align='center'><img src="https://i.loli.net/2018/10/01/5bb234b6321ea.jpg#shadow"></img></div><h2 id="IP地址"><a href="#IP地址" class="headerlink" title="IP地址"></a>IP地址</h2><ol><li>网络地址</li></ol><p>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p><ol><li><p>广播地址<br>IP 地址由网络号（包括子网号）和主机号组成，网络地址的主机号全为 0，网络地址代表着整个网络。</p></li><li><p>地址划分</p></li></ol><ul><li>A 类地址以 0 开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255</li><li>B 类地址以 10 开头，前两个字节作为网络号，地址范围是：127.0.0.0~191.255.255.255</li><li>C 类地址以 110 开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255</li><li>D 类地址以 1110 开头，地址范围是 224.0.0.0~239.255.255.255，D 类地址作为组播地址（一对多的通信）</li><li>E 类地址以 1111 开头，地址范围是 240.0.0.0~255.255.255.255，E 类地址为保留地址，供以后使用</li></ul><p>注：只有 A、B、C 由网络号和主机号之分，D、E 没有划分网络号和主机号</p><h2 id="特殊地址"><a href="#特殊地址" class="headerlink" title="特殊地址"></a>特殊地址</h2><ul><li>255.255.255.255</li></ul><p>该 IP 地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，首先广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的的地址的分组；一般广播地址既可以在本地广播，也可跨网段广播。例如：主机 192.168.1.1/30 直接广播数据包后，另一个网段 192.168.1.15/30 也能直接收到该数据包；若发送受限广播数据包则不能收到</p><p>注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址则不能通过路由器</p><ul><li><p>0.0.0.0<br>常用于寻找自己的 IP 地址，例如在 RARP，BOOTP 和 DHCP 协议中，若某个位置 IP 地址的无盘机想要知道自己的 IP 地址，他就以 255.255.255.255 为目的地址，像本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送 IP 请求分组</p></li><li><p>回环地址<br>127.0.0.0/8 被用作回环地址，会换地址表示本机的地址，常用于对本机的测试，用得最多的是 127.0.0.1</p></li><li><p>A、B、C 类私有地址<br>私有地址（Private Address）也叫专用地址，他们不会在全球使用，只具有本地意义</p></li></ul><ul><li>A 类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255</li><li>B 类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255</li><li>C 类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255</li></ul><h2 id="子网掩码及网络划分"><a href="#子网掩码及网络划分" class="headerlink" title="子网掩码及网络划分"></a>子网掩码及网络划分</h2><p>子网掩码是标志两个 IP 地址是否属于一个子网的，也是 32 位二进制地址，其每一个 1 代表该位是网络位，0 代表主机位。它和 IP 地址一样也是使用点式十进制来表示的。如果两个 IP 地址在子网掩码的按位与计算所得结果相同，即表明它们共属于同一个子网中。</p><p>注：在计算子网掩码时，要注意 IP 地址中的保留地址，即 “0” 地址和广播地址，他们是指主机地址或网络全为 “0” 或 “1” 时的 IP 地址，他们代表着本网络地址和广播地址，一般是不能被计算在内的。</p><ul><li>求子网掩码前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数</li></ul><p>（1）将子网数转化为二进制表示：如欲将 B 类 IP 地址 168.195.0.0 划分成 27 个子网：27=11011;</p><p>（2）取得该二进制的位数，为 N：该二进制为 5 位数，N=5</p><p>（3）取得该 IP 地址的类子网掩码，将其主机地址部分的前 N 位置 1 即得出该 IP 地址划分子网的子网掩码</p><p>该 B 类地址的子网掩码 255.255.0.0 的主机地址前 5 位置 1，得到 255.255.248.0.</p><ul><li>利用主机数来计算<br>如欲将 B 类 IP 地址 168.195.0.0 划分成若干子网，每个子网内由主机 700 台：</li></ul><p>（1）将主机数转化为二进制表示：700=1010111100;</p><p>（2）如果主机数小于或等于 254，则取得该主机的二进制位数，为 N。如果大于 254，也就是说主机地址将占据不止 8 位，该二进制为十位数，N=10</p><p>（3）使用 255.255.255.255 来将该类 IP 地址的主机地址位数全部置 1，然后从后向前将 N 位全部置 0，即为子网掩码值。将该 B 类地址的子网掩码 255.255.0.0 的主机地址全部置 1，得到 255.255.255.255，然后再从后向前将十位置 0，即为：11111111.11111111.11111100.00000000，即 255.255.252.0. 这就是将划分成主机为 700 台 B 类 IP 地址 168.195.0.0 的子网掩码。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;计算机网络基础&quot;&gt;&lt;a href=&quot;#计算机网络基础&quot; class=&quot;headerlink&quot; title=&quot;计算机网络基础&quot;&gt;&lt;/a&gt;计算机网络基础&lt;/h1&gt;&lt;div align=&#39;center&#39;&gt;
&lt;img src=&quot;https://i.loli.net/201
      
    
    </summary>
    
    
      <category term="计算机网络" scheme="https://shyshy903.github.io//categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="计网" scheme="https://shyshy903.github.io//tags/%E8%AE%A1%E7%BD%91/"/>
    
  </entry>
  
  <entry>
    <title>Python复习笔记</title>
    <link href="https://shyshy903.github.io/2020/07/23/python/shypython/"/>
    <id>https://shyshy903.github.io/2020/07/23/python/shypython/</id>
    <published>2020-07-22T16:00:00.000Z</published>
    <updated>2020-07-22T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Shypython-learn-notes"><a href="#Shypython-learn-notes" class="headerlink" title="Shypython-learn-notes"></a>Shypython-learn-notes</h1><h2 id="1-python-数据类型"><a href="#1-python-数据类型" class="headerlink" title="1. python 数据类型"></a>1. python 数据类型</h2><h3 id="1-1-变量"><a href="#1-1-变量" class="headerlink" title="1.1 变量"></a>1.1 变量</h3><p><strong>1.1.1 算术运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 加减乘除+、-、*、/</span><br><span class="line">- 取余、取整、取绝对值 %、//、abs()</span><br><span class="line">- 最小、最大值 min()、max()</span><br><span class="line">- 复数 complex(re,im)</span><br><span class="line">- 取共轭 c.conjugate()</span><br><span class="line">- 返回商和余数 divmod(x,y)</span><br></pre></td></tr></table></figure></p><p><strong>1.1.2 布尔运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-  小于、大于 &lt; 、 &gt;</span><br><span class="line">- 等于、不等于 == 、 != </span><br><span class="line">- 与、或、非 <span class="keyword">and</span> 、<span class="keyword">or</span> 、<span class="keyword">not</span></span><br></pre></td></tr></table></figure><br><strong>1.1.3 赋值运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- a=a+b  <span class="keyword">is</span>  a+=b</span><br><span class="line">- a=a-b  <span class="keyword">is</span>  a-=b</span><br><span class="line">- a=a*/b  <span class="keyword">is</span>  a*/b</span><br><span class="line">- a=a**(//)b  <span class="keyword">is</span>  a**(//)=b</span><br></pre></td></tr></table></figure></p><p><strong>1.1.4 位运算符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 与或 &amp; 、 |  </span><br><span class="line">- 异或、取反 ^ 、~ </span><br><span class="line">- 左位移、右位移  &lt;&lt;   、 &gt;&gt;</span><br></pre></td></tr></table></figure></p><p><strong>1.1.5 转义符</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">- 续行符 \</span><br><span class="line">- 反斜杠符号 \\</span><br><span class="line">- 引号 \<span class="string">'</span></span><br><span class="line"><span class="string">- 响铃 \a</span></span><br><span class="line"><span class="string">- 退格 \b</span></span><br><span class="line"><span class="string">- 转义 \e</span></span><br><span class="line"><span class="string">- 空 \000</span></span><br><span class="line"><span class="string">- 换行 \n</span></span><br><span class="line"><span class="string">- 纵向制表符 \v</span></span><br><span class="line"><span class="string">- 横向制表符 \t</span></span><br><span class="line"><span class="string">- 回车 \r</span></span><br><span class="line"><span class="string">- 换页 \f</span></span><br><span class="line"><span class="string">- 八进制 \oyy</span></span><br><span class="line"><span class="string">- 十六进制 \xyy</span></span><br></pre></td></tr></table></figure></p><h3 id="1-2-字符串-不可变类型"><a href="#1-2-字符串-不可变类型" class="headerlink" title="1.2 字符串(不可变类型)"></a>1.2 字符串(不可变类型)</h3><p><strong>1.2.1 切片操作</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当索引为正数时，从0开始，当索引为负数时，从-1开始（从右往左）</span></span><br><span class="line">- newstr = s[a:b:c]  从索引a开始到b ,每隔c取一个值，左开右闭</span><br><span class="line">- newstr = s[<span class="number">0</span>:]  </span><br><span class="line">- newstr = s[:]  和上面的式子等价</span><br><span class="line">- newstr = s[::<span class="number">-1</span>] 实现字符串的逆序</span><br></pre></td></tr></table></figure><br><strong>1.2.2 字符串运算及方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">'I LOVE PYTHON!!'</span></span><br><span class="line">- 标准化字符串 <span class="string">r'str'</span> 或者 repr(str)</span><br><span class="line">- x <span class="keyword">in</span> s 子字符串 </span><br><span class="line">- s1 + s2 字符串连接 </span><br><span class="line">- s*n 字符串副本的拼接 </span><br><span class="line">- s[i] 字符串索引 </span><br><span class="line">- str.index(<span class="string">'s'</span>) 获得字符串s字符的索引位置</span><br><span class="line">- len(s) 字符串长度 </span><br><span class="line">- ord(str) 字符串的编码 </span><br><span class="line">- chr(number) 返回某个编码得到的字符 </span><br><span class="line">- str.spilt(<span class="string">', '</span>) 字符串的分割 ,返回值是一个列表</span><br><span class="line">- chr.join(list)  字符串编码的连接 , <span class="string">" "</span>.join(list)</span><br><span class="line">str = <span class="string">"www.runoob.com"</span></span><br><span class="line">- print(str.upper())  把所有字符中的小写字母转换成大写字母</span><br><span class="line">- print(str.lower())  把所有字符中的大写字母转换成小写字母</span><br><span class="line">- print(str.capitalize()) 把第一个字母转化为大写字母，其余小写</span><br><span class="line">- print(str.title())  把每个单词的第一个字母转化为大写，其余小写</span><br></pre></td></tr></table></figure><br><strong>1.2.3深入研究字符串的方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str.find(x)  返回x的第一个字符出现的索引位置</span><br><span class="line">str.count(x)  返回x出现的次数</span><br><span class="line">str.replace(<span class="string">'top'</span>,<span class="string">'bot'</span>)  返回一个修改的副本</span><br><span class="line">str.spilt()</span><br><span class="line">tabel = str.maketrans(<span class="string">'xyz'</span>,<span class="string">'uvw'</span>) ; str.translate(table)  返回一个映射后的副本</span><br><span class="line">str.strip()  返回字符串的一个副本，并且消除前后空格</span><br></pre></td></tr></table></figure></p><h3 id="1-3-列表（可变类型）"><a href="#1-3-列表（可变类型）" class="headerlink" title="1.3 列表（可变类型）"></a>1.3 列表（可变类型）</h3><p><strong>1.3.1 list的内置方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">1</span>,<span class="number">3</span>,a,[<span class="number">4</span>,<span class="number">5</span>],<span class="number">6</span>]</span><br><span class="line">- list.append(x)  在尾部增加一个元素</span><br><span class="line">- list.insert(x,i)  在索引i处添加一个元素</span><br><span class="line">- list.index(x)  获得元素x的索引</span><br><span class="line">- list.remove(x)  删除列表的原色</span><br><span class="line">- list.pop(i)  弹出索引为i的元素并在列表中删除它</span><br><span class="line">- list.clear() 清楚列表</span><br><span class="line">- list.count(x) 返回列表x出现的次数</span><br><span class="line">- list.sort() 对列表直接排序， 区别于排序函数 sorted()</span><br><span class="line">- list.reverse() 对列表进行反转</span><br><span class="line">- len(list)</span><br><span class="line">- <span class="keyword">for</span> item <span class="keyword">in</span> list:   对列表的遍历</span><br></pre></td></tr></table></figure><br> <strong>1.3.2 列表和字符串的相互转化</strong><br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1.</span> str &gt;&gt;&gt;list </span><br><span class="line"></span><br><span class="line">str1 = <span class="string">"12345"</span></span><br><span class="line">list1 = list(str1)</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"> </span><br><span class="line">str2 = <span class="string">"123 sjhid dhi"</span></span><br><span class="line">list2 = str2.split() <span class="comment">#or list2 = str2.split(" ")</span></span><br><span class="line"><span class="keyword">print</span> list2</span><br><span class="line"> </span><br><span class="line">str3 = <span class="string">"www.google.com"</span></span><br><span class="line">list3 = str3.split(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">print</span> list3</span><br><span class="line"> </span><br><span class="line">输出为：</span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>, <span class="string">'5'</span>]</span><br><span class="line">[<span class="string">'123'</span>, <span class="string">'sjhid'</span>, <span class="string">'dhi'</span>]</span><br><span class="line">[<span class="string">'www'</span>, <span class="string">'google'</span>, <span class="string">'com'</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> list &gt;&gt;&gt;str</span><br><span class="line">str4 = <span class="string">""</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str4</span><br><span class="line">str5 = <span class="string">"."</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str5</span><br><span class="line">str6 = <span class="string">" "</span>.join(list3)</span><br><span class="line"><span class="keyword">print</span> str6</span><br><span class="line">输出为：</span><br><span class="line">wwwgooglecom</span><br><span class="line">www.google.com</span><br><span class="line">www google com</span><br></pre></td></tr></table></figure></p><h3 id="1-3-元组类型（不可变类型）"><a href="#1-3-元组类型（不可变类型）" class="headerlink" title="1.3 元组类型（不可变类型）"></a>1.3 元组类型（不可变类型）</h3><p><strong>1.3.1 元组的运算及操作</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 元组为不可修改的字符串</span></span><br><span class="line">tuple = (<span class="number">2019</span>,<span class="string">'a'</span>,(b,c),<span class="string">'science'</span>)</span><br></pre></td></tr></table></figure><br><strong>1.3.2 元组与列表的转换</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tuple = tuple(list)</span><br><span class="line">list  = list(touple)</span><br></pre></td></tr></table></figure></p><h3 id="1-4-集合类型（消除关系重复元素）"><a href="#1-4-集合类型（消除关系重复元素）" class="headerlink" title="1.4 集合类型（消除关系重复元素）"></a>1.4 集合类型（消除关系重复元素）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">myset = [<span class="string">'nature'</span>,<span class="string">'science'</span>]</span><br><span class="line">set.add(x)</span><br><span class="line">set.remove(X)</span><br><span class="line">set.discard(X)</span><br><span class="line">set.clear()</span><br><span class="line">set.pop()</span><br><span class="line">len(set)</span><br><span class="line"><span class="keyword">in</span> / <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">set.issubset(set2)  判断set是否是set2的子集，返回bool类型</span><br><span class="line">set.isuperset(set2)  </span><br><span class="line">set.union(set2)  计算并集</span><br><span class="line">set.intersection(set2)  计算交集</span><br><span class="line">set.difference(set2)  计算差集</span><br><span class="line">set.symmetric_difference(set2)  计算对称差集</span><br></pre></td></tr></table></figure><h3 id="1-5-字典类型（键值对）"><a href="#1-5-字典类型（键值对）" class="headerlink" title="1.5 字典类型（键值对）"></a>1.5 字典类型（键值对）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">'a'</span>:<span class="number">1</span>,<span class="string">'b'</span>:<span class="number">2</span>,<span class="string">'c'</span>:<span class="number">3</span>&#125;</span><br><span class="line">len(dict)</span><br><span class="line">str(dict)</span><br><span class="line">dict(<span class="string">'a'</span>)  访问字典中键为a的值</span><br><span class="line">dict.clear()</span><br><span class="line">dict.items() 以列表形式返回可遍历的（键，值）元素数组</span><br><span class="line">dict.keys() 以列表形式返回一个字典中的所有键</span><br><span class="line">dict.values() 以字典形式返回一个字典中的所有值</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">## 2. 语句类型</span></span><br><span class="line"><span class="comment">### 2.1 if 语句</span></span><br><span class="line">```python </span><br><span class="line"><span class="keyword">if</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">elif</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="2-2-while语句"><a href="#2-2-while语句" class="headerlink" title="2.2 while语句"></a>2.2 while语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> &lt;条件&gt;：</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 死循环</span></span><br><span class="line"><span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="2-3-for-语句"><a href="#2-3-for-语句" class="headerlink" title="2.3 for 语句"></a>2.3 for 语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在for循环中使用内置函数 range()</span></span><br><span class="line">range(a,b,c)  返回一个数字区间的所有整数</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的冒泡排序算法S</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(n)<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(n)-i<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> n[j] &gt; n[i]:</span><br><span class="line">            n[j], n[j+<span class="number">1</span>] = n[j+<span class="number">1</span>], n[j]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在for循环中使用内置函数zip()</span></span><br><span class="line">zip(x,y)  将多序列生成一个新的序列，每个序列的元素以元组形式存储数据</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t1,t2 <span class="keyword">in</span> zip(x,y):</span><br><span class="line">    print(t1,t2)</span><br></pre></td></tr></table></figure><h3 id="2-4-控制语句"><a href="#2-4-控制语句" class="headerlink" title="2.4 控制语句"></a>2.4 控制语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">break</span>  跳出循环</span><br><span class="line"><span class="keyword">continue</span>  终止当前一次循环、继续进行下一次循环</span><br><span class="line"><span class="keyword">pass</span> 什么都不做，保持结构完整性</span><br></pre></td></tr></table></figure><h2 id="3-格式化输入与输出"><a href="#3-格式化输入与输出" class="headerlink" title="3. 格式化输入与输出"></a>3. 格式化输入与输出</h2><h3 id="3-1-格式化输入"><a href="#3-1-格式化输入" class="headerlink" title="3.1 格式化输入"></a>3.1 格式化输入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = input(&lt;info&gt;)</span><br><span class="line">a = input(repr(str))</span><br></pre></td></tr></table></figure><h3 id="3-2-格式化输出"><a href="#3-2-格式化输出" class="headerlink" title="3.2 格式化输出"></a>3.2 格式化输出</h3><p><strong>3.2.1 print语句</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(a, b)   同行以空格隔开输出</span><br><span class="line">print(a, b,s ep=<span class="string">','</span>)  以逗号隔开进行输出</span><br><span class="line">print(s, ewp=<span class="string">'\n'</span>)   以换行隔开进行输出</span><br><span class="line">print(name, end=<span class="string">'!'</span>)  每个输出都要添加!</span><br><span class="line">print(<span class="string">' i love %s '</span> % s)  <span class="keyword">print</span> 格式化输出</span><br><span class="line"><span class="comment"># 对字符串的每个元素进行换行输出</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> list:</span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure><br><strong>3.2.2 字符串方法format()</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'&#123;0&#125; : &#123;1&#125; : &#123;2&#125;'</span>.format(hour, minute, second))</span><br><span class="line"><span class="comment"># 按照对齐格式化进行排列</span></span><br><span class="line">print(<span class="string">'&#123;0:3&#125;,&#123;1:5&#125;'</span>.format(<span class="number">12</span>, <span class="number">534</span>)) :后面的内容为只等的格式，表示占位数，如果不够，在前面用空格补齐</span><br></pre></td></tr></table></figure><br><strong>3.2.3 数据输出的格式类型</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a表示占位，不够，按照原长度打印，多了左侧空格补齐，小数点后面为保留几位小数，f位数据类型</span></span><br><span class="line">print(<span class="string">'i love %a.bf'</span>, num)</span><br><span class="line">- b 以二进制形式输出</span><br><span class="line">- c 输出证书值对应的unicode字符</span><br><span class="line">- d 以十进制形式输出数值</span><br><span class="line">- e 以科学计数法形式输出</span><br><span class="line">- o 以八进制形式输出</span><br><span class="line">- x 以小写形式的十六进制输出</span><br><span class="line">- X 以大写形式的十六进制输出</span><br></pre></td></tr></table></figure></p><h2 id="4-函数"><a href="#4-函数" class="headerlink" title="4. 函数"></a>4. 函数</h2><h3 id="4-1-函数定义及调用函数"><a href="#4-1-函数定义及调用函数" class="headerlink" title="4.1 函数定义及调用函数"></a>4.1 函数定义及调用函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- <span class="function"><span class="keyword">def</span> <span class="title">funname</span><span class="params">(para1, para2,*，para3...)</span>:</span></span><br><span class="line">    函数体</span><br><span class="line"><span class="comment"># 在参数列表中使用（*），代表调用函数时，在（*）后面的参数都必须指定参数名称，如下</span></span><br><span class="line">funname(para1, para2，para3=<span class="number">2.</span>..)   调用函数</span><br><span class="line">- <span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(strname, age=<span class="number">32</span>)</span>：   后面的参数位默认形参，默认形参必须放在后面</span></span><br><span class="line"><span class="function">- 对元组和列表进行解包</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">fun</span><span class="params">(*person)</span>:</span></span><br><span class="line">fun(<span class="string">'shy'</span>,<span class="string">'21'</span>)</span><br><span class="line">mylist = [<span class="string">'shy'</span>,<span class="string">'21'</span>]</span><br><span class="line">fun(*mylist)</span><br><span class="line">- 对字典进行解包定义参数及调用</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(**person)</span></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'姓名'</span>,person[<span class="string">'name'</span>],<span class="string">'年纪'</span>,person[<span class="string">'age'</span>])</span></span></span><br><span class="line"><span class="function"><span class="title">fun</span><span class="params">(<span class="string">'shy'</span>,<span class="string">'21'</span>)</span></span></span><br><span class="line">mydict = &#123;'name':'shy','age':21&#125;</span><br><span class="line">fun(**mydict)</span><br></pre></td></tr></table></figure><h3 id="4-2-函数类型"><a href="#4-2-函数类型" class="headerlink" title="4.2 函数类型"></a>4.2 函数类型</h3><p><strong>4.2.1 python内置函数</strong></p><p>下图python3.8官方文档给出的内置函数库：</p><div style="align: center"><img src="https://img.vim-cn.com/5f/bd7267b5d93389d33733905d3bb7216a43d7c1.png"/></div><p>官方中文文档的连接：<a href="https://docs.python.org/zh-cn/3/library/functions.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/functions.html</a></p><p><strong>4.2.2 匿名函数与可迭代函数</strong></p><ul><li><strong>匿名函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匿名函数</span></span><br><span class="line"><span class="keyword">lambda</span> para1, para2... : 表达式</span><br><span class="line">r = <span class="keyword">lambda</span> x,y:x*y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与reduce函数的组合应用</span></span><br><span class="line">reduce(fun, seq, initial)  <span class="comment">#用序列值依次调用fun</span></span><br><span class="line"><span class="keyword">from</span> funtools <span class="keyword">import</span> reduce</span><br><span class="line">a = reduce(<span class="keyword">lambda</span> x,y:x + y, range(<span class="number">1</span>,<span class="number">101</span>))  <span class="comment">#实现求1~100的和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与map函数的组合应用</span></span><br><span class="line">map(fun, seq[,seq,])  <span class="comment">#将seq内部的元素作为参数依次调用</span></span><br><span class="line">t = map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])  <span class="comment">#返回一个map对象</span></span><br><span class="line">print(list(t))   <span class="comment">#打印值为[1,4,9,16,25]</span></span><br><span class="line">y =map(<span class="keyword">lambda</span> x,y:x+y,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(list(t))   <span class="comment"># 打印值为[5,7,9]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 匿名函数与filter函数的组合应用</span></span><br><span class="line">filter(fun <span class="keyword">or</span> none, seq)  <span class="comment">#将序列对象依次放到fun中，如果返回true就留下</span></span><br><span class="line">t = filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(list(t))   <span class="comment"># 打印值为[2,4,6]</span></span><br></pre></td></tr></table></figure></li><li><strong>可迭代函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个生成器对象会有一个__next__()方法</span></span><br><span class="line">t = filter(<span class="keyword">lambda</span> x:x%<span class="number">2</span>==<span class="number">0</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">print(t.__next__())  <span class="comment"># 打印2</span></span><br><span class="line">print(t.__next__())  <span class="comment"># 打印4</span></span><br></pre></td></tr></table></figure><strong>4.2.3 生成器函数与工厂函数</strong></li><li><strong>生成器函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成器与迭代器不同，迭代器的内容存在内存里，用next函数遍历，生成器用完立即销毁</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Reverse</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(data)<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">yield</span> data[idx]   <span class="comment"># 生成器函数用yield返回</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> Reverse(<span class="string">'Python'</span>):</span><br><span class="line">    print(c, end = <span class="string">' '</span>)  <span class="comment"># 打印 n o h t y P</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器表达式</span></span><br><span class="line">mylist = [x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]  <span class="comment"># 使用生成器表达式返回一个对象</span></span><br></pre></td></tr></table></figure></li><li><strong>工厂函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 闭合函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">fun = wrapperfun('shy')</span><br><span class="line">fun(<span class="number">37</span>)    <span class="comment"># 打印 shy 37</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 装饰器属性：在原有的函数包一个函数，不改变原代码的基础上，添加新功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span>    <span class="comment"># 装饰器函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(strname)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(strname.(str))：</span><br><span class="line">            <span class="keyword">return</span> fn(strname)  <span class="comment"># 判断字符串类型</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'error'</span>)  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line">wrapperfun2 = checkParams(wrapperfun)</span><br><span class="line">fun = wrapperfun(<span class="string">'shy'</span>)</span><br><span class="line">fun(<span class="number">37</span>)  <span class="comment"># 打印 shy 37</span></span><br><span class="line">fun = wrapperfun2(<span class="number">37</span>)  <span class="comment"># 输入不合法</span></span><br></pre></td></tr></table></figure></li><li><strong>@修饰符</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkParams</span><span class="params">(fn)</span>:</span>    <span class="comment"># 装饰器函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(strname)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(strname.(str))：</span><br><span class="line">            <span class="keyword">return</span> fn(strname)  <span class="comment"># 判断字符串类型</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'error'</span>)  </span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@checkParams   # 使用装饰器函数对wrapperfun函数进行修饰</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wrapperfun</span><span class="params">(strname)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recorder</span><span class="params">(age)</span>：</span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(strname,age)</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">recorder</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">fun = wrapperfun('shy')</span><br><span class="line">fun(<span class="number">37</span>)  <span class="comment"># 打印 shy 37</span></span><br><span class="line">fun = wrapperfun2(<span class="number">37</span>)  <span class="comment"># 输入不合法</span></span><br></pre></td></tr></table></figure><strong>4.4.4 偏函数与递归函数</strong></li><li><strong>偏函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 偏函数是重新定义一个函数，并指定了默认参数值</span></span><br><span class="line"><span class="keyword">from</span> funtools <span class="keyword">import</span> partial</span><br><span class="line">partial(fun, *args, **keywords)</span><br></pre></td></tr></table></figure></li><li><strong>递归函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归函数是自己调用自己的函数，所有的函数调用都是压栈的过程，所以耗内存，栈空间有限，如果程序栈空间地址写满，程序最后会崩溃</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> n*fun(n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><strong>4.4.5 eval 和 exec函数</strong></li><li><strong>eval函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># exec执行不返回结果，eval执行返回结果</span></span><br><span class="line">dic = &#123;&#125;</span><br><span class="line">dic[<span class="string">'b'</span>] = <span class="number">3</span></span><br><span class="line">exec(<span class="string">'a=4'</span>,dic)</span><br><span class="line">print(dic.keys())   <span class="comment">#打印dict_keys(['a','__builtins__','b'])</span></span><br><span class="line"><span class="comment"># 使用这两个函数第一个参数一定是可执行代码</span></span><br></pre></td></tr></table></figure><h3 id="4-3-变量的作用域"><a href="#4-3-变量的作用域" class="headerlink" title="4.3 变量的作用域"></a>4.3 变量的作用域</h3><strong>4.3.1 global语句</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># global 可以把局部声明为全局</span></span><br><span class="line">a = <span class="number">6</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a</span><br><span class="line">    a = <span class="number">5</span></span><br><span class="line">print(a)   <span class="comment"># 打印5</span></span><br></pre></td></tr></table></figure><strong>4.3.2 nonlocal语句</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nonlocal可以把全局往下一作用域调用</span></span><br><span class="line">a = <span class="number">6</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">()</span>:</span></span><br><span class="line">    a = <span class="number">7</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nested</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">nonlocal</span> a</span><br><span class="line">            a+=<span class="number">1</span></span><br><span class="line">    nested()</span><br><span class="line">    print(<span class="string">'本地:'</span>,a)   <span class="comment">#打印 8</span></span><br><span class="line">func()</span><br><span class="line">print(<span class="string">'全局'</span>,a)  <span class="comment"># 打印6</span></span><br><span class="line">print(a)   <span class="comment"># 打印5</span></span><br></pre></td></tr></table></figure><h2 id="5-面向对象的程序设计"><a href="#5-面向对象的程序设计" class="headerlink" title="5. 面向对象的程序设计"></a>5. 面向对象的程序设计</h2><h3 id="5-1-类的结构"><a href="#5-1-类的结构" class="headerlink" title="5.1 类的结构"></a>5.1 类的结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>：</span></span><br><span class="line"><span class="class">    <span class="title">def</span> <span class="title">__init__</span><span class="params">(self,属性)</span>:</span>  <span class="comment"># 构造函数</span></span><br><span class="line">        self.属性 = 属性</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getname</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">myc = MyClass(属性)   <span class="comment"># 初始化实例对象，构造函数的属性</span></span><br><span class="line">a = myc.getname()   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 类还具有一些内置属性</span></span><br><span class="line">__name__   名称</span><br><span class="line">__doc__  类的文档字符串</span><br><span class="line">__nodule__ 类的模块</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure><h3 id="5-2-类方法"><a href="#5-2-类方法" class="headerlink" title="5.2 类方法"></a>5.2 类方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmthod  # 声明为类方法，可以直接用类名进行调用，当然初始化实例对象进行调用也是正确的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(cls)</span>:</span></span><br><span class="line"><span class="meta">@staticmethod  # 等同于普通函数，只是被封装在类中，独立于整个类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>  # 同上的调用方式，且参数没有限制要求</span></span><br></pre></td></tr></table></figure><h3 id="5-3-类的私有化属性"><a href="#5-3-类的私有化属性" class="headerlink" title="5.3 类的私有化属性"></a>5.3 类的私有化属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 私有化属性方法，在类的属性前面加双下划线，同时会提供一个私有化属性的访问函数，不可以被修改,但可以针对具体实例进行对象修改</span></span><br><span class="line">clas MyClass:</span><br><span class="line">    __Occupation = <span class="string">'scientist'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(sefl,name,age)</span>；</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"><span class="comment"># 使用装饰器函数实现类的私有化</span></span><br><span class="line">clas MyClass:</span><br><span class="line">    __Occupation = <span class="string">'scientist'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(sefl,name,age)</span>；</span></span><br><span class="line"><span class="function">    @<span class="title">property</span>  # 装饰为属性，使类的私有化属性也可以被访问</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br></pre></td></tr></table></figure><h3 id="5-4-类的继承"><a href="#5-4-类的继承" class="headerlink" title="5.4 类的继承"></a>5.4 类的继承</h3><strong>5.4.1 继承结构体</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DerivedClass</span><span class="params">(FatherClass1, FatherClass2)</span>：</span></span><br><span class="line"><span class="class">    <span class="title">pass</span></span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Record</span>:</span></span><br><span class="line">    <span class="string">""" A record class """</span></span><br><span class="line">    __Ocuupation = <span class="string">"scientist"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"Occupation:"</span>,self.getOccupation())</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GirlRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        Record.showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line"></span><br><span class="line">myc = GirlRecord(<span class="string">"Anaa"</span>, <span class="number">21</span>)</span><br><span class="line">myc.showrecode()</span><br></pre></td></tr></table></figure><strong>5.4.2 super()函数</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保障了调用父类方法时只调用一次</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Record</span>:</span></span><br><span class="line">    <span class="string">""" A record class """</span></span><br><span class="line">    __Ocuupation = <span class="string">"scientist"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"Occupation:"</span>,self.getOccupation())</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getOccupation</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__Occupation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GirlRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaleRecord</span><span class="params">(Record)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThisRecord</span><span class="params">(GirlRecord, MaleRecord)</span>:</span></span><br><span class="line">    <span class="string">""" A girlrecord class """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showrecode</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().showrecode(self)</span><br><span class="line">        print(<span class="string">"the girl:"</span>, self.name, <span class="string">"age:"</span>, self.age)</span><br><span class="line">myc = ThisRecord(<span class="string">"Anaa"</span>, <span class="number">21</span>)</span><br><span class="line">myc.showrecode()</span><br></pre></td></tr></table></figure><h3 id="5-5-类相关的内置函数"><a href="#5-5-类相关的内置函数" class="headerlink" title="5.5 类相关的内置函数"></a>5.5 类相关的内置函数</h3></li><li><strong>判断实例（isinstance)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">isinstance(object, class_name)</span><br></pre></td></tr></table></figure></li><li><strong>判断字类（issubclass)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">issubclass(class1, class2)</span><br></pre></td></tr></table></figure></li><li><strong>判断类实例中是否包含某一个属性（hasattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hasattr(object, name)</span><br></pre></td></tr></table></figure></li><li><strong>获得类实例中的某一个属性（getattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getattr(object, name[,default])</span><br></pre></td></tr></table></figure></li><li><strong>设置类实例中的某一个属性值（setattr)</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setattr(object, name, value)</span><br></pre></td></tr></table></figure><h3 id="5-6-重载运算符"><a href="#5-6-重载运算符" class="headerlink" title="5.6 重载运算符"></a>5.6 重载运算符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="string">""" A record class"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span>  <span class="comment"># 将值转化为字符串进行输出</span></span><br><span class="line">        retrun <span class="string">"name:"</span>+self.name;<span class="string">"age:"</span>+str(self.age)</span><br><span class="line">    </span><br><span class="line">    __repr__ = __str__  <span class="comment"># 转化为解释器读取的形式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, record)</span>:</span>  <span class="comment">#重载比较运算符</span></span><br><span class="line">        <span class="keyword">if</span> self.age &lt; record.age:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, record)</span>:</span>   <span class="comment">#重载加号运算符</span></span><br><span class="line">        <span class="keyword">return</span> MyClass(self.name, self.age+record.age)</span><br><span class="line"></span><br><span class="line">myc = MyClass(<span class="string">"A"</span>, <span class="number">42</span>)</span><br><span class="line">myc1 = MyClass(<span class="string">"B"</span>, <span class="number">23</span>)</span><br><span class="line"></span><br><span class="line">print(repr(myc))  </span><br><span class="line">print(myc)</span><br><span class="line">print(str(myc))</span><br><span class="line">print(myc&lt;myc1)</span><br><span class="line">print(myc+myc1)</span><br></pre></td></tr></table></figure></li><li><strong>运算符重载</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"> 方法名                  运算符和表达式      说明</span><br><span class="line">__add__(self,rhs)        self + rhs        加法</span><br><span class="line">__sub__(self,rhs)        self - rhs         减法</span><br><span class="line">__mul__(self,rhs)        self * rhs         乘法</span><br><span class="line">__truediv__(self,rhs)    self / rhs          除法</span><br><span class="line">__floordiv__(self,rhs)   self //rhs          地板除</span><br><span class="line">__mod__(self,rhs)        self % rhs       取模(求余)</span><br><span class="line">__pow__(self,rhs)        self **rhs         幂运算</span><br><span class="line">合赋值算术运算符的重载:</span><br><span class="line">方法名                  运算符和表达式      说明</span><br><span class="line">__iadd__(self,rhs)       self += rhs        加法</span><br><span class="line">__isub__(self,rhs)       self -= rhs         减法</span><br><span class="line">__imul__(self,rhs)       self *= rhs         乘法</span><br><span class="line">__itruediv__(self,rhs)   self /= rhs        除法</span><br><span class="line">__ifloordiv__(self,rhs)  self //=rhs        地板除</span><br><span class="line">__imod__(self,rhs)       self %= rhs     取模(求余)</span><br><span class="line">__ipow__(self,rhs)       self **=rhs       幂运算</span><br><span class="line"></span><br><span class="line">比较算术运算符的重载:</span><br><span class="line">方法名                  运算符和表达式      说明</span><br><span class="line">__lt__(self,rhs)       self &lt; rhs        小于</span><br><span class="line">__le__(self,rhs)       self &lt;= rhs       小于等于</span><br><span class="line">__gt__(self,rhs)       self &gt; rhs        大于</span><br><span class="line">__ge__(self,rhs)       self &gt;= rhs       大于等于</span><br><span class="line">__eq__(self,rhs)       self == rhs       等于</span><br><span class="line">__ne__(self,rhs)       self != rhs       不等于</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">位运算符重载</span><br><span class="line">方法名              运算符和表达式        说明</span><br><span class="line">__and__(self,rhs)       self &amp; rhs           位与</span><br><span class="line">__or__(self,rhs)        self | rhs           位或</span><br><span class="line">__xor__(self,rhs)       self ^ rhs           位异或</span><br><span class="line"> __lshift__(self,rhs)    self &lt;&lt;rhs           左移</span><br><span class="line"> __rshift__(self,rhs)    self &gt;&gt;rhs           右移</span><br><span class="line"></span><br><span class="line">反向位运算符重载</span><br><span class="line"></span><br><span class="line">方法名            运算符和表达式       说明</span><br><span class="line">__and__(self,lhs)       lhs &amp; rhs        位与</span><br><span class="line">__or__(self,lhs)         lhs | rhs       位或</span><br><span class="line">__xor__(self,lhs)       lhs ^ rhs        位异或</span><br><span class="line">__lshift__(self,lhs)    lhs &lt;&lt;rhs        左移</span><br><span class="line">__rshift__(self,lhs)    lhs &gt;&gt;rhs        右移</span><br><span class="line"></span><br><span class="line">复合赋值位相关运算符重载</span><br><span class="line">方法名              运算符和表达式          说明</span><br><span class="line">__iand__(self,rhs)       self &amp; rhs       位与</span><br><span class="line">__ior__(self,rhs)        self | rhs       位或</span><br><span class="line">__ixor__(self,rhs)       self ^ rhs       位异或</span><br><span class="line">__ilshift__(self,rhs)    self &lt;&lt;rhs       左移</span><br><span class="line">__irshift__(self,rhs)    self &gt;&gt;rhs       右移</span><br><span class="line"></span><br><span class="line">一元运算符的重载</span><br><span class="line">方法名              运算符和表达式       说明</span><br><span class="line">__neg__(self)         - self           负号</span><br><span class="line">__pos__(self)         + self           正号</span><br><span class="line">__invert__(self)      ~ self           取反</span><br></pre></td></tr></table></figure><h2 id="6-错误异常与文件读写"><a href="#6-错误异常与文件读写" class="headerlink" title="6 错误异常与文件读写"></a>6 错误异常与文件读写</h2><h3 id="6-1-错误异常捕捉"><a href="#6-1-错误异常捕捉" class="headerlink" title="6.1 错误异常捕捉"></a>6.1 错误异常捕捉</h3><strong>6.1.1 异常语句结构</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">except</span>(ZeroDivisionError, ValueError):</span><br><span class="line">    print(<span class="string">'错误'</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    print(<span class="string">'其它异常’)</span></span><br><span class="line"><span class="string">except Exception as e:  # 捕捉未知异常</span></span><br><span class="line"><span class="string">    print(e)</span></span><br><span class="line"><span class="string">else:</span></span><br><span class="line"><span class="string">    pass   # 没有异常发生时执行</span></span><br><span class="line"><span class="string">finally:</span></span><br><span class="line"><span class="string">    pass  # 最终处理语句，无论是否有异常，都要执行这个语句</span></span><br></pre></td></tr></table></figure><strong>6.1.2 异常类型</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异常名称         异常解释</span></span><br><span class="line">AttributeError试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x</span><br><span class="line">IOError输入/输出异常；基本上是无法打开文件</span><br><span class="line">ImportError无法引入模块或包；基本上是路径问题或名称错误</span><br><span class="line">IndentationError语法错误（的子类） ；代码没有正确对齐</span><br><span class="line">IndexError下标索引超出序列边界，比如当x只有三个元素，却试图访问x[<span class="number">5</span>]</span><br><span class="line">KeyError试图访问字典里不存在的键</span><br><span class="line">KeyboardInterruptCtrl+C被按下</span><br><span class="line">NameError使用一个还未被赋予对象的变量</span><br><span class="line">SyntaxErrorPython代码非法，代码不能编译(个人认为这是语法错误，写错了）</span><br><span class="line">TypeError传入对象类型与要求的不符合</span><br><span class="line">UnboundLocalError</span><br><span class="line">试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它</span><br><span class="line">ValueError传入一个调用者不期望的值，即使值的类型是正确</span><br><span class="line">BaseException                        　　　　所有异常的基类</span><br><span class="line">SystemExit　　　　　　　　 　　　　解释器请求退出</span><br><span class="line">KeyboardInterrupt　　　　　 　　　　用户中断执行(通常是输入^C)</span><br><span class="line">Exception　　　　　　　　　　　　 常规错误的基类</span><br><span class="line">StopIteration 　　　　　　　　　　　　迭代器没有更多的值</span><br><span class="line">GeneratorExit 　　　　　　　　　　生成器(generator)发生异常来通知退出</span><br><span class="line">StandardError　　　　　　　　 　　所有的内建标准异常的基类</span><br><span class="line">ArithmeticError　　　　　　　　　　 所有数值计算错误的基类</span><br><span class="line">FloatingPointError 　　　　　　　　浮点计算错误</span><br><span class="line">OverflowError 　　　　　　　　　　数值运算超出最大限制</span><br><span class="line">ZeroDivisionError　　　　　　 　　除(或取模)零 (所有数据类型)</span><br><span class="line">AssertionError　　　　　　　　 　　断言语句失败</span><br><span class="line">AttributeError 　　　　　　　　　　对象没有这个属性</span><br><span class="line">EOFError 　　　　　　　　　　　　没有内建输入,到达EOF 标记</span><br><span class="line">EnvironmentError　　　　　　 　　操作系统错误的基类</span><br><span class="line">IOError　　　　　　　　　　　　 输入/输出操作失败</span><br><span class="line">OSError　　　　　　　　　　　　 操作系统错误</span><br><span class="line">WindowsError 　　　　　　　　　　系统调用失败</span><br><span class="line">ImportError　　　　　　　　　　 导入模块/对象失败</span><br><span class="line">LookupError　　　　　　　　　　 无效数据查询的基类</span><br><span class="line">IndexError 　　　　　　　　　　序列中没有此索引(index)</span><br><span class="line">KeyError　　　　　　　　　　　　 映射中没有这个键</span><br><span class="line">MemoryError　　　　　　　　　　 内存溢出错误(对于Python 解释器不是致命的)</span><br><span class="line">NameError 　　　　　　　　　　未声明/初始化对象 (没有属性)</span><br><span class="line">UnboundLocalError 　　　　　　　　访问未初始化的本地变量</span><br><span class="line">ReferenceError　　　　　　　　 弱引用(Weak reference)试图访问已经垃圾回收了的对象</span><br><span class="line">RuntimeError　　　　　　　　　　 一般的运行时错误</span><br><span class="line">NotImplementedError 　　　　　　　　尚未实现的方法</span><br><span class="line">SyntaxError Python　　　　　　　　 语法错误</span><br><span class="line">IndentationError　　　　　　　　　　 缩进错误</span><br><span class="line">TabError Tab　　　　　　　　　　 和空格混用</span><br><span class="line">SystemError 　　　　　　　　　　　　一般的解释器系统错误</span><br><span class="line">TypeError　　　　　　　　　　　　 对类型无效的操作</span><br><span class="line">ValueError 　　　　　　　　　　　　传入无效的参数</span><br><span class="line">UnicodeError Unicode　　　　　　　　 相关的错误</span><br><span class="line">UnicodeDecodeError Unicode　　　　 解码时的错误</span><br><span class="line">UnicodeEncodeError Unicode　　　　 编码时错误</span><br><span class="line">UnicodeTranslateError Unicode 　　　　转换时错误</span><br><span class="line">Warning　　　　　　　　　　　　　　 警告的基类</span><br><span class="line">DeprecationWarning　　　　　　　　 关于被弃用的特征的警告</span><br><span class="line">FutureWarning 　　　　　　　　　　　　关于构造将来语义会有改变的警告</span><br><span class="line">OverflowWarning　　　　　　　　　　　 旧的关于自动提升为长整型(long)的警告</span><br><span class="line">PendingDeprecationWarning　　　　　　 关于特性将会被废弃的警告</span><br><span class="line">RuntimeWarning 　　　　　　　　　　　可疑的运行时行为(runtime behavior)的警告</span><br><span class="line">SyntaxWarning　　　　　　　　　　 可疑的语法的警告</span><br><span class="line">UserWarning 　　　　　　　　　　用户代码生成的警告</span><br></pre></td></tr></table></figure><h3 id="6-2-文件读写与导入"><a href="#6-2-文件读写与导入" class="headerlink" title="6.2 文件读写与导入"></a>6.2 文件读写与导入</h3><strong>6.2.1 语句结构</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">f = open(<span class="string">'new/test.txt'</span>,<span class="string">'a+'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">f.write(<span class="string">'\n今天天气很好'</span>)</span><br><span class="line">f.close()</span><br><span class="line">f = open(<span class="string">'new/test.txt'</span>,<span class="string">'rb'</span>)</span><br><span class="line">w=f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(w)</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'workfile'</span>, <span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'workfile'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    read_data = f.read()</span><br><span class="line">f.close()</span><br><span class="line">文件常见的读写模式</span><br><span class="line">w     以写方式打开，</span><br><span class="line">W     文件若存在，首先要清空，然后（重新）创建</span><br><span class="line">a     以追加模式打开 (从 EOF 开始, 必要时创建新文件)</span><br><span class="line">r+     以读写模式打开</span><br><span class="line">w+     以读写模式打开 (参见 w )</span><br><span class="line">a+     以读写模式打开 (参见 a )</span><br><span class="line">rb     以二进制读模式打开</span><br><span class="line">wb     以二进制写模式打开 (参见 w )</span><br><span class="line">ab     以二进制追加模式打开 (参见 a )</span><br><span class="line">rb+    以二进制读写模式打开 (参见 r+ )</span><br><span class="line">wb+    以二进制读写模式打开 (参见 w+ )</span><br><span class="line">ab+    以二进制读写模式打开 (参见 a+ )</span><br></pre></td></tr></table></figure><strong>6.2.1 文件读写方法</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">f.read()</span><br><span class="line">f.read(size)</span><br><span class="line">f.readline()  <span class="comment"># 读取一行</span></span><br><span class="line">f.readlines()  <span class="comment"># 读取所有行，每一行存储为列表的每一个元素</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    print(line, end=<span class="string">''</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">'This is a test\n'</span>)</span><br><span class="line"><span class="number">15</span></span><br><span class="line">f.tell() <span class="comment"># 返回一个整数，给出文件对象在文件中的当前位置，表示为二进制模式下时从文件开始的字节数，以及文本模式下的不透明数字。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 要改变文件对象的位置，请使用 f.seek(offset, whence)。 通过向一个参考点添加 offset 来计算位置；  </span></span><br><span class="line"><span class="comment"># 参考点由 whence 参数指定。  </span></span><br><span class="line"><span class="comment"># whence 的 0 值表示从文件开头起算，1 表示使用当前文件位置，2 表示使用文件末尾作为参考点。   </span></span><br><span class="line"><span class="comment"># whence 如果省略则默认值为 0，即使用文件开头作为参考点。</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = open(<span class="string">'workfile'</span>, <span class="string">'rb+'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">b'0123456789abcdef'</span>)</span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.seek(<span class="number">5</span>)      <span class="comment"># Go to the 6th byte in the file</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read(<span class="number">1</span>)</span><br><span class="line"><span class="string">b'5'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.seek(<span class="number">-3</span>, <span class="number">2</span>)  <span class="comment"># Go to the 3rd byte before the end</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read(<span class="number">1</span>)</span><br><span class="line"><span class="string">b'd'</span></span><br></pre></td></tr></table></figure><h3 id="6-3-常见文件类型的读取方式"><a href="#6-3-常见文件类型的读取方式" class="headerlink" title="6.3 常见文件类型的读取方式"></a>6.3 常见文件类型的读取方式</h3></li><li><strong>csv文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'C:/users/lenovo/desktop/student_score.csv'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines(): <span class="comment">#逐行读取</span></span><br><span class="line">        print(line)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用panda读取数据，存为一个矩阵的形式</span></span><br><span class="line"><span class="keyword">import</span> panda <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(sys.argv[<span class="number">1</span>])</span><br></pre></td></tr></table></figure></li><li><strong>txt文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">'my_file.txt'</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:      <span class="comment">#逐行读取</span></span><br><span class="line">print(line.strip())  <span class="comment">#使用strip删除空格和空行，否则会有\n在最后</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用numpy读取数据,转化一个数组</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">dataset = np.loadtxt(<span class="string">'路径'</span>)</span><br><span class="line">dataset.shape( )  <span class="comment"># 查看数组维度</span></span><br><span class="line">newset = reshape(dataset, (<span class="number">100</span>,<span class="number">3</span>))  <span class="comment"># 转化为100行 3列的数组</span></span><br></pre></td></tr></table></figure></li><li><strong>excle文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xlrd   <span class="comment">#使用库函数</span></span><br><span class="line"></span><br><span class="line">workbook = xlrd.open_workbook(<span class="string">'C:/users/lenovo/desktop/student_score.xlsx'</span>)  <span class="comment">#读取路径</span></span><br><span class="line">sheet = workbook.sheet_by_name(<span class="string">'Sheet1'</span>)     <span class="comment">#读取excel中的第一个sheet</span></span><br><span class="line"></span><br><span class="line">data_name = sheet.col_values(<span class="number">0</span>)    <span class="comment">#按列读取，读取第一列</span></span><br><span class="line"><span class="comment">#data_name1 = sheet.row_values(0)  #按行读取，读取第一行</span></span><br><span class="line">data_st_ID = sheet.col_values(<span class="number">1</span>)</span><br><span class="line">data_st_score = sheet.col_values(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更加高效的是使用panda读取数据</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">test_df = pd.read_excel(<span class="string">r'G:\test.xlsx'</span>)</span><br></pre></td></tr></table></figure></li><li><strong>mat文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line">os.chdir(<span class="string">r'F:/data'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line">data = scio.loadmat(<span class="string">'97.mat'</span>)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">de = data[<span class="string">'X097_DE_time'</span>]</span><br><span class="line"></span><br><span class="line">读取的结果是一个字典</span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line"><span class="keyword">from</span> mat4py <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = np.array(loadmat(<span class="string">'test.mat'</span>)[<span class="string">'dictKey'</span>]).astype(<span class="string">'float'</span>)</span><br></pre></td></tr></table></figure><h3 id="6-3-使用json保存结构化数据"><a href="#6-3-使用json保存结构化数据" class="headerlink" title="6.3 使用json保存结构化数据"></a>6.3 使用json保存结构化数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>json.dumps([<span class="number">1</span>, <span class="string">'simple'</span>, <span class="string">'list'</span>])</span><br><span class="line"><span class="string">'[1, "simple", "list"]'</span></span><br></pre></td></tr></table></figure><strong>6.3.1 将数据保存为json文件</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model=&#123;&#125; <span class="comment">#数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./hmm.json"</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json.dump(model,json_file,ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><strong>6.3.2 从json文件读取数据</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model=&#123;&#125; <span class="comment">#存放读取的数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./hmm.json"</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    model=json.load(json_file)</span><br><span class="line"></span><br><span class="line">读取返回的为python字典</span><br><span class="line"><span class="keyword">or</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"> </span><br><span class="line">str_file = <span class="string">'./960x540/config.json'</span></span><br><span class="line"><span class="keyword">with</span> open(str_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    print(<span class="string">"Load str file from &#123;&#125;"</span>.format(str_file))</span><br><span class="line">    r = json.load(f)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r)</span><br><span class="line">print(r[<span class="string">'under_game_score_y'</span>])</span><br></pre></td></tr></table></figure><h2 id="7-python标准库"><a href="#7-python标准库" class="headerlink" title="7. python标准库"></a>7. python标准库</h2></li></ul><div align=" center"><img src="https://img.vim-cn.com/47/19c6e06be5f16f01de1a5d1ef6733fd9129648.png"></div><p><strong>也可以查看标准库文档</strong><br><a href="https://docs.python.org/zh-cn/3/tutorial/index.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/tutorial/index.html</a><br><a href="https://docs.python.org/zh-cn/3/library/index.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/index.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Shypython-learn-notes&quot;&gt;&lt;a href=&quot;#Shypython-learn-notes&quot; class=&quot;headerlink&quot; title=&quot;Shypython-learn-notes&quot;&gt;&lt;/a&gt;Shypython-learn-notes&lt;/
      
    
    </summary>
    
    
      <category term="python" scheme="https://shyshy903.github.io//categories/python/"/>
    
    
      <category term="python" scheme="https://shyshy903.github.io//tags/python/"/>
    
  </entry>
  
  <entry>
    <title>桶排序</title>
    <link href="https://shyshy903.github.io/2020/05/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%A1%B6%E6%8E%92%E5%BA%8F/"/>
    <id>https://shyshy903.github.io/2020/05/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%A1%B6%E6%8E%92%E5%BA%8F/</id>
    <published>2020-05-11T16:00:00.000Z</published>
    <updated>2020-05-11T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p>桶排序有三个核心问题：</p><ul><li>每个桶的长度是多少</li><li>总共需要多少个桶</li><li>如何确定元素应该在哪一个桶</li></ul><p>因此桶排序有几个公式需要记忆一下：</p><script type="math/tex; mode=display">length = \frac{max(nums)-min(nums)}{len(nums)-1}</script><script type="math/tex; mode=display">count = \frac{max(nums)-min(nums)}{length}+1</script><script type="math/tex; mode=display">location = \frac{nums[i]-min(nums)}{length}</script><p><strong>leetcode164中有一道题如下：</strong><br>给定一个无序的数组，找出数组在排序之后，相邻元素之间最大的差值。</p><p>如果数组元素个数小于 2，则返回 0。</p><p>示例 1:</p><p>输入: [3,6,9,1]<br>输出: 3<br>解释: 排序后的数组是 [1,3,6,9], 其中相邻元素 (3,6) 和 (6,9) 之间都存在最大差值 3。<br>示例 2:</p><p>输入: [10]<br>输出: 0<br>解释: 数组元素个数小于 2，因此返回 0。<br>说明:</p><p>你可以假设数组中所有元素都是非负整数，且数值在 32 位有符号整数范围内。<br>请尝试在线性时间复杂度和空间复杂度的条件下解决此问题。</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/maximum-gap" target="_blank" rel="noopener">https://leetcode-cn.com/problems/maximum-gap</a></p><p>python代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maximumGap</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums <span class="keyword">or</span> len(nums) &lt; <span class="number">2</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    max_ = max(nums)</span><br><span class="line">    min_ = min(nums)</span><br><span class="line">    max_gap = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    each_bucket_len = max(<span class="number">1</span>,(max_-min_) // (len(nums)<span class="number">-1</span>))</span><br><span class="line">    buckets =[[] <span class="keyword">for</span> _ <span class="keyword">in</span> range((max_-min_) // each_bucket_len + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">        loc = (nums[i] - min_) // each_bucket_len</span><br><span class="line">        buckets[loc].append(nums[i])</span><br><span class="line">    </span><br><span class="line">    prev_max = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(buckets)):</span><br><span class="line">        <span class="keyword">if</span> buckets[i] <span class="keyword">and</span> prev_max != float(<span class="string">'inf'</span>):</span><br><span class="line">            max_gap = max(max_gap, min(buckets[i])-prev_max)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> buckets[i]:</span><br><span class="line">            prev_max = max(buckets[i])</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> max_gap</span><br></pre></td></tr></table></figure></p><p>C++代码如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maximumGap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> minVal = *min_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">int</span> maxVal = *max_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">int</span> d = <span class="built_in">max</span>(<span class="number">1</span>, (maxVal - minVal) / (n - <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">int</span> bucketSize = (maxVal - minVal) / d + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        vector&lt;pair&lt;int, int&gt;&gt; bucket(bucketSize, &#123;-1, -1&#125;);  // 存储 (桶内最小值，桶内最大值) 对，(-1, -1) 表示该桶是空的</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> idx = (nums[i] - minVal) / d;</span><br><span class="line">            <span class="keyword">if</span> (bucket[idx].first == <span class="number">-1</span>) &#123;</span><br><span class="line">                bucket[idx].first = bucket[idx].second = nums[i];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                bucket[idx].first = <span class="built_in">min</span>(bucket[idx].first, nums[i]);</span><br><span class="line">                bucket[idx].second = <span class="built_in">max</span>(bucket[idx].second, nums[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> prev = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bucketSize; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (bucket[i].first == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span> (prev != <span class="number">-1</span>) &#123;</span><br><span class="line">                ret = <span class="built_in">max</span>(ret, bucket[i].first - bucket[prev].second);</span><br><span class="line">            &#125;</span><br><span class="line">            prev = i;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;桶排序&quot;&gt;&lt;a href=&quot;#桶排序&quot; class=&quot;headerlink&quot; title=&quot;桶排序&quot;&gt;&lt;/a&gt;桶排序&lt;/h2&gt;&lt;p&gt;桶排序有三个核心问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个桶的长度是多少&lt;/li&gt;
&lt;li&gt;总共需要多少个桶&lt;/li&gt;
&lt;li&gt;如何确定
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>基数排序</title>
    <link href="https://shyshy903.github.io/2020/03/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    <id>https://shyshy903.github.io/2020/03/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/</id>
    <published>2020-03-11T16:00:00.000Z</published>
    <updated>2020-03-11T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><p>基数排序（radix sort)是一种分配排序算法，排序过程种无需比较关键字，而是通过分配与收集的过程实现排序，它们的时间复杂度为O(n)，在线性阶层上。</p><p>排序方法分为LSD 与MSD两种方法，LSD是从低阶往高阶进行排序，MSD方法是从高阶往低阶进行排序。</p><p>实现方法与代码：</p><ul><li>获取数组中的最高位数</li><li>建立一个桶数组，根据个位数进行赋值，遍历数组，将它们分配至编号0-9的桶中去</li><li>将桶中的数值串联起来，通过buf数组暂存，并拷贝到原始数组中</li><li>重复前2个步骤</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxbit1</span><span class="params">(<span class="keyword">int</span> data[], <span class="keyword">int</span> n)</span> <span class="comment">//辅助函数，求解数据的最大位数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> d = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(data[i] &gt;= p)</span><br><span class="line">        &#123;</span><br><span class="line">            p *= <span class="number">10</span>;</span><br><span class="line">            ++d;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxbit2</span><span class="params">(<span class="keyword">int</span> maxVal)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> d = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">while</span>(maxVal &gt;= p)</span><br><span class="line">    &#123;</span><br><span class="line">        p *= <span class="number">10</span>;</span><br><span class="line">        ++d;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">radixSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">buf</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">int</span> maxVal = *max_element(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>()); </span><br><span class="line">    <span class="keyword">int</span> maxbit =  maxbit2(maxVal);</span><br><span class="line">    <span class="comment">// 进入循环</span></span><br><span class="line">    <span class="keyword">int</span> dev = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; maxbit; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//开始统计每个桶的元素个数</span></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">count</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> digit = (nums[j] / dev) % <span class="number">10</span>;</span><br><span class="line">            count[digit]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将count数组变为数组的索引</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; <span class="number">10</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            count[j] += count[j<span class="number">-1</span>];</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将桶中的数值串联起来</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = n<span class="number">-1</span> ;j &gt;= <span class="number">0</span>; j --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> digit = (nums[j] / dev) % <span class="number">10</span>;</span><br><span class="line">            buf[count[digit] - <span class="number">1</span>] = nums[j];</span><br><span class="line">            count[digit]--;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将临时数组拷贝给原数组</span></span><br><span class="line">        copy(buf.<span class="built_in">begin</span>(), buf.<span class="built_in">end</span>(), nums.<span class="built_in">begin</span>());</span><br><span class="line">        dev *= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>leetcode164中有一道题如下：</strong><br>给定一个无序的数组，找出数组在排序之后，相邻元素之间最大的差值。</p><p>如果数组元素个数小于 2，则返回 0。</p><p>示例 1:</p><p>输入: [3,6,9,1]<br>输出: 3<br>解释: 排序后的数组是 [1,3,6,9], 其中相邻元素 (3,6) 和 (6,9) 之间都存在最大差值 3。<br>示例 2:</p><p>输入: [10]<br>输出: 0<br>解释: 数组元素个数小于 2，因此返回 0。<br>说明:</p><p>你可以假设数组中所有元素都是非负整数，且数值在 32 位有符号整数范围内。<br>请尝试在线性时间复杂度和空间复杂度的条件下解决此问题。</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/maximum-gap" target="_blank" rel="noopener">https://leetcode-cn.com/problems/maximum-gap</a></p><p>基于上面的基数排序可以进行求解，只要再加上一个maxGap的函数就可以<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxGap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> n = nums.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ret = <span class="built_in">max</span>(ret, nums[i] - nums[i<span class="number">-1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基数排序&quot;&gt;&lt;a href=&quot;#基数排序&quot; class=&quot;headerlink&quot; title=&quot;基数排序&quot;&gt;&lt;/a&gt;基数排序&lt;/h2&gt;&lt;p&gt;基数排序（radix sort)是一种分配排序算法，排序过程种无需比较关键字，而是通过分配与收集的过程实现排序，它们的时间复
      
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构与算法" scheme="https://shyshy903.github.io//tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="https://shyshy903.github.io/2020/02/17/Deep_learning/Transformer/"/>
    <id>https://shyshy903.github.io/2020/02/17/Deep_learning/Transformer/</id>
    <published>2020-02-16T16:00:00.000Z</published>
    <updated>2020-02-16T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="transformer模型"><a href="#transformer模型" class="headerlink" title="transformer模型"></a>transformer模型</h2><p>在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：</p><ul><li>CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。</li><li>RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。<br>为了整合CNN和RNN的优势，[Vaswani et al., 2017] 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。</li></ul><p>Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：</p><ul><li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li><li>Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li><li>Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li></ul><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960"></img></div><p><img src="https://i.bmp.ovh/imgs/2020/02/6801838056cf8851.png" alt=""></p><p><img src="https://i.bmp.ovh/imgs/2020/02/be0c163e470769bb.png" alt=""></p><h2 id="transformer的pytorch实现"><a href="#transformer的pytorch实现" class="headerlink" title="transformer的pytorch实现"></a>transformer的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'../'</span>)</span><br><span class="line"><span class="keyword">import</span> d2l</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    X_len = X_len.to(X.device)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)</span><br><span class="line">    mask = mask[<span class="literal">None</span>, :] &lt; X_len[:, <span class="literal">None</span>]</span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[~mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h2 id="多头注意力层"><a href="#多头注意力层" class="headerlink" title="多头注意力层"></a>多头注意力层</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/c20478604f4c831c.png" alt=""></p><h3 id="多头注意力模型"><a href="#多头注意力模型" class="headerlink" title="多头注意力模型"></a>多头注意力模型</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/b9fcbb9a53f2e4db.png" alt=""></p><p>在我们讨论多头注意力层之前，先来迅速理解以下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320"></img></div><p>多头注意力层h包含个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。</p><p><img src="C:\Users\shy\AppData\Roaming\Typora\typora-user-images\image-20200217212647159.png" alt="image-20200217212647159"></p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640"></img></div><p><img src="https://i.bmp.ovh/imgs/2020/02/5246bbe9920c926b.png" alt=""></p><h3 id="多头注意力pytorch"><a href="#多头注意力pytorch" class="headerlink" title="多头注意力pytorch"></a>多头注意力pytorch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_heads, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_k = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_v = nn.Linear(input_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_o = nn.Linear(hidden_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        <span class="comment"># query, key, and value shape: (batch_size, seq_len, dim),</span></span><br><span class="line">        <span class="comment"># where seq_len is the length of input sequence</span></span><br><span class="line">        <span class="comment"># valid_length shape is either (batch_size, )</span></span><br><span class="line">        <span class="comment"># or (batch_size, seq_len).</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project and transpose query, key, and value from</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * num_heads) to</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, hidden_size).</span></span><br><span class="line">        </span><br><span class="line">        query = transpose_qkv(self.W_q(query), self.num_heads)</span><br><span class="line">        key = transpose_qkv(self.W_k(key), self.num_heads)</span><br><span class="line">        value = transpose_qkv(self.W_v(value), self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Copy valid_length by num_heads times</span></span><br><span class="line">            device = valid_length.device</span><br><span class="line">            valid_length = valid_length.cpu().numpy() <span class="keyword">if</span> valid_length.is_cuda <span class="keyword">else</span> valid_length.numpy()</span><br><span class="line">            <span class="keyword">if</span> valid_length.ndim == <span class="number">1</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            valid_length = valid_length.to(device)</span><br><span class="line">            </span><br><span class="line">        output = self.attention(query, key, value, valid_length)</span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># Original X shape: (batch_size, seq_len, hidden_size * num_heads),</span></span><br><span class="line">    <span class="comment"># -1 means inferring its value, after first reshape, X shape:</span></span><br><span class="line">    <span class="comment"># (batch_size, seq_len, num_heads, hidden_size)</span></span><br><span class="line">    X = X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)</span></span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Merge the first two dimensions. Use reverse=True to infer shape from</span></span><br><span class="line">    <span class="comment"># right to left.</span></span><br><span class="line">    <span class="comment"># output shape: (batch_size * num_heads, seq_len, hidden_size)</span></span><br><span class="line">    output = X.view(<span class="number">-1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Saved in the d2l package for later use</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span><span class="params">(X, num_heads)</span>:</span></span><br><span class="line">    <span class="comment"># A reversed version of transpose_qkv</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.transpose(<span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    <span class="keyword">return</span> X.view(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="基于位置的前馈网络（FFN"><a href="#基于位置的前馈网络（FFN" class="headerlink" title="基于位置的前馈网络（FFN)"></a>基于位置的前馈网络（FFN)</h2><p>Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。</p><p>下面我们来实现PositionWiseFFN：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs)</span>:</span></span><br><span class="line">        super(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)</span><br><span class="line">        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.ffn_2(F.relu(self.ffn_1(X)))</span><br></pre></td></tr></table></figure><h2 id="Add-and-Norm"><a href="#Add-and-Norm" class="headerlink" title="Add and Norm"></a>Add and Norm</h2><p>除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。 </p><p><img src="https://i.bmp.ovh/imgs/2020/02/3799a624c6b07777.png" alt=""></p><p><img src="https://i.bmp.ovh/imgs/2020/02/ef62acc1523f3c3f.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">layernorm = nn.LayerNorm(normalized_shape=<span class="number">2</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">batchnorm = nn.BatchNorm1d(num_features=<span class="number">2</span>, affine=<span class="literal">True</span>)</span><br><span class="line">X = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(<span class="string">'layer norm:'</span>, layernorm(X))</span><br><span class="line">print(<span class="string">'batch norm:'</span>, batchnorm(X))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.norm = nn.LayerNorm(hidden_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/2903fe22b21afacc.png" alt=""></p><p>衡量单词向量的距离，与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640"></img></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, dropout, max_len=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = np.zeros((<span class="number">1</span>, max_len, embedding_size))</span><br><span class="line">        X = np.arange(<span class="number">0</span>, max_len).reshape(<span class="number">-1</span>, <span class="number">1</span>) / np.power(</span><br><span class="line">            <span class="number">10000</span>, np.arange(<span class="number">0</span>, embedding_size, <span class="number">2</span>)/embedding_size)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = np.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = np.cos(X)</span><br><span class="line">        self.P = torch.FloatTensor(self.P)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> X.is_cuda <span class="keyword">and</span> <span class="keyword">not</span> self.P.is_cuda:</span><br><span class="line">            self.P = self.P.cuda()</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器(Encoder)"></a>编码器(Encoder)</h2><p>我们已经有了组成Transformer的各个模块，现在我们可以开始搭建了！编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，我们的输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为我们要将前一层的输出与原始输入相加并归一化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length)</span>:</span></span><br><span class="line">        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>现在我们来实现整个Transformer 编码器模型，整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以$\sqrt{d}$以防止其值过小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                EncoderBlock(embedding_size, ffn_hidden_size,</span><br><span class="line">                             num_heads, dropout))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, valid_length, *args)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_length)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h2 id="解码器（Decoder"><a href="#解码器（Decoder" class="headerlink" title="解码器（Decoder)"></a>解码器（Decoder)</h2><p>Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。</p><p><img src="https://i.loli.net/2020/02/17/klqSyL9aibmEQBH.png" alt="image.png"></p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5kpefhcyg.png?imageView2/0/w/800/h/800"></img></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs)</span>:</span></span><br><span class="line">        super(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_1 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)</span><br><span class="line">        self.addnorm_2 = AddNorm(embedding_size, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)</span><br><span class="line">        self.addnorm_3 = AddNorm(embedding_size, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, enc_valid_length = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># state[2][self.i] stores all the previous t-1 query state of layer-i</span></span><br><span class="line">        <span class="comment"># len(state[2]) = num_layers</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If training:</span></span><br><span class="line">        <span class="comment">#     state[2] is useless.</span></span><br><span class="line">        <span class="comment"># If predicting:</span></span><br><span class="line">        <span class="comment">#     In the t-th timestep:</span></span><br><span class="line">        <span class="comment">#         state[2][self.i].shape = (batch_size, t-1, hidden_size)</span></span><br><span class="line">        <span class="comment"># Demo:</span></span><br><span class="line">        <span class="comment"># love dogs ! [EOS]</span></span><br><span class="line">        <span class="comment">#  |    |   |   |</span></span><br><span class="line">        <span class="comment">#   Transformer </span></span><br><span class="line">        <span class="comment">#    Decoder</span></span><br><span class="line">        <span class="comment">#  |   |   |   |</span></span><br><span class="line">        <span class="comment">#  I love dogs !</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># shape of key_values = (batch_size, t, hidden_size)</span></span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), dim=<span class="number">1</span>) </span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, seq_len, _ = X.shape</span><br><span class="line">            <span class="comment"># Shape: (batch_size, seq_len), the values in the j-th column are j+1</span></span><br><span class="line">            valid_length = torch.FloatTensor(np.tile(np.arange(<span class="number">1</span>, seq_len+<span class="number">1</span>), (batch_size, <span class="number">1</span>))) </span><br><span class="line">            valid_length = valid_length.to(X.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention_1(X, key_values, key_values, valid_length)</span><br><span class="line">        Y = self.addnorm_1(X, X2)</span><br><span class="line">        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)</span><br><span class="line">        Z = self.addnorm_2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm_3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。下面让我们来实现一下Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_size, ffn_hidden_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_heads, num_layers, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embedding_size)</span><br><span class="line">        self.pos_encoding = PositionalEncoding(embedding_size, dropout)</span><br><span class="line">        self.blks = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.blks.append(</span><br><span class="line">                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,</span><br><span class="line">                             dropout, i))</span><br><span class="line">        self.dense = nn.Linear(embedding_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_length, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_length, [<span class="literal">None</span>]*self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>Transformer同样基于编码器-解码器架构</li><li>Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。</li><li>dd and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。</li><li>osition encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。</li><li>增强训练，可以做个label_smoothing</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;h2 id=&quot;transformer模型&quot;&gt;&lt;a href=&quot;#tran
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络基础（CNN)</title>
    <link href="https://shyshy903.github.io/2020/02/17/Deep_learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88CNN)/"/>
    <id>https://shyshy903.github.io/2020/02/17/Deep_learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88CNN)/</id>
    <published>2020-02-16T16:00:00.000Z</published>
    <updated>2020-02-16T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><h3 id="二维互相关运算"><a href="#二维互相关运算" class="headerlink" title="二维互相关运算"></a>二维互相关运算</h3><p>虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。<br>我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为$3 \times 3$或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即$2 \times 2$。图5.1中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$0\times0+1\times1+3\times2+4\times3=19$。</p><div align=center><img width="250" src="https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640"/></div><div align=center>图5.1 二维互相关运算</div><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出：</p><script type="math/tex; mode=display">0\times0+1\times1+3\times2+4\times3=19,\\1\times0+2\times1+4\times2+5\times3=25,\\3\times0+4\times1+6\times2+7\times3=37,\\4\times0+5\times1+7\times2+8\times3=43.\\</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维互相关运算核心示例</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr2d</span><span class="params">(X, K)</span>:</span></span><br><span class="line">    H, W = X.shape</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(H - h + <span class="number">1</span>, W - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h2 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h2><p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二维卷积层的pytorch示例</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Conv2D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size)</span>:</span></span><br><span class="line">        super(Conv2D, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><h2 id="互相关运算与卷积运算"><a href="#互相关运算与卷积运算" class="headerlink" title="互相关运算与卷积运算"></a>互相关运算与卷积运算</h2><p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。</p><p>那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出图5.1中的核数组。设其他条件不变，使用卷积运算学出的核数组即图5.1中的核数组按上下、左右翻转。也就是说，输入与学出的已翻转的核数组再做卷积运算时，依然得到图5.1中的输出。为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。</p><h2 id="特征图和感受野"><a href="#特征图和感受野" class="headerlink" title="特征图和感受野"></a>特征图和感受野</h2><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图5.1中形状为$2 \times 2$的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为$2 \times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p><p>我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。当含义明确时，本书不对这两个术语做严格区分。</p><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><p>我们使用高和宽为3的输入与高和宽为2的卷积核得到高和宽为2的输出。一般来说，假设输入形状是$n_h\times n_w$，卷积核窗口形状是$k_h\times k_w$，那么输出形状将会是</p><script type="math/tex; mode=display">(n_h-k_h+1) \times (n_w-k_w+1).</script><p>所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。</p><div align='center'><img src="https://cdn.kesci.com/upload/image/q5nfl6ejy4.png?imageView2/0/w/640/h/640"></img>    </div><p>一般来说，如果在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是</p><script type="math/tex; mode=display">(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1),</script><p>也就是说，输出的高和宽会分别增加$p_h$和$p_w$。</p><p>在很多情况下，我们会设置$p_h=k_h-1$和$p_w=k_w-1$来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里$k_h$是奇数，我们会在高的两侧分别填充$p_h/2$行。如果$k_h$是偶数，一种可能是在输入的顶端一侧填充$\lceil p_h/2\rceil$行，而在底端一侧填充$\lfloor p_h/2\rfloor$行。在宽的两侧填充同理。</p><p>卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组<code>X</code>，设它的第<code>i</code>行第<code>j</code>列的元素为<code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出<code>Y[i,j]</code>是由输入以<code>X[i,j]</code>为中心的窗口同卷积核进行互相关计算得到的。</p><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。图5.3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图5.3中的阴影部分为输出元素及其计算所使用的输入和核数组元素：$0\times0+0\times1+1\times2+2\times3=8$、$0\times0+6\times1+0\times2+0\times3=6$。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nflohnqg.png?imageView2/0/w/640/h/640"/></div><div align=center>高和宽上步幅分别为3和2的二维互相关运算</div><p>一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为</p><script type="math/tex; mode=display">\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.</script><p>如果设置$p_h=k_h-1$和$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h/s_h) \times (n_w/s_w)$。</p><h2 id="多输入通道和多输出通道"><a href="#多输入通道和多输出通道" class="headerlink" title="多输入通道和多输出通道"></a>多输入通道和多输出通道</h2><p>输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\times h\times w$的多维数组。我们将大小为3的这一维称为通道（channel）维。本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。设卷积核窗口形状为$k_h\times k_w$。当$c_i=1$时，我们知道卷积核只包含一个形状为$k_h\times k_w$的二维数组。当$c_i &gt; 1$时，我们将会为每个输入通道各分配一个形状为$k_h\times k_w$的核数组。把这$c_i$个数组在输入通道维上连结，即得到一个形状为$c_i\times k_h\times k_w$的卷积核。由于输入和卷积核各有$c_i$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。</p><p>图5.4展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。图5.4中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nfmdnwbq.png?imageView2/0/w/640/h/640"/></div><div align=center>图5.4 含2个输入通道的互相关计算</div><h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3><p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\times k_h\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\times c_i\times k_h\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。</p><h2 id="1x1卷积层"><a href="#1x1卷积层" class="headerlink" title="1x1卷积层"></a>1x1卷积层</h2><p>卷积窗口形状为$1\times 1$（$k_h=k_w=1$）的多通道卷积层。我们通常称之为$1\times 1$卷积层，并将其中的卷积运算称为$1\times 1$卷积。因为使用了最小窗口，$1\times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times 1$卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的$1\times 1$卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么$1\times 1$卷积层的作用与全连接层等价</strong>。</p><div align=center><img width="400" src="https://cdn.kesci.com/upload/image/q5nfmq980r.png?imageView2/0/w/640/h/640"/></div><div align=center> 1x1卷积核的互相关计算。输入和输出具有相同的高和宽</div><h2 id="卷积层与全连接层的比较"><a href="#卷积层与全连接层的比较" class="headerlink" title="卷积层与全连接层的比较"></a>卷积层与全连接层的比较</h2><p>二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：</p><ul><li><p>一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。</p></li><li><p>二是卷积层的参数量更少。使用卷积层可以以较少的参数数量来处理更大的图像。</p></li></ul><h2 id="卷积层的pytorch实现"><a href="#卷积层的pytorch实现" class="headerlink" title="卷积层的pytorch实现"></a>卷积层的pytorch实现</h2><ul><li>in_channels (python:int) – Number of channels in the input imag</li><li>out_channels (python:int) – Number of channels produced by the convolution</li><li>kernel_size (python:int or tuple) – Size of the convolving kernel</li><li>stride (python:int or tuple, optional) – Stride of the convolution. Default: 1</li><li>padding (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0</li><li>bias (bool, optional) – If True, adds a learnable bias to the output. Default: True</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(in_channels=<span class="number">2</span>, out_channels=<span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), stride=<span class="number">1</span>, padding=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">Y = conv2d(X)</span><br><span class="line">print(<span class="string">'Y.shape: '</span>, Y.shape)</span><br><span class="line">print(<span class="string">'weight.shape: '</span>, conv2d.weight.shape)</span><br><span class="line">print(<span class="string">'bias.shape: '</span>, conv2d.bias.shape)</span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h3 id="二维池化层"><a href="#二维池化层" class="headerlink" title="二维池化层"></a>二维池化层</h3><ul><li>二维最大池化层<br>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。<br><div align=center><img width="300" src="https://cdn.kesci.com/upload/image/q5nfob3odo.png?imageView2/0/w/640/h/640"/></div><br><div align=center>图5.6 池化窗口形状为 2 x 2 的最大池化</div><br>池化窗口形状为$2\times 2$的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为2，其中的4个元素由取最大值运算$\text{max}$得出：</li></ul><script type="math/tex; mode=display">\max(0,1,3,4)=4,\\\max(1,2,4,5)=5,\\\max(3,4,6,7)=7,\\\max(4,5,7,8)=8.\\</script><ul><li>二维平均池化层</li></ul><p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \times q$的池化层称为$p \times q$池化层，其中的池化运算叫作$p \times q$池化。</p><p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为$2\times 2$最大池化的输入。设该卷积层输入是<code>X</code>、池化层输出为<code>Y</code>。无论是<code>X[i, j]</code>和<code>X[i, j+1]</code>值不同，还是<code>X[i, j+1]</code>和<code>X[i, j+2]</code>不同，池化层输出均有<code>Y[i, j]=1</code>。也就是说，使用$2\times 2$最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p><h3 id="池化层的pytorch实现"><a href="#池化层的pytorch实现" class="headerlink" title="池化层的pytorch实现"></a>池化层的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">32</span>, dtype=torch.float32).view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均池化层使用的是nn.AvgPool2d，使用方法与nn.MaxPool2d相同。</span></span><br><span class="line"></span><br><span class="line">pool2d = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">Y = pool2d(X)</span><br><span class="line">print(X)</span><br><span class="line">print(Y)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络基础&quot;&gt;&lt;a href=&quot;#卷积神经网络基础&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络基础&quot;&gt;&lt;/a&gt;卷积神经网络基础&lt;/h1&gt;&lt;h3 id=&quot;二维互相关运算&quot;&gt;&lt;a href=&quot;#二维互相关运算&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>LeNet、AlexNet、VGG、NiN、GoogLeNet</title>
    <link href="https://shyshy903.github.io/2020/02/17/Deep_learning/LeNet%E3%80%81AlexNet%E3%80%81VGG%E3%80%81NiN%E3%80%81GooLeNet/"/>
    <id>https://shyshy903.github.io/2020/02/17/Deep_learning/LeNet%E3%80%81AlexNet%E3%80%81VGG%E3%80%81NiN%E3%80%81GooLeNet/</id>
    <published>2020-02-16T16:00:00.000Z</published>
    <updated>2020-02-16T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LeNet、AlexNet、VGG、NiN、GoogLeNet"><a href="#LeNet、AlexNet、VGG、NiN、GoogLeNet" class="headerlink" title="LeNet、AlexNet、VGG、NiN、GoogLeNet"></a>LeNet、AlexNet、VGG、NiN、GoogLeNet</h1><h2 id="全连接层与卷积层的优势对比"><a href="#全连接层与卷积层的优势对比" class="headerlink" title="全连接层与卷积层的优势对比"></a>全连接层与卷积层的优势对比</h2><p>使用全连接层的局限性：</p><ul><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li><p>对于大尺寸的输入图像，使用全连接层容易导致模型过大。<br>使用卷积层的优势：</p></li><li><p>卷积层保留输入形状。</p></li><li>卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><h3 id="LeNet模型"><a href="#LeNet模型" class="headerlink" title="LeNet模型"></a>LeNet模型</h3>LeNet分为卷积层块和全连接层块两个部分。<br><div align=center><img width="600" src="https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960"/></div><br><div align=center>LeNet网络结构</div><br>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</li></ul><p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><h3 id="LeNet的pytorch实现"><a href="#LeNet的pytorch实现" class="headerlink" title="LeNet的pytorch实现"></a>LeNet的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"../"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Flatten</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment">#展平操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reshape</span><span class="params">(torch.nn.Module)</span>:</span> <span class="comment">#将图像大小重定型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x.view(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)      <span class="comment">#(B x C x H x W)</span></span><br><span class="line">    </span><br><span class="line">net = torch.nn.Sequential(     <span class="comment">#Lelet                                                  </span></span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment">#b*1*28*28  =&gt;b*6*28*28</span></span><br><span class="line">    nn.Sigmoid(),                                                       </span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                              <span class="comment">#b*6*28*28  =&gt;b*6*14*14</span></span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>),           <span class="comment">#b*6*14*14  =&gt;b*16*10*10</span></span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),                              <span class="comment">#b*16*10*10  =&gt; b*16*5*5</span></span><br><span class="line">    Flatten(),                                                          <span class="comment">#b*16*5*5   =&gt; b*400</span></span><br><span class="line">    nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>LeNet: 在大的真实数据集上的表现并不尽如⼈意。<br>1.神经网络计算复杂。<br>2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。</p><p>机器学习的特征提取:手工定义的特征提取函数<br>神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。</p><p>神经网络发展的限制:数据、硬件</p><h3 id="AlexNet模型"><a href="#AlexNet模型" class="headerlink" title="AlexNet模型"></a>AlexNet模型</h3><p>AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p><div align=center><img width="600" src="https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640"/></div><div align=center>AlexNet网络结构</div><p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p><p>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面我们来详细描述这些层的设计。</p><p>AlexNet第一层中的卷积窗口形状是$11\times11$。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到$5\times5$，之后全采用$3\times3$。此外，第一、第二和第五个卷积层之后都使用了窗口形状为$3\times3$、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。</p><p>紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。</p><p>第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p><p>第三，AlexNet通过丢弃法（参见3.13节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。</p><p>第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p><h3 id="AlexNet的pytorch实现"><a href="#AlexNet的pytorch实现" class="headerlink" title="AlexNet的pytorch实现"></a>AlexNet的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input/"</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment">#由于使用CPU镜像，精简网络，若为GPU镜像可添加该层</span></span><br><span class="line">            <span class="comment">#nn.Linear(4096, 4096),</span></span><br><span class="line">            <span class="comment">#nn.ReLU(),</span></span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img)</span>:</span></span><br><span class="line"></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br></pre></td></tr></table></figure><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><h3 id="VGG模型"><a href="#VGG模型" class="headerlink" title="VGG模型"></a>VGG模型</h3><p>VGG：通过重复使⽤简单的基础块来构建深度模型。<br>Block:数个相同的填充为1、窗口形状为的卷积层,接上一个步幅为2、窗口形状为的最大池化层。<br>卷积层保持输入的高和宽不变，而池化层则对其减半。</p><p><img src="https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640" alt="VGG"></p><p>与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个<code>vgg_block</code>，其超参数由变量<code>conv_arch</code>定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。</p><h3 id="VGG的实现"><a href="#VGG的实现" class="headerlink" title="VGG的实现"></a>VGG的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, in_channels, out_channels)</span>:</span> <span class="comment">#卷积层个数，输入通道数，输出通道数</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span><span class="params">(conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span>)</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> enumerate(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">"vgg_block_"</span> + str(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h2 id="NiN-网络中的网络）"><a href="#NiN-网络中的网络）" class="headerlink" title="NiN(网络中的网络）"></a>NiN(网络中的网络）</h2><p>LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。<br>NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。<br>⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。</p><h3 id="NiN模型"><a href="#NiN模型" class="headerlink" title="NiN模型"></a>NiN模型</h3><p><img src="https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960" alt="NiN"><br>1×1卷积核作用</p><ol><li>放缩通道数：通过控制卷积核的数量达到通道数的放缩。</li><li>增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。</li><li>计算参数少</li></ol><h3 id="NiN的pytorch实现"><a href="#NiN的pytorch实现" class="headerlink" title="NiN的pytorch实现"></a>NiN的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span><span class="params">(in_channels, out_channels, kernel_size, stride, padding)</span>:</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure><h2 id="GooLeNet"><a href="#GooLeNet" class="headerlink" title="GooLeNet"></a>GooLeNet</h2><ol><li>由Inception基础块组成。</li><li>Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。</li><li>可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</li></ol><h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><p><img src="https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640" alt="goolnet"><br>Inception块里有4条并行的线路。前3条线路使用窗口大小分别是$1\times 1$、$3\times 3$和$5\times 5$的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做$1\times 1$卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用$3\times 3$最大池化层，后接$1\times 1$卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p><p>Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。</p><h3 id="完整goolenet模型"><a href="#完整goolenet模型" class="headerlink" title="完整goolenet模型"></a>完整goolenet模型</h3><p>GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的$3\times 3$最大池化层来减小输出高宽。</p><ul><li>第一模块使用一个64通道的$7\times 7$卷积层。</li><li>第二模块使用2个卷积层：首先是64通道的$1\times 1$卷积层，然后是将通道增大3倍的$3\times 3$卷积层。它对应Inception块中的第二条线路。</li><li>第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为$64+128+32+32=256$，其中4条线路的输出通道数比例为$64:128:32:32=2:4:1:1$。其中第二、第三条线路先分别将输入通道数减小至$96/192=1/2$和$16/192=1/12$后，再接上第二层卷积层。第二个Inception块输出通道数增至$128+192+96+64=480$，每条线路的输出通道数之比为$128:192:96:64 = 4:6:3:2$。其中第二、第三条线路先分别将输入通道数减小至$128/256=1/2$和$32/256=1/8$</li><li>第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。这些线路的通道数分配和第三模块中的类似，首先含$3\times 3$卷积层的第二条线路输出最多通道，其次是仅含$1\times 1$卷积层的第一条线路，之后是含$5\times 5$卷积层的第三条线路和含$3\times 3$最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。</li><li>第五模块有输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。<br><div align=center><img width="500" src="https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640"/></div><br><div align=center>完整模型</div><h3 id="GooLeNet的pytorch"><a href="#GooLeNet的pytorch" class="headerlink" title="GooLeNet的pytorch"></a>GooLeNet的pytorch</h3></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># c1 - c4为每条线路里的层的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, c1, c2, c3, c4)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_c, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_c, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上连结输出</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, </span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children(): </span><br><span class="line">    X = blk(X)</span><br><span class="line">    print(<span class="string">'output shape: '</span>, X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batchsize=128</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line"><span class="comment">#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)</span></span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>卷积神经网络就是含卷积层的网络。</li><li>LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。</li><li>AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。</li><li>虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</li></ul><ul><li>VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。</li></ul><ul><li>NiN重复使用由卷积层和代替全连接层的$1\times 1$卷积层构成的NiN块来构建深层网络。</li><li>NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。</li><li><p>NiN的以上设计思想影响了后面一系列卷积神经网络的设计。</p></li><li><p>Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1\times 1$卷积层减少通道数从而降低模型复杂度。</p></li><li>GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li><li>GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;LeNet、AlexNet、VGG、NiN、GoogLeNet&quot;&gt;&lt;a href=&quot;#LeNet、AlexNet、VGG、NiN、GoogLeNet&quot; class=&quot;headerlink&quot; title=&quot;LeNet、AlexNet、VGG、NiN、GoogLeNe
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制（Attention)</title>
    <link href="https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</id>
    <published>2020-02-15T16:00:00.000Z</published>
    <updated>2020-02-15T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><p>在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。</p><p><img src="https://i.bmp.ovh/imgs/2020/02/0d5061d054b296a1.png" alt=""></p><p>与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。<br><img src="https://img-blog.csdnimg.cn/20200216222846156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="注意力机制框架"><a href="#注意力机制框架" class="headerlink" title="注意力机制框架"></a>注意力机制框架</h2><p>Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 Query  , attention layer得到输出与value的维度一致 . 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量则是value的加权求和，而每个key计算的权重与value一一对应。</p><p>为了计算输出，我们首先假设有一个函数 用于计算query和key的相似性，然后可以计算所有的 attention scores ${a_1, \ldots, a_n }$by</p><script type="math/tex; mode=display">a_i = \alpha(\mathbf q, \mathbf k_i)</script><p>我们使用 softmax函数 获得注意力权重：</p><script type="math/tex; mode=display">b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n)</script><p>最终的输出就是value的加权求和：</p><script type="math/tex; mode=display">\mathbf o = \sum_{i=1}^n b_i \mathbf v_i</script><p><img src="https://img-blog.csdnimg.cn/20200216224849212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。</p><h3 id="softmax的屏蔽"><a href="#softmax的屏蔽" class="headerlink" title="softmax的屏蔽"></a>softmax的屏蔽</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SequenceMask</span><span class="params">(X, X_len,value=<span class="number">-1e6</span>)</span>:</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )</span></span><br><span class="line">    mask = torch.arange((maxlen),dtype=torch.float)[<span class="literal">None</span>, :] &gt;= X_len[:, <span class="literal">None</span>]   </span><br><span class="line">    <span class="comment">#print(mask)</span></span><br><span class="line">    X[mask]=value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span><span class="params">(X, valid_length)</span>:</span></span><br><span class="line">    <span class="comment"># X: 3-D tensor, valid_length: 1-D or 2-D tensor</span></span><br><span class="line">    softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_length <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> softmax(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_length.dim() == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[<span class="number">1</span>], axis=<span class="number">0</span>))<span class="comment">#[2,2,3,3]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_length = valid_length.reshape((<span class="number">-1</span>,))</span><br><span class="line">        <span class="comment"># fill masked elements with a large negative, whose exp is 0</span></span><br><span class="line">        X = SequenceMask(X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>])), valid_length)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> softmax(X).reshape(shape)</span><br><span class="line"></span><br><span class="line">masked_softmax(torch.rand((<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),dtype=torch.float), torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>]))</span><br></pre></td></tr></table></figure><h3 id="超出二维矩阵的乘法"><a href="#超出二维矩阵的乘法" class="headerlink" title="超出二维矩阵的乘法"></a>超出二维矩阵的乘法</h3><p> X和  Y是维度分别为(b,n,m)和(b, m, k)的张量，进行 b次二维矩阵乘法后得到 , 维度为 (b, n, k)。</p><script type="math/tex; mode=display">Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,…,n\</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bmm(torch.ones((<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>), dtype = torch.float), torch.ones((<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>), dtype = torch.float))</span><br></pre></td></tr></table></figure><h2 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h2><p>The dot product 假设query和keys有相同的维度, 即 . 通过计算query和key转置的乘积来计算attention score,通常还会除去sqrt{d}减少计算出来的score对维度𝑑的依赖性，如下<br>𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \sqrt{d}<br>假设𝐐∈ℝ^{𝑚×𝑑}有m个query， 有n个keys. 我们可以通过矩阵运算的方式计算所有mn个score：<br>𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\sqrt{d}<br>现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dropout, **kwargs)</span>:</span></span><br><span class="line">        super(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># query: (batch_size, #queries, d)</span></span><br><span class="line">    <span class="comment"># key: (batch_size, #kv_pairs, d)</span></span><br><span class="line">    <span class="comment"># value: (batch_size, #kv_pairs, dim_v)</span></span><br><span class="line">    <span class="comment"># valid_length: either (batch_size, ) or (batch_size, xx)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length=None)</span>:</span></span><br><span class="line">        d = query.shape[<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># set transpose_b=True to swap the last two dimensions of key</span></span><br><span class="line">        </span><br><span class="line">        scores = torch.bmm(query, key.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        print(<span class="string">"attention_weight\n"</span>,attention_weights)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h2 id="多层感知机注意力"><a href="#多层感知机注意力" class="headerlink" title="多层感知机注意力"></a>多层感知机注意力</h2><p>将score函数定义:</p><script type="math/tex; mode=display">a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),</script><p>. 然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为 ℎ and 输出的size为 1 .隐层激活函数为tanh，无偏置.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save to the d2l package.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPAttention</span><span class="params">(nn.Module)</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units,ipt_dim,dropout, **kwargs)</span>:</span></span><br><span class="line">        super(MLPAttention, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Use flatten=True to keep query's and key's 3-D shapes.</span></span><br><span class="line">        self.W_k = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(ipt_dim, units, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v = nn.Linear(units, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, valid_length)</span>:</span></span><br><span class="line">        query, key = self.W_k(query), self.W_q(key)</span><br><span class="line">        <span class="comment">#print("size",query.size(),key.size())</span></span><br><span class="line">        <span class="comment"># expand query to (batch_size, #querys, 1, units), and key to</span></span><br><span class="line">        <span class="comment"># (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.</span></span><br><span class="line">        features = query.unsqueeze(<span class="number">2</span>) + key.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print("features:",features.size())  #--------------开启</span></span><br><span class="line">        scores = self.v(features).squeeze(<span class="number">-1</span>) </span><br><span class="line">        attention_weights = self.dropout(masked_softmax(scores, valid_length))</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(attention_weights, value)</span><br></pre></td></tr></table></figure><h3 id="计算背景变量"><a href="#计算背景变量" class="headerlink" title="计算背景变量"></a>计算背景变量</h3><p>我们先描述第一个关键点，即计算背景变量。图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。<br><img src="https://img-blog.csdnimg.cn/20200216230638792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>具体来说，令编码器在时间步$t$的隐藏状态为$\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t’$的背景变量为所有编码器隐藏状态的加权平均：</p><script type="math/tex; mode=display">\boldsymbol{c}_{t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,</script><p>其中给定$t’$时，权重$\alpha_{t’ t}$在$t=1,\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:</p><script type="math/tex; mode=display">\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.</script><p>现在，我们需要定义如何计算上式中softmax运算的输入$e_{t’ t}$。由于$e_{t’ t}$同时取决于解码器的时间步$t’$和编码器的时间步$t$，我们不妨以解码器在时间步$t’-1$的隐藏状态$\boldsymbol{s}_{t’ - 1}$与编码器在时间步$t$的隐藏状态$\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t’ t}$：</p><script type="math/tex; mode=display">e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).</script><p>这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：</p><script type="math/tex; mode=display">a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),</script><p>其中$\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$都是可以学习的模型参数。</p><h3 id="矢量化计算"><a href="#矢量化计算" class="headerlink" title="矢量化计算"></a>矢量化计算</h3><p>我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。</p><p>在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。<br>让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\boldsymbol{s}_{t’ - 1} \in \mathbb{R}^{h}$和编码器所有隐藏状态$\boldsymbol{h}_t \in \mathbb{R}^{h}, t = 1,\ldots,T$来计算背景向量$\boldsymbol{c}_{t’}\in \mathbb{R}^{h}$。<br>我们可以将查询项矩阵$\boldsymbol{Q} \in \mathbb{R}^{1 \times h}$设为$\boldsymbol{s}_{t’ - 1}^\top$，并令键项矩阵$\boldsymbol{K} \in \mathbb{R}^{T \times h}$和值项矩阵$\boldsymbol{V} \in \mathbb{R}^{T \times h}$相同且第$t$行均为$\boldsymbol{h}_t^\top$。此时，我们只需要通过矢量化计算</p><script type="math/tex; mode=display">\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}</script><p>即可算出转置后的背景向量$\boldsymbol{c}_{t’}^\top$。当查询项矩阵$\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。</p><h2 id="引入注意力机制的S2S"><a href="#引入注意力机制的S2S" class="headerlink" title="引入注意力机制的S2S"></a>引入注意力机制的S2S</h2><p>本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入拼接起来一起送到解码器：<br><img src="https://img-blog.csdnimg.cn/2020021623101586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构<br><img src="https://img-blog.csdnimg.cn/20200216231034160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU3ODAzMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:</p><ul><li>the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values</li><li>the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state</li><li>the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）  </li></ul><p>在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, enc_valid_len, *args)</span>:</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line"><span class="comment">#         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())</span></span><br><span class="line">        <span class="comment"># Transpose outputs to (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>), hidden_state, enc_valid_len)</span><br><span class="line">        <span class="comment">#outputs.swapaxes(0, 1)</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_len = state</span><br><span class="line">        <span class="comment">#("X.size",X.size())</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#         print("Xembeding.size2",X.size())</span></span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> l, x <span class="keyword">in</span> enumerate(X):</span><br><span class="line"><span class="comment">#             print(f"\n&#123;l&#125;-th token")</span></span><br><span class="line"><span class="comment">#             print("x.first.size()",x.size())</span></span><br><span class="line">            <span class="comment"># query shape: (batch_size, 1, hidden_size)</span></span><br><span class="line">            <span class="comment"># select hidden state of the last rnn layer as query</span></span><br><span class="line">            query = hidden_state[<span class="number">0</span>][<span class="number">-1</span>].unsqueeze(<span class="number">1</span>) <span class="comment"># np.expand_dims(hidden_state[0][-1], axis=1)</span></span><br><span class="line">            <span class="comment"># context has same shape as query</span></span><br><span class="line"><span class="comment">#             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())</span></span><br><span class="line">            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)</span><br><span class="line">            <span class="comment"># Concatenate on the feature dimension</span></span><br><span class="line"><span class="comment">#             print("context.size:",context.size())</span></span><br><span class="line">            x = torch.cat((context, x.unsqueeze(<span class="number">1</span>)), dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># Reshape x to (1, batch_size, embed_size+hidden_size)</span></span><br><span class="line"><span class="comment">#             print("rnn",x.size(), len(hidden_state))</span></span><br><span class="line">            out, hidden_state = self.rnn(x.transpose(<span class="number">0</span>,<span class="number">1</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.transpose(<span class="number">0</span>, <span class="number">1</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                        enc_valid_len]</span><br><span class="line"></span><br><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                            num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># encoder.initialize()</span></span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>,</span><br><span class="line">                                  num_hiddens=<span class="number">16</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>),dtype=torch.long)</span><br><span class="line">print(<span class="string">"batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n"</span>)</span><br><span class="line">print(<span class="string">'encoder output size:'</span>, encoder(X)[<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder hidden size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">0</span>].size())</span><br><span class="line">print(<span class="string">'encoder memory size:'</span>, encoder(X)[<span class="number">1</span>][<span class="number">1</span>].size())</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">out, state = decoder(X, state)</span><br><span class="line">out.shape, len(state), state[<span class="number">0</span>].shape, len(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;注意力机制&quot;&gt;&lt;a href=&quot;#注意力机制&quot; class=&quot;headerlink&quot; title=&quot;注意力机制&quot;&gt;&lt;/a&gt;注意力机制&lt;/h1&gt;&lt;p&gt;在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>模型选择与过拟合与欠拟合</title>
    <link href="https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
    <id>https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/</id>
    <published>2020-02-15T16:00:00.000Z</published>
    <updated>2020-02-15T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型选择与过拟合与欠拟合"><a href="#模型选择与过拟合与欠拟合" class="headerlink" title="模型选择与过拟合与欠拟合"></a>模型选择与过拟合与欠拟合</h1><h2 id="训练误差与泛化误差"><a href="#训练误差与泛化误差" class="headerlink" title="训练误差与泛化误差"></a>训练误差与泛化误差</h2><p>训练误差（training error）指模型在训练数据集上表现出的误差。<br>泛化误差（generalization error）指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。<br>计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。<br>所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。</p><p>机器学习模型应关注降低泛化误差。</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><p>测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。<br>预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）</p><h3 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h3><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是$K$折交叉验证（$K$-fold cross-validation）。在$K$折交叉验证中，我们把原始训练数据集分割成$K$个不重合的子数据集，然后我们做$K$次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他$K-1$个子数据集来训练模型。在这$K$次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这$K$次训练误差和验证误差分别求平均。</p><h2 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h2><ul><li>一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。 在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/cf7248d423fab8a5.png"/></div><div align=center>图3.4 模型复杂度对欠拟合和过拟合的影响</div><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p><h2 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h2><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/aca411216e606cf8.png"/></div><div align=center>正常拟合</div><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/4b81e7b5075cc20d.png"/></div><div align=center>欠拟合</div><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/6ee4f9ffe1f7059f.png"/></div><div align=center>过拟合</div><h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>上一节中我们观察了过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述$L_2$范数正则化，再解释它为何又称权重衰减。</p><p>$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以3.1节（线性回归）中的线性回归损失函数</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2</script><p>为例，其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为</p><script type="math/tex; mode=display">\ell(w_1, w_2, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2,</script><p>其中超参数$\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为</p><script type="math/tex; mode=display">\begin{aligned}w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).\end{aligned}</script><p>可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p><h3 id="权重衰减的pytorch实现"><a href="#权重衰减的pytorch实现" class="headerlink" title="权重衰减的pytorch实现"></a>权重衰减的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span><span class="params">(wd)</span>:</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line">    </span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line">            </span><br><span class="line">            l.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">'epochs'</span>, <span class="string">'loss'</span>,</span><br><span class="line">                 range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">    print(<span class="string">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>多层感知机的计算表达式为</p><script type="math/tex; mode=display">h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right)</script><p>这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$</p><script type="math/tex; mode=display">h_i' = \frac{\xi_i}{1-p} h_i</script><p>由于$E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display">E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即<strong>丢弃法不改变其输入的期望值</strong>。</p><div align=center><img width="350" src="https://i.bmp.ovh/imgs/2020/02/ec3bfe47de764684.png"/></div><div align=center> 隐藏层使用了丢弃法的多层感知机</div><h3 id="dropout的pytorch实现"><a href="#dropout的pytorch实现" class="headerlink" title="dropout的pytorch实现"></a>dropout的pytorch实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1),</span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2),</span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h1 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h1><p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(L)}$的权重参数为$\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（衰减）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p><h1 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h1><h2 id="PyTorch的默认随机初始化"><a href="#PyTorch的默认随机初始化" class="headerlink" title="PyTorch的默认随机初始化"></a>PyTorch的默认随机初始化</h2><p>随机初始化模型参数的方法有很多。使用<code>torch.nn.init.normal_()</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。不过，PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p><h2 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h2><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化[1]。<br>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).</script><p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><ul><li>正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</li><li>权重衰减等价于$L_2$范数正则化，通常会使学到的权重参数的元素较接近0。</li><li>权重衰减可以通过优化器中的<code>weight_decay</code>超参数来指定。</li><li>可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。</li><li>我们可以通过使用丢弃法应对过拟合。</li><li>丢弃法只在训练模型时使用</li><li>深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。</li><li>我们通常需要随机初始化神经网络的模型参数，如权重参数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;模型选择与过拟合与欠拟合&quot;&gt;&lt;a href=&quot;#模型选择与过拟合与欠拟合&quot; class=&quot;headerlink&quot; title=&quot;模型选择与过拟合与欠拟合&quot;&gt;&lt;/a&gt;模型选择与过拟合与欠拟合&lt;/h1&gt;&lt;h2 id=&quot;训练误差与泛化误差&quot;&gt;&lt;a href=&quot;#训练误差
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>机器翻译基础</title>
    <link href="https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    <id>https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/</id>
    <published>2020-02-15T16:00:00.000Z</published>
    <updated>2020-02-15T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><p>机器翻译是指将一段文本从一种语言自动翻译到另一种语言。因为一段文本序列在不同语言中的长度不一定相同，所以我们使用机器翻译为例来介绍编码器—解码器和注意力机制的应用。</p><h2 id="读取和预处理数据"><a href="#读取和预处理数据" class="headerlink" title="读取和预处理数据"></a>读取和预处理数据</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将数据集清洗、转化为神经网络的输入minbatch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%导入模块</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'.'</span>)</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_raw</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>)</span><br><span class="line">    out = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(text.lower()):</span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> (<span class="string">','</span>, <span class="string">'!'</span>, <span class="string">'.'</span>) <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> text[i<span class="number">-1</span>] != <span class="string">' '</span>:</span><br><span class="line">            out += <span class="string">' '</span></span><br><span class="line">        out += char</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_examples = <span class="number">50000</span></span><br><span class="line">source, target = [], []</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; num_examples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    parts = line.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">if</span> len(parts) &gt;= <span class="number">2</span>:</span><br><span class="line">        source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>))</span><br><span class="line">        target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">        </span><br><span class="line">source[<span class="number">0</span>:<span class="number">3</span>], target[<span class="number">0</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><h3 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/46ee15360205c7fc.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(tokens)</span>:</span></span><br><span class="line">    tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> d2l.data.base.Vocab(tokens, min_freq=<span class="number">3</span>, use_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">src_vocab = build_vocab(source)</span><br><span class="line">len(src_vocab)</span><br></pre></td></tr></table></figure><h3 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h3><p><img src="https://i.bmp.ovh/imgs/2020/02/a1e6875ba2581929.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span><span class="params">(line, max_len, padding_token)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(line) &gt; max_len:</span><br><span class="line">        <span class="keyword">return</span> line[:max_len]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (max_len - len(line))</span><br><span class="line">pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab.pad)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array</span><span class="params">(lines, vocab, max_len, is_source)</span>:</span></span><br><span class="line">    lines = [vocab[line] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_source:</span><br><span class="line">        lines = [[vocab.bos] + line + [vocab.eos] <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([pad(line, max_len, vocab.pad) <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab.pad).sum(<span class="number">1</span>) <span class="comment">#第一个维度</span></span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span><span class="params">(batch_size, max_len)</span>:</span> <span class="comment"># This function is saved in d2l.</span></span><br><span class="line">    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)</span><br><span class="line">    src_array, src_valid_len = build_array(source, src_vocab, max_len, <span class="literal">True</span>)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, <span class="literal">False</span>)</span><br><span class="line">    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    train_iter = data.DataLoader(train_data, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> src_vocab, tgt_vocab, train_iter</span><br></pre></td></tr></table></figure><h2 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h2><p>可以应用在对话系统、生成式任务中。<br><img src="https://i.bmp.ovh/imgs/2020/02/e025e4ae861f8d1c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, **kwargs)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_X, dec_X, *args)</span>:</span></span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p><img src="https://i.bmp.ovh/imgs/2020/02/95d2ba1473ca6471.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span><span class="params">(d2l.Encoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens=num_hiddens</span><br><span class="line">        self.num_layers=num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begin_state</span><span class="params">(self, batch_size, device)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),</span><br><span class="line">                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, *args)</span>:</span></span><br><span class="line">        X = self.embedding(X) <span class="comment"># X shape: (batch_size, seq_len, embed_size)</span></span><br><span class="line">        X = X.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># RNN needs first axes to be time</span></span><br><span class="line">        <span class="comment"># state = self.begin_state(X.shape[1], device=X.device)</span></span><br><span class="line">        out, state = self.rnn(X)</span><br><span class="line">        <span class="comment"># The shape of out is (seq_len, batch_size, num_hiddens).</span></span><br><span class="line">        <span class="comment"># state contains the hidden state and the memory cell</span></span><br><span class="line">        <span class="comment"># of the last time step, the shape is (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> out, state</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span><span class="params">(d2l.Decoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens,vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span><span class="params">(self, enc_outputs, *args)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X, state)</span>:</span></span><br><span class="line">        X = self.embedding(X).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        out, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># Make the batch to be the first dimension to simplify loss computation.</span></span><br><span class="line">        out = self.dense(out).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out, state</span><br></pre></td></tr></table></figure><h2 id="Beamsearch"><a href="#Beamsearch" class="headerlink" title="Beamsearch"></a>Beamsearch</h2><p><img src="attachment:image.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器翻译&quot;&gt;&lt;a href=&quot;#机器翻译&quot; class=&quot;headerlink&quot; title=&quot;机器翻译&quot;&gt;&lt;/a&gt;机器翻译&lt;/h1&gt;&lt;p&gt;机器翻译是指将一段文本从一种语言自动翻译到另一种语言。因为一段文本序列在不同语言中的长度不一定相同，所以我们使用机器翻译为例
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="https://shyshy903.github.io/2020/02/14/Deep_learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://shyshy903.github.io/2020/02/14/Deep_learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2020-02-13T16:00:00.000Z</published>
    <updated>2020-02-13T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="简单循环神经网络的构造"><a href="#简单循环神经网络的构造" class="headerlink" title="简单循环神经网络的构造"></a>简单循环神经网络的构造</h2><p><img src="https://img.vim-cn.com/a8/d90fe522138ebfb79547e687f5fd82684648fa.png" alt=""></p><h2 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h2><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量  g ，并设裁剪的阈值是 θ 。裁剪后的梯度</p><h2 id="循环神经网络的pytorch实现"><a href="#循环神经网络的pytorch实现" class="headerlink" title="循环神经网络的pytorch实现"></a>循环神经网络的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">num_steps, batch_size = <span class="number">35</span>, <span class="number">2</span></span><br><span class="line">X = torch.rand(num_steps, batch_size, vocab_size)</span><br><span class="line">state = <span class="literal">None</span></span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">print(Y.shape, state_new.shape)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_layer, vocab_size)</span>:</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, state)</span>:</span></span><br><span class="line">        <span class="comment"># inputs.shape: (batch_size, num_steps)</span></span><br><span class="line">        X = to_onehot(inputs, vocab_size)</span><br><span class="line">        X = torch.stack(X)  <span class="comment"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class="line">        hiddens, state = self.rnn(X, state)</span><br><span class="line">        hiddens = hiddens.view(<span class="number">-1</span>, hiddens.shape[<span class="number">-1</span>])  <span class="comment"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class="line">        output = self.dense(hiddens)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn_pytorch</span><span class="params">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class="line"><span class="function"><span class="params">                      char_to_idx)</span>:</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    output = [char_to_idx[prefix[<span class="number">0</span>]]]  <span class="comment"># output记录prefix加上预测的num_chars个字符</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars + len(prefix) - <span class="number">1</span>):</span><br><span class="line">        X = torch.tensor([output[<span class="number">-1</span>]], device=device).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        (Y, state) = model(X, state)  <span class="comment"># 前向计算不需要传入模型参数</span></span><br><span class="line">        <span class="keyword">if</span> t &lt; len(prefix) - <span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx[prefix[t + <span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(Y.argmax(dim=<span class="number">1</span>).item())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = RNNModel(rnn_layer, vocab_size).to(device)</span><br><span class="line">predict_rnn_pytorch(<span class="string">'分开'</span>, <span class="number">10</span>, model, vocab_size, device, idx_to_char, char_to_idx)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_predict_rnn_pytorch</span><span class="params">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class="line"><span class="function"><span class="params">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class="line"><span class="function"><span class="params">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        l_sum, n, start = <span class="number">0.0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class="comment"># 相邻采样</span></span><br><span class="line">        state = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> X, Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class="line">                <span class="keyword">if</span> isinstance (state, tuple): <span class="comment"># LSTM, state:(h, c)  </span></span><br><span class="line">                    state[<span class="number">0</span>].detach_()</span><br><span class="line">                    state[<span class="number">1</span>].detach_()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    state.detach_()</span><br><span class="line">            (output, state) = model(X, state) <span class="comment"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class="line">            y = torch.flatten(Y.T)</span><br><span class="line">            l = loss(output, y.long())</span><br><span class="line">            </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            l_sum += l.item() * y.shape[<span class="number">0</span>]</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % pred_period == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class="line">                epoch + <span class="number">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class="line">            <span class="keyword">for</span> prefix <span class="keyword">in</span> prefixes:</span><br><span class="line">                print(<span class="string">' -'</span>, predict_rnn_pytorch(</span><br><span class="line">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class="line">                    char_to_idx))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, batch_size, lr, clipping_theta = <span class="number">250</span>, <span class="number">32</span>, <span class="number">1e-3</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">50</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><p>RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）<br>⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系<br><img src="https://img.vim-cn.com/aa/5f1857a2a2320a5a6dd637d88c7018dffcbe43.png" alt="rnn1"></p><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><img src="https://img.vim-cn.com/7d/cdb99137827b9038f1e5f34593f7169bbc3d87.png" alt="gru"></p><ul><li>重置⻔有助于捕捉时间序列⾥短期的依赖关系；</li><li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul><li>长短期记忆long short-term memory :</li><li>遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入</li><li>输出门:控制从记忆细胞到隐藏状态</li><li>记忆细胞：⼀种特殊的隐藏状态的信息的流动<br><img src="https://img.vim-cn.com/de/2d40e304a6f05b02fc8747d5e2a6f947fc064a.png" alt="lstm"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="深度循环网络"><a href="#深度循环网络" class="headerlink" title="深度循环网络"></a>深度循环网络</h3><p>通过<code>num_layers</code>来进行控制</p><p><img src="https://img.vim-cn.com/d3/51bc59f0ae24e767576ee017fa06a031892f2b.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class="number">2</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h3 id="双向循环网络"><a href="#双向循环网络" class="headerlink" title="双向循环网络"></a>双向循环网络</h3><p><img src="https://img.vim-cn.com/4f/b21d208beaf51f1d33ef0455772236dc512ac0.png" alt=""></p><p>通过参数<code>bidirectional=True</code>来进行控制</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">128</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e-2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络&quot;&gt;&lt;a href=&quot;#循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络&quot;&gt;&lt;/a&gt;循环神经网络&lt;/h1&gt;&lt;h2 id=&quot;简单循环神经网络的构造&quot;&gt;&lt;a href=&quot;#简单循环神经网络的构造&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>文本预处理与语言模型</title>
    <link href="https://shyshy903.github.io/2020/02/14/Deep_learning/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://shyshy903.github.io/2020/02/14/Deep_learning/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-02-13T16:00:00.000Z</published>
    <updated>2020-02-13T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><ol><li>读入文本</li><li>分词</li><li>建立字典，将每个词映射到一个唯一的索引（index）</li><li>将文本从词的序列转换为索引的序列，方便输入模型</li></ol><h2 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_time_machine</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'/home/kesci/input/timemachine7163/timemachine.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = [re.sub(<span class="string">'[^a-z]+'</span>, <span class="string">' '</span>, line.strip().lower()) <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line">print(<span class="string">'# sentences %d'</span> % len(lines))</span><br></pre></td></tr></table></figure><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>将一个句子划分为若干个<code>token</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(sentences, token=<span class="string">'word'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Split sentences into word or char tokens"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'word'</span>:</span><br><span class="line">        <span class="keyword">return</span> [sentence.split(<span class="string">' '</span>) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>:</span><br><span class="line">        <span class="keyword">return</span> [list(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'ERROR: unkown token type '</span>+token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line">tokens[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><h2 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vocab</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, min_freq=<span class="number">0</span>, use_special_tokens=False)</span>:</span></span><br><span class="line">        counter = count_corpus(tokens)  <span class="comment"># : </span></span><br><span class="line">        self.token_freqs = list(counter.items())</span><br><span class="line">        self.idx_to_token = []</span><br><span class="line">        <span class="keyword">if</span> use_special_tokens:</span><br><span class="line">            <span class="comment"># padding, begin of sentence, end of sentence, unknown</span></span><br><span class="line">            self.pad, self.bos, self.eos, self.unk = (<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>, <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.unk = <span class="number">0</span></span><br><span class="line">            self.idx_to_token += [<span class="string">''</span>]</span><br><span class="line">        self.idx_to_token += [token <span class="keyword">for</span> token, freq <span class="keyword">in</span> self.token_freqs</span><br><span class="line">                        <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.idx_to_token]</span><br><span class="line">        self.token_to_idx = dict()</span><br><span class="line">        <span class="keyword">for</span> idx, token <span class="keyword">in</span> enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] = idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(tokens, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_tokens</span><span class="params">(self, indices)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(indices, (list, tuple)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_corpus</span><span class="params">(sentences)</span>:</span></span><br><span class="line">    tokens = [tk <span class="keyword">for</span> st <span class="keyword">in</span> sentences <span class="keyword">for</span> tk <span class="keyword">in</span> st]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)  <span class="comment"># 返回一个字典，记录每个词的出现次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将词转化为索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'words:'</span>, tokens[i])</span><br><span class="line">    print(<span class="string">'indices:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure><h2 id="用现有工具包分词"><a href="#用现有工具包分词" class="headerlink" title="用现有工具包分词"></a>用现有工具包分词</h2><h3 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"Mr. Chen doesn't agree with my suggestion."</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> data</span><br><span class="line">data.path.append(<span class="string">'/home/kesci/input/nltk_data3784/nltk_data'</span>)</span><br><span class="line">print(word_tokenize(text))</span><br></pre></td></tr></table></figure><h3 id="SPACY"><a href="#SPACY" class="headerlink" title="SPACY"></a>SPACY</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc])</span><br></pre></td></tr></table></figure><h1 id="语言模型（基于统计的语言模型）"><a href="#语言模型（基于统计的语言模型）" class="headerlink" title="语言模型（基于统计的语言模型）"></a>语言模型（基于统计的语言模型）</h1><p><img src="https://img.vim-cn.com/74/bb98fdc35dd04377837535271dcebd321dfa19.png" alt=""></p><h1 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h1><p><img src="https://img.vim-cn.com/14/04dcd7cd184f97d2cbaed69a419d20bdd74c96.png" alt=""></p><h2 id="相邻采样"><a href="#相邻采样" class="headerlink" title="相邻采样"></a>相邻采样</h2><p>在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻</p><h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><p>下面的代码每次从数据里随机采样一个小批量。其中批量大小<code>batch_size</code>是每个小批量的样本数，<code>num_steps</code>是每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;文本预处理&quot;&gt;&lt;a href=&quot;#文本预处理&quot; class=&quot;headerlink&quot; title=&quot;文本预处理&quot;&gt;&lt;/a&gt;文本预处理&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;读入文本&lt;/li&gt;
&lt;li&gt;分词&lt;/li&gt;
&lt;li&gt;建立字典，将每个词映射到一个唯一的索引（index）
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>多层感知机</title>
    <link href="https://shyshy903.github.io/2020/02/13/Deep_learning/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>https://shyshy903.github.io/2020/02/13/Deep_learning/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</id>
    <published>2020-02-12T16:00:00.000Z</published>
    <updated>2020-02-12T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。在本节中，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h2 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h2><p>多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。图3.3展示了一个多层感知机的神经网络图。</p><p><img src="https://img.vim-cn.com/2e/80d067a824cf71512d77c655855fe8c3488cc3.png" alt="带有隐藏层的多层感知机。它含有一个隐藏层，该层中有5个隐藏单元"></p><p>在图3.3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3.3中的多层感知机的层数为2。由图3.3可见，隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。</p><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><p>$$<br>\begin{aligned}<br>\boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\<br>\boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,<br>\end{aligned}<br>$$</p><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><p>$$<br>\boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><p>$$\text{ReLU}(x) = \max(x, 0).$$</p><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数<code>xyplot</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">'..'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xyplot</span><span class="params">(x_vals,y_vals,name)</span>:</span></span><br><span class="line">    x_vals=x_vals.detach().numpy() <span class="comment"># we can't directly use var.numpy() because varibles might </span></span><br><span class="line">    y_vals=y_vals.detach().numpy() <span class="comment"># already required grad.,thus using var.detach().numpy() </span></span><br><span class="line">    plt.plot(x_vals,y_vals) </span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(name+<span class="string">'(x)'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.nn.functional.relu(x)</span><br><span class="line">xyplot(x,y,<span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_2_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">"grad of relu"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_3_0.png" alt="png"></p><h3 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h3><p>sigmod函数可将元素的值变为0，1之间</p><p>$$\sigma(sigmod)= \frac{1}{1+exp^(-x)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.sigmoid(x)</span><br><span class="line">xyplot(x,y,<span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_5_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">'grad of sigmoid'</span>)</span><br></pre></td></tr></table></figure><p><img src="output_6_0.png" alt="png"></p><h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><p>tanh函数可以将元素的值变为-1，1之间<br>$$ tanh(x) = \frac{1-exp^(-2x)}{1+exp^(-2x)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=Variable(torch.arange(<span class="number">-8.0</span>,<span class="number">8.0</span>,<span class="number">0.1</span>,dtype=torch.float32).reshape(int(<span class="number">16</span>/<span class="number">0.1</span>),<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=torch.tanh(x)</span><br><span class="line">xyplot(x,y,<span class="string">"tanh"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_8_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x),retain_graph=<span class="literal">True</span>)</span><br><span class="line">xyplot(x,x.grad,<span class="string">"grad of tanh"</span>)</span><br></pre></td></tr></table></figure><p><img src="output_9_0.png" alt="png"></p><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h2 id="多层感知机的pytorch实现"><a href="#多层感知机的pytorch实现" class="headerlink" title="多层感知机的pytorch实现"></a>多层感知机的pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 输出如下</span></span><br><span class="line">epoch <span class="number">1</span>, loss <span class="number">0.0031</span>, train acc <span class="number">0.703</span>, test acc <span class="number">0.757</span></span><br><span class="line">epoch <span class="number">2</span>, loss <span class="number">0.0019</span>, train acc <span class="number">0.824</span>, test acc <span class="number">0.822</span></span><br><span class="line">epoch <span class="number">3</span>, loss <span class="number">0.0016</span>, train acc <span class="number">0.845</span>, test acc <span class="number">0.825</span></span><br><span class="line">epoch <span class="number">4</span>, loss <span class="number">0.0015</span>, train acc <span class="number">0.855</span>, test acc <span class="number">0.811</span></span><br><span class="line">epoch <span class="number">5</span>, loss <span class="number">0.0014</span>, train acc <span class="number">0.865</span>, test acc <span class="number">0.846</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;多层感知机&quot;&gt;&lt;a href=&quot;#多层感知机&quot; class=&quot;headerlink&quot; title=&quot;多层感知机&quot;&gt;&lt;/a&gt;多层感知机&lt;/h1&gt;&lt;p&gt;我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。在本节中，我们将以多
      
    
    </summary>
    
    
      <category term="deep_learning" scheme="https://shyshy903.github.io//categories/deep-learning/"/>
    
    
      <category term="DL" scheme="https://shyshy903.github.io//tags/DL/"/>
    
  </entry>
  
</feed>
