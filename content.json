{"meta":{"title":"问渠哪得清如许？","subtitle":"just do it","description":"https://img.vim-cn.com/33/461edfd8f8283ad38990aa7a83131b4494eb2c.jpg","author":"Haiyang Song","url":"https://shyshy903.github.io","root":"/"},"pages":[{"title":"archives","date":"2020-01-01T13:19:24.000Z","updated":"2020-01-01T13:19:24.000Z","comments":true,"path":"archives/index.html","permalink":"https://shyshy903.github.io/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-01-01T13:14:12.000Z","updated":"2020-01-01T13:14:12.000Z","comments":true,"path":"categories/index.html","permalink":"https://shyshy903.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-01T13:14:30.000Z","updated":"2020-01-01T13:14:30.000Z","comments":true,"path":"tags/index.html","permalink":"https://shyshy903.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LeNet、AlexNet、VGG、NiN、GoogLeNet","slug":"Deep_learning/LeNet、AlexNet、VGG、NiN、GooLeNet","date":"2020-02-16T16:00:00.000Z","updated":"2020-02-16T16:00:00.000Z","comments":true,"path":"2020/02/17/Deep_learning/LeNet、AlexNet、VGG、NiN、GooLeNet/","link":"","permalink":"https://shyshy903.github.io/2020/02/17/Deep_learning/LeNet%E3%80%81AlexNet%E3%80%81VGG%E3%80%81NiN%E3%80%81GooLeNet/","excerpt":"","text":"LeNet、AlexNet、VGG、NiN、GoogLeNet全连接层与卷积层的优势对比使用全连接层的局限性： 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。 对于大尺寸的输入图像，使用全连接层容易导致模型过大。使用卷积层的优势： 卷积层保留输入形状。 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大LeNetLeNet模型LeNet分为卷积层块和全连接层块两个部分。 LeNet网络结构卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为$2\\times 2$，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。 卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。 LeNet的pytorch实现123456789101112131415161718192021222324252627282930313233#importimport syssys.path.append(\"../\")import d2lzh1981 as d2limport torchimport torch.nn as nnimport torch.optim as optimimport timeclass Flatten(torch.nn.Module): #展平操作 def forward(self, x): return x.view(x.shape[0], -1)class Reshape(torch.nn.Module): #将图像大小重定型 def forward(self, x): return x.view(-1,1,28,28) #(B x C x H x W) net = torch.nn.Sequential( #Lelet Reshape(), nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28 =&gt;b*6*28*28 nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), #b*6*28*28 =&gt;b*6*14*14 nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), #b*6*14*14 =&gt;b*16*10*10 nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), #b*16*10*10 =&gt; b*16*5*5 Flatten(), #b*16*5*5 =&gt; b*400 nn.Linear(in_features=16*5*5, out_features=120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) AlexNetLeNet: 在大的真实数据集上的表现并不尽如⼈意。1.神经网络计算复杂。2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。 机器学习的特征提取:手工定义的特征提取函数神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。 神经网络发展的限制:数据、硬件 AlexNet模型AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。 AlexNet网络结构 AlexNet与LeNet的设计理念非常相似，但也有显著的区别。 第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面我们来详细描述这些层的设计。 AlexNet第一层中的卷积窗口形状是$11\\times11$。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到$5\\times5$，之后全采用$3\\times3$。此外，第一、第二和第五个卷积层之后都使用了窗口形状为$3\\times3$、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。 紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。 第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。 第三，AlexNet通过丢弃法（参见3.13节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。 第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。 AlexNet的pytorch实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import timeimport torchfrom torch import nn, optimimport torchvisionimport numpy as npimport syssys.path.append(\"/home/kesci/input/\") import d2lzh1981 as d2limport osimport torch.nn.functional as Fdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')class AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.conv = nn.Sequential( nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding nn.ReLU(), nn.MaxPool2d(3, 2), # kernel_size, stride # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数 nn.Conv2d(96, 256, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(3, 2), # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。 # 前两个卷积层后不使用池化层来减小输入的高和宽 nn.Conv2d(256, 384, 3, 1, 1), nn.ReLU(), nn.Conv2d(384, 384, 3, 1, 1), nn.ReLU(), nn.Conv2d(384, 256, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(3, 2) ) # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合 self.fc = nn.Sequential( nn.Linear(256*5*5, 4096), nn.ReLU(), nn.Dropout(0.5), #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层 #nn.Linear(4096, 4096), #nn.ReLU(), #nn.Dropout(0.5), # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000 nn.Linear(4096, 10), ) def forward(self, img): feature = self.conv(img) output = self.fc(feature.view(img.shape[0], -1)) VGGVGG模型VGG：通过重复使⽤简单的基础块来构建深度模型。Block:数个相同的填充为1、窗口形状为的卷积层,接上一个步幅为2、窗口形状为的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。 与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。 VGG的实现12345678910111213141516171819202122232425262728def vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数 blk = [] for i in range(num_convs): if i == 0: blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) else: blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)) blk.append(nn.ReLU()) blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半 return nn.Sequential(*blk)def vgg(conv_arch, fc_features, fc_hidden_units=4096): net = nn.Sequential() # 卷积层部分 for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch): # 每经过一个vgg_block都会使宽高减半 net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels)) # 全连接层部分 net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(), nn.Linear(fc_features, fc_hidden_units), nn.ReLU(), nn.Dropout(0.5), nn.Linear(fc_hidden_units, fc_hidden_units), nn.ReLU(), nn.Dropout(0.5), nn.Linear(fc_hidden_units, 10) )) return net NiN(网络中的网络）LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。 NiN模型1×1卷积核作用 放缩通道数：通过控制卷积核的数量达到通道数的放缩。 增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。 计算参数少 NiN的pytorch实现1234567891011121314151617181920212223242526272829def nin_block(in_channels, out_channels, kernel_size, stride, padding): blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) return blkclass GlobalAvgPool2d(nn.Module): # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现 def __init__(self): super(GlobalAvgPool2d, self).__init__() def forward(self, x): return F.avg_pool2d(x, kernel_size=x.size()[2:])net = nn.Sequential( nin_block(1, 96, kernel_size=11, stride=4, padding=0), nn.MaxPool2d(kernel_size=3, stride=2), nin_block(96, 256, kernel_size=5, stride=1, padding=2), nn.MaxPool2d(kernel_size=3, stride=2), nin_block(256, 384, kernel_size=3, stride=1, padding=1), nn.MaxPool2d(kernel_size=3, stride=2), nn.Dropout(0.5), # 标签类别数是10 nin_block(384, 10, kernel_size=3, stride=1, padding=1), GlobalAvgPool2d(), # 将四维的输出转成二维的输出，其形状为(批量大小, 10) d2l.FlattenLayer()) GooLeNet 由Inception基础块组成。 Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 Inception块Inception块里有4条并行的线路。前3条线路使用窗口大小分别是$1\\times 1$、$3\\times 3$和$5\\times 5$的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做$1\\times 1$卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用$3\\times 3$最大池化层，后接$1\\times 1$卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。 Inception块中可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 完整goolenet模型GoogLeNet跟VGG一样，在主体卷积部分中使用5个模块（block），每个模块之间使用步幅为2的$3\\times 3$最大池化层来减小输出高宽。 第一模块使用一个64通道的$7\\times 7$卷积层。 第二模块使用2个卷积层：首先是64通道的$1\\times 1$卷积层，然后是将通道增大3倍的$3\\times 3$卷积层。它对应Inception块中的第二条线路。 第三模块串联2个完整的Inception块。第一个Inception块的输出通道数为$64+128+32+32=256$，其中4条线路的输出通道数比例为$64:128:32:32=2:4:1:1$。其中第二、第三条线路先分别将输入通道数减小至$96/192=1/2$和$16/192=1/12$后，再接上第二层卷积层。第二个Inception块输出通道数增至$128+192+96+64=480$，每条线路的输出通道数之比为$128:192:96:64 = 4:6:3:2$。其中第二、第三条线路先分别将输入通道数减小至$128/256=1/2$和$32/256=1/8$ 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是$192+208+48+64=512$、$160+224+64+64=512$、$128+256+64+64=512$、$112+288+64+64=528$和$256+320+128+128=832$。这些线路的通道数分配和第三模块中的类似，首先含$3\\times 3$卷积层的第二条线路输出最多通道，其次是仅含$1\\times 1$卷积层的第一条线路，之后是含$5\\times 5$卷积层的第三条线路和含$3\\times 3$最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。 第五模块有输出通道数为$256+320+128+128=832$和$384+384+128+128=1024$的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。 完整模型GooLeNet的pytorch 12345678910111213141516171819202122class Inception(nn.Module): # c1 - c4为每条线路里的层的输出通道数 def __init__(self, in_c, c1, c2, c3, c4): super(Inception, self).__init__() # 线路1，单1 x 1卷积层 self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1) # 线路2，1 x 1卷积层后接3 x 3卷积层 self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1 x 1卷积层后接5 x 5卷积层 self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3 x 3最大池化层后接1 x 1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) return torch.cat((p1, p2, p3, p4), dim=1) # 在通道维上连结输出 123456789101112131415161718192021222324252627282930313233343536373839404142b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), d2l.GlobalAvgPool2d())net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))X = torch.rand(1, 1, 96, 96)for blk in net.children(): X = blk(X) print('output shape: ', X.shape)#batchsize=128batch_size = 16# 如出现“out of memory”的报错信息，可减小batch_size或resize#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)lr, num_epochs = 0.001, 5optimizer = torch.optim.Adam(net.parameters(), lr=lr)d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs) 小结 卷积神经网络就是含卷积层的网络。 LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。 AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。 虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。 VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。 NiN重复使用由卷积层和代替全连接层的$1\\times 1$卷积层构成的NiN块来构建深层网络。 NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。 NiN的以上设计思想影响了后面一系列卷积神经网络的设计。 Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1\\times 1$卷积层减少通道数从而降低模型复杂度。 GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。 GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"Transformer","slug":"Deep_learning/Transformer","date":"2020-02-16T16:00:00.000Z","updated":"2020-02-16T16:00:00.000Z","comments":true,"path":"2020/02/17/Deep_learning/Transformer/","link":"","permalink":"https://shyshy903.github.io/2020/02/17/Deep_learning/Transformer/","excerpt":"","text":"Transformertransformer模型在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾： CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。 RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。为了整合CNN和RNN的优势，[Vaswani et al., 2017] 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。 Transformer同样基于编码器-解码器架构，其区别主要在于以下三点： Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。 Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。 Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。 transformer的pytorch实现123456789import osimport mathimport numpy as npimport torch import torch.nn as nnimport torch.nn.functional as Fimport syssys.path.append('../')import d2l 123456789101112131415161718192021222324252627282930313233343536373839404142434445def SequenceMask(X, X_len,value=-1e6): maxlen = X.size(1) X_len = X_len.to(X.device) #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] ) mask = torch.arange((maxlen), dtype=torch.float, device=X.device) mask = mask[None, :] &lt; X_len[:, None] #print(mask) X[~mask]=value return Xdef masked_softmax(X, valid_length): # X: 3-D tensor, valid_length: 1-D or 2-D tensor softmax = nn.Softmax(dim=-1) if valid_length is None: return softmax(X) else: shape = X.shape if valid_length.dim() == 1: try: valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3] except: valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3] else: valid_length = valid_length.reshape((-1,)) # fill masked elements with a large negative, whose exp is 0 X = SequenceMask(X.reshape((-1, shape[-1])), valid_length) return softmax(X).reshape(shape)# Save to the d2l package.class DotProductAttention(nn.Module): def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # query: (batch_size, #queries, d) # key: (batch_size, #kv_pairs, d) # value: (batch_size, #kv_pairs, dim_v) # valid_length: either (batch_size, ) or (batch_size, xx) def forward(self, query, key, value, valid_length=None): d = query.shape[-1] # set transpose_b=True to swap the last two dimensions of key scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d) attention_weights = self.dropout(masked_softmax(scores, valid_length)) return torch.bmm(attention_weights, value) 多头注意力层多头注意力模型在我们讨论多头注意力层之前，先来迅速理解以下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。 多头注意力层h包含个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。 ### 多头注意力pytorch 1234567891011121314151617181920212223242526272829303132333435363738class MultiHeadAttention(nn.Module): def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs): super(MultiHeadAttention, self).__init__(**kwargs) self.num_heads = num_heads self.attention = DotProductAttention(dropout) self.W_q = nn.Linear(input_size, hidden_size, bias=False) self.W_k = nn.Linear(input_size, hidden_size, bias=False) self.W_v = nn.Linear(input_size, hidden_size, bias=False) self.W_o = nn.Linear(hidden_size, hidden_size, bias=False) def forward(self, query, key, value, valid_length): # query, key, and value shape: (batch_size, seq_len, dim), # where seq_len is the length of input sequence # valid_length shape is either (batch_size, ) # or (batch_size, seq_len). # Project and transpose query, key, and value from # (batch_size, seq_len, hidden_size * num_heads) to # (batch_size * num_heads, seq_len, hidden_size). query = transpose_qkv(self.W_q(query), self.num_heads) key = transpose_qkv(self.W_k(key), self.num_heads) value = transpose_qkv(self.W_v(value), self.num_heads) if valid_length is not None: # Copy valid_length by num_heads times device = valid_length.device valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy() if valid_length.ndim == 1: valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads)) else: valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1))) valid_length = valid_length.to(device) output = self.attention(query, key, value, valid_length) output_concat = transpose_output(output, self.num_heads) return self.W_o(output_concat) 12345678910111213141516171819202122def transpose_qkv(X, num_heads): # Original X shape: (batch_size, seq_len, hidden_size * num_heads), # -1 means inferring its value, after first reshape, X shape: # (batch_size, seq_len, num_heads, hidden_size) X = X.view(X.shape[0], X.shape[1], num_heads, -1) # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size) X = X.transpose(2, 1).contiguous() # Merge the first two dimensions. Use reverse=True to infer shape from # right to left. # output shape: (batch_size * num_heads, seq_len, hidden_size) output = X.view(-1, X.shape[2], X.shape[3]) return output# Saved in the d2l package for later usedef transpose_output(X, num_heads): # A reversed version of transpose_qkv X = X.view(-1, num_heads, X.shape[1], X.shape[2]) X = X.transpose(2, 1).contiguous() return X.view(X.shape[0], X.shape[1], -1) ## 基于位置的前馈网络（FFN) Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。 下面我们来实现PositionWiseFFN： 123456789class PositionWiseFFN(nn.Module): def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.ffn_1 = nn.Linear(input_size, ffn_hidden_size) self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out) def forward(self, X): return self.ffn_2(F.relu(self.ffn_1(X))) ## Add and Norm 除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。 (ref) 1234567891011121314layernorm = nn.LayerNorm(normalized_shape=2, elementwise_affine=True)batchnorm = nn.BatchNorm1d(num_features=2, affine=True)X = torch.FloatTensor([[1,2], [3,4]])print('layer norm:', layernorm(X))print('batch norm:', batchnorm(X))class AddNorm(nn.Module): def __init__(self, hidden_size, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.norm = nn.LayerNorm(hidden_size) def forward(self, X, Y): return self.norm(self.dropout(Y) + X) ## 位置编码 与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。 12345678910111213141516class PositionalEncoding(nn.Module): def __init__(self, embedding_size, dropout, max_len=1000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(dropout) self.P = np.zeros((1, max_len, embedding_size)) X = np.arange(0, max_len).reshape(-1, 1) / np.power( 10000, np.arange(0, embedding_size, 2)/embedding_size) self.P[:, :, 0::2] = np.sin(X) self.P[:, :, 1::2] = np.cos(X) self.P = torch.FloatTensor(self.P) def forward(self, X): if X.is_cuda and not self.P.is_cuda: self.P = self.P.cuda() X = X + self.P[:, :X.shape[1], :] return self.dropout(X) 编码器(Encoder)我们已经有了组成Transformer的各个模块，现在我们可以开始搭建了！编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，我们的输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为我们要将前一层的输出与原始输入相加并归一化。 123456789101112class EncoderBlock(nn.Module): def __init__(self, embedding_size, ffn_hidden_size, num_heads, dropout, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout) self.addnorm_1 = AddNorm(embedding_size, dropout) self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size) self.addnorm_2 = AddNorm(embedding_size, dropout) def forward(self, X, valid_length): Y = self.addnorm_1(X, self.attention(X, X, X, valid_length)) return self.addnorm_2(Y, self.ffn(Y)) 现在我们来实现整个Transformer 编码器模型，整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以$\\sqrt{d}$以防止其值过小。 123456789101112131415161718class TransformerEncoder(d2l.Encoder): def __init__(self, vocab_size, embedding_size, ffn_hidden_size, num_heads, num_layers, dropout, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.embedding_size = embedding_size self.embed = nn.Embedding(vocab_size, embedding_size) self.pos_encoding = PositionalEncoding(embedding_size, dropout) self.blks = nn.ModuleList() for i in range(num_layers): self.blks.append( EncoderBlock(embedding_size, ffn_hidden_size, num_heads, dropout)) def forward(self, X, valid_length, *args): X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size)) for blk in self.blks: X = blk(X, valid_length) return X 解码器（Decoder)Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class DecoderBlock(nn.Module): def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout) self.addnorm_1 = AddNorm(embedding_size, dropout) self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout) self.addnorm_2 = AddNorm(embedding_size, dropout) self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size) self.addnorm_3 = AddNorm(embedding_size, dropout) def forward(self, X, state): enc_outputs, enc_valid_length = state[0], state[1] # state[2][self.i] stores all the previous t-1 query state of layer-i # len(state[2]) = num_layers # If training: # state[2] is useless. # If predicting: # In the t-th timestep: # state[2][self.i].shape = (batch_size, t-1, hidden_size) # Demo: # love dogs ! [EOS] # | | | | # Transformer # Decoder # | | | | # I love dogs ! if state[2][self.i] is None: key_values = X else: # shape of key_values = (batch_size, t, hidden_size) key_values = torch.cat((state[2][self.i], X), dim=1) state[2][self.i] = key_values if self.training: batch_size, seq_len, _ = X.shape # Shape: (batch_size, seq_len), the values in the j-th column are j+1 valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) valid_length = valid_length.to(X.device) else: valid_length = None X2 = self.attention_1(X, key_values, key_values, valid_length) Y = self.addnorm_1(X, X2) Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length) Z = self.addnorm_2(Y, Y2) return self.addnorm_3(Z, self.ffn(Z)), state 对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。下面让我们来实现一下Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。 1234567891011121314151617181920212223class TransformerDecoder(d2l.Decoder): def __init__(self, vocab_size, embedding_size, ffn_hidden_size, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.embedding_size = embedding_size self.num_layers = num_layers self.embed = nn.Embedding(vocab_size, embedding_size) self.pos_encoding = PositionalEncoding(embedding_size, dropout) self.blks = nn.ModuleList() for i in range(num_layers): self.blks.append( DecoderBlock(embedding_size, ffn_hidden_size, num_heads, dropout, i)) self.dense = nn.Linear(embedding_size, vocab_size) def init_state(self, enc_outputs, enc_valid_length, *args): return [enc_outputs, enc_valid_length, [None]*self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size)) for blk in self.blks: X, state = blk(X, state) return self.dense(X), state 小结 Transformer同样基于编码器-解码器架构 Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。 dd and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。 osition encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。 12","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"卷积神经网络基础（CNN)","slug":"Deep_learning/卷积神经网络基础（CNN)","date":"2020-02-16T16:00:00.000Z","updated":"2020-02-16T16:00:00.000Z","comments":true,"path":"2020/02/17/Deep_learning/卷积神经网络基础（CNN)/","link":"","permalink":"https://shyshy903.github.io/2020/02/17/Deep_learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88CNN)/","excerpt":"","text":"卷积神经网络基础二维互相关运算虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为$3 \\times 3$或（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即$2 \\times 2$。图5.1中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$。 图5.1 二维互相关运算 在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。图5.1中的输出数组高和宽分别为2，其中的4个元素由二维互相关运算得出： 0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\ 1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\ 3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\ 4\\times0+5\\times1+7\\times2+8\\times3=43.\\\\12345678910111213# 二维互相关运算核心示例import torch import torch.nn as nndef corr2d(X, K): H, W = X.shape h, w = K.shape Y = torch.zeros(H - h + 1, W - w + 1) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i, j] = (X[i: i + h, j: j + w] * K).sum() return Y 二维卷积层二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。 12345678910# 二维卷积层的pytorch示例class Conv2D(nn.Module): def __init__(self, kernel_size): super(Conv2D, self).__init__() self.weight = nn.Parameter(torch.randn(kernel_size)) self.bias = nn.Parameter(torch.randn(1)) def forward(self, x): return corr2d(x, self.weight) + self.bias 互相关运算与卷积运算实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。 那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。为了解释这一点，假设卷积层使用互相关运算学出图5.1中的核数组。设其他条件不变，使用卷积运算学出的核数组即图5.1中的核数组按上下、左右翻转。也就是说，输入与学出的已翻转的核数组再做卷积运算时，依然得到图5.1中的输出。为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。 特征图和感受野二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图5.1中形状为$2 \\times 2$的输出记为$Y$，并考虑一个更深的卷积神经网络：将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。 我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。当含义明确时，本书不对这两个术语做严格区分。 填充和步幅我们使用高和宽为3的输入与高和宽为2的卷积核得到高和宽为2的输出。一般来说，假设输入形状是$n_h\\times n_w$，卷积核窗口形状是$k_h\\times k_w$，那么输出形状将会是 (n_h-k_h+1) \\times (n_w-k_w+1).所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。 填充填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。 一般来说，如果在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，那么输出形状将会是 (n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1),也就是说，输出的高和宽会分别增加$p_h$和$p_w$。 在很多情况下，我们会设置$p_h=k_h-1$和$p_w=k_w-1$来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里$k_h$是奇数，我们会在高的两侧分别填充$p_h/2$行。如果$k_h$是偶数，一种可能是在输入的顶端一侧填充$\\lceil p_h/2\\rceil$行，而在底端一侧填充$\\lfloor p_h/2\\rfloor$行。在宽的两侧填充同理。 卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组X，设它的第i行第j列的元素为X[i,j]。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出Y[i,j]是由输入以X[i,j]为中心的窗口同卷积核进行互相关计算得到的。 步幅卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。 目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。图5.3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出。图5.3中的阴影部分为输出元素及其计算所使用的输入和核数组元素：$0\\times0+0\\times1+1\\times2+2\\times3=8$、$0\\times0+6\\times1+0\\times2+0\\times3=6$。 高和宽上步幅分别为3和2的二维互相关运算 一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为 \\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.如果设置$p_h=k_h-1$和$p_w=k_w-1$，那么输出形状将简化为$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h/s_h) \\times (n_w/s_w)$。 多输入通道和多输出通道输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\\times h\\times w$的多维数组。我们将大小为3的这一维称为通道（channel）维。本节我们将介绍含多个输入通道或多个输出通道的卷积核。 多输入通道当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。假设输入数据的通道数为$c_i$，那么卷积核的输入通道数同样为$c_i$。设卷积核窗口形状为$k_h\\times k_w$。当$c_i=1$时，我们知道卷积核只包含一个形状为$k_h\\times k_w$的二维数组。当$c_i &gt; 1$时，我们将会为每个输入通道各分配一个形状为$k_h\\times k_w$的核数组。把这$c_i$个数组在输入通道维上连结，即得到一个形状为$c_i\\times k_h\\times k_w$的卷积核。由于输入和卷积核各有$c_i$个通道，我们可以在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组。这就是含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出。 图5.4展示了含2个输入通道的二维互相关计算的例子。在每个通道上，二维输入数组与二维核数组做互相关运算，再按通道相加即得到输出。图5.4中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$。 图5.4 含2个输入通道的互相关计算 多输出通道当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。 1x1卷积层卷积窗口形状为$1\\times 1$（$k_h=k_w=1$）的多通道卷积层。我们通常称之为$1\\times 1$卷积层，并将其中的卷积运算称为$1\\times 1$卷积。因为使用了最小窗口，$1\\times 1$卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\\times 1$卷积的主要计算发生在通道维上。图5.5展示了使用输入通道数为3、输出通道数为2的$1\\times 1$卷积核的互相关计算。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\\times 1$卷积层的作用与全连接层等价。 1x1卷积核的互相关计算。输入和输出具有相同的高和宽 卷积层与全连接层的比较二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势： 一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。 二是卷积层的参数量更少。使用卷积层可以以较少的参数数量来处理更大的图像。 卷积层的pytorch实现 in_channels (python:int) – Number of channels in the input imag out_channels (python:int) – Number of channels produced by the convolution kernel_size (python:int or tuple) – Size of the convolving kernel stride (python:int or tuple, optional) – Stride of the convolution. Default: 1 padding (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0 bias (bool, optional) – If True, adds a learnable bias to the output. Default: True 12345678X = torch.rand(4, 2, 3, 5)print(X.shape)conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))Y = conv2d(X)print('Y.shape: ', Y.shape)print('weight.shape: ', conv2d.weight.shape)print('bias.shape: ', conv2d.bias.shape) 池化层二维池化层 二维最大池化层同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。 图5.6 池化窗口形状为 2 x 2 的最大池化池化窗口形状为$2\\times 2$的最大池化，阴影部分为第一个输出元素及其计算所使用的输入元素。输出数组的高和宽分别为2，其中的4个元素由取最大值运算$\\text{max}$得出： \\max(0,1,3,4)=4,\\\\ \\max(1,2,4,5)=5,\\\\ \\max(3,4,6,7)=7,\\\\ \\max(4,5,7,8)=8.\\\\ 二维平均池化层 二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \\times q$的池化层称为$p \\times q$池化层，其中的池化运算叫作$p \\times q$池化。 让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为$2\\times 2$最大池化的输入。设该卷积层输入是X、池化层输出为Y。无论是X[i, j]和X[i, j+1]值不同，还是X[i, j+1]和X[i, j+2]不同，池化层输出均有Y[i, j]=1。也就是说，使用$2\\times 2$最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。 池化层的pytorch实现12345678X = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)# 平均池化层使用的是nn.AvgPool2d，使用方法与nn.MaxPool2d相同。pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))Y = pool2d(X)print(X)print(Y)","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"注意力机制（Attention)","slug":"Deep_learning/注意力机制","date":"2020-02-15T16:00:00.000Z","updated":"2020-02-15T16:00:00.000Z","comments":true,"path":"2020/02/16/Deep_learning/注意力机制/","link":"","permalink":"https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","excerpt":"","text":"注意力机制在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。 与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。 注意力机制框架Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。 Query , attention layer得到输出与value的维度一致 . 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量则是value的加权求和，而每个key计算的权重与value一一对应。 为了计算输出，我们首先假设有一个函数 用于计算query和key的相似性，然后可以计算所有的 attention scores ${a_1, \\ldots, a_n }$by a_i = \\alpha(\\mathbf q, \\mathbf k_i)我们使用 softmax函数 获得注意力权重： b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n)最终的输出就是value的加权求和： \\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i 不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。 softmax的屏蔽123456789101112131415161718192021222324252627def SequenceMask(X, X_len,value=-1e6): maxlen = X.size(1) #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] ) mask = torch.arange((maxlen),dtype=torch.float)[None, :] &gt;= X_len[:, None] #print(mask) X[mask]=value return Xdef masked_softmax(X, valid_length): # X: 3-D tensor, valid_length: 1-D or 2-D tensor softmax = nn.Softmax(dim=-1) if valid_length is None: return softmax(X) else: shape = X.shape if valid_length.dim() == 1: try: valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3] except: valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3] else: valid_length = valid_length.reshape((-1,)) # fill masked elements with a large negative, whose exp is 0 X = SequenceMask(X.reshape((-1, shape[-1])), valid_length) return softmax(X).reshape(shape)masked_softmax(torch.rand((2,2,4),dtype=torch.float), torch.FloatTensor([2,3])) 超出二维矩阵的乘法 X和 Y是维度分别为(b,n,m)和(b, m, k)的张量，进行 b次二维矩阵乘法后得到 , 维度为 (b, n, k)。 Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\\qquad for\\ i= 1,…,n\\1torch.bmm(torch.ones((2,1,3), dtype = torch.float), torch.ones((2,3,2), dtype = torch.float)) 点积注意力The dot product 假设query和keys有相同的维度, 即 . 通过计算query和key转置的乘积来计算attention score,通常还会除去sqrt{d}减少计算出来的score对维度𝑑的依赖性，如下𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d}假设𝐐∈ℝ^{𝑚×𝑑}有m个query， 有n个keys. 我们可以通过矩阵运算的方式计算所有mn个score：𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重. 1234567891011121314151617class DotProductAttention(nn.Module): def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # query: (batch_size, #queries, d) # key: (batch_size, #kv_pairs, d) # value: (batch_size, #kv_pairs, dim_v) # valid_length: either (batch_size, ) or (batch_size, xx) def forward(self, query, key, value, valid_length=None): d = query.shape[-1] # set transpose_b=True to swap the last two dimensions of key scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d) attention_weights = self.dropout(masked_softmax(scores, valid_length)) print(\"attention_weight\\n\",attention_weights) return torch.bmm(attention_weights, value) 多层感知机注意力将score函数定义: a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}),. 然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为 ℎ and 输出的size为 1 .隐层激活函数为tanh，无偏置. 1234567891011121314151617181920# Save to the d2l package.class MLPAttention(nn.Module): def __init__(self, units,ipt_dim,dropout, **kwargs): super(MLPAttention, self).__init__(**kwargs) # Use flatten=True to keep query's and key's 3-D shapes. self.W_k = nn.Linear(ipt_dim, units, bias=False) self.W_q = nn.Linear(ipt_dim, units, bias=False) self.v = nn.Linear(units, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, query, key, value, valid_length): query, key = self.W_k(query), self.W_q(key) #print(\"size\",query.size(),key.size()) # expand query to (batch_size, #querys, 1, units), and key to # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast. features = query.unsqueeze(2) + key.unsqueeze(1) #print(\"features:\",features.size()) #--------------开启 scores = self.v(features).squeeze(-1) attention_weights = self.dropout(masked_softmax(scores, valid_length)) return torch.bmm(attention_weights, value) 计算背景变量我们先描述第一个关键点，即计算背景变量。图描绘了注意力机制如何为解码器在时间步2计算背景变量。首先，函数$a$根据解码器在时间步1的隐藏状态和编码器在各个时间步的隐藏状态计算softmax运算的输入。softmax运算输出概率分布并对编码器各个时间步的隐藏状态做加权平均，从而得到背景变量。 具体来说，令编码器在时间步$t$的隐藏状态为$\\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t’$的背景变量为所有编码器隐藏状态的加权平均： \\boldsymbol{c}_{t'} = \\sum_{t=1}^T \\alpha_{t' t} \\boldsymbol{h}_t,其中给定$t’$时，权重$\\alpha_{t’ t}$在$t=1,\\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算: \\alpha_{t' t} = \\frac{\\exp(e_{t' t})}{ \\sum_{k=1}^T \\exp(e_{t' k}) },\\quad t=1,\\ldots,T.现在，我们需要定义如何计算上式中softmax运算的输入$e_{t’ t}$。由于$e_{t’ t}$同时取决于解码器的时间步$t’$和编码器的时间步$t$，我们不妨以解码器在时间步$t’-1$的隐藏状态$\\boldsymbol{s}_{t’ - 1}$与编码器在时间步$t$的隐藏状态$\\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t’ t}$： e_{t' t} = a(\\boldsymbol{s}_{t' - 1}, \\boldsymbol{h}_t).这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]： a(\\boldsymbol{s}, \\boldsymbol{h}) = \\boldsymbol{v}^\\top \\tanh(\\boldsymbol{W}_s \\boldsymbol{s} + \\boldsymbol{W}_h \\boldsymbol{h}),其中$\\boldsymbol{v}$、$\\boldsymbol{W}_s$、$\\boldsymbol{W}_h$都是可以学习的模型参数。 矢量化计算我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。 在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\\boldsymbol{s}, \\boldsymbol{h})=\\boldsymbol{s}^\\top \\boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\\boldsymbol{s}_{t’ - 1} \\in \\mathbb{R}^{h}$和编码器所有隐藏状态$\\boldsymbol{h}_t \\in \\mathbb{R}^{h}, t = 1,\\ldots,T$来计算背景向量$\\boldsymbol{c}_{t’}\\in \\mathbb{R}^{h}$。我们可以将查询项矩阵$\\boldsymbol{Q} \\in \\mathbb{R}^{1 \\times h}$设为$\\boldsymbol{s}_{t’ - 1}^\\top$，并令键项矩阵$\\boldsymbol{K} \\in \\mathbb{R}^{T \\times h}$和值项矩阵$\\boldsymbol{V} \\in \\mathbb{R}^{T \\times h}$相同且第$t$行均为$\\boldsymbol{h}_t^\\top$。此时，我们只需要通过矢量化计算 \\text{softmax}(\\boldsymbol{Q}\\boldsymbol{K}^\\top)\\boldsymbol{V}即可算出转置后的背景向量$\\boldsymbol{c}_{t’}^\\top$。当查询项矩阵$\\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。 引入注意力机制的S2S本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入拼接起来一起送到解码器：下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构 Decoder由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态: the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings） 在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Seq2SeqAttentionDecoder(d2l.Decoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqAttentionDecoder, self).__init__(**kwargs) self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens,vocab_size) def init_state(self, enc_outputs, enc_valid_len, *args): outputs, hidden_state = enc_outputs# print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size()) # Transpose outputs to (batch_size, seq_len, hidden_size) return (outputs.permute(1,0,-1), hidden_state, enc_valid_len) #outputs.swapaxes(0, 1) def forward(self, X, state): enc_outputs, hidden_state, enc_valid_len = state #(\"X.size\",X.size()) X = self.embedding(X).transpose(0,1)# print(\"Xembeding.size2\",X.size()) outputs = [] for l, x in enumerate(X):# print(f\"\\n&#123;l&#125;-th token\")# print(\"x.first.size()\",x.size()) # query shape: (batch_size, 1, hidden_size) # select hidden state of the last rnn layer as query query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1) # context has same shape as query# print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size()) context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len) # Concatenate on the feature dimension# print(\"context.size:\",context.size()) x = torch.cat((context, x.unsqueeze(1)), dim=-1) # Reshape x to (1, batch_size, embed_size+hidden_size)# print(\"rnn\",x.size(), len(hidden_state)) out, hidden_state = self.rnn(x.transpose(0,1), hidden_state) outputs.append(out) outputs = self.dense(torch.cat(outputs, dim=0)) return outputs.transpose(0, 1), [enc_outputs, hidden_state, enc_valid_len]encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)# encoder.initialize()decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)X = torch.zeros((4, 7),dtype=torch.long)print(\"batch size=4\\nseq_length=7\\nhidden dim=16\\nnum_layers=2\\n\")print('encoder output size:', encoder(X)[0].size())print('encoder hidden size:', encoder(X)[1][0].size())print('encoder memory size:', encoder(X)[1][1].size())state = decoder.init_state(encoder(X), None)out, state = decoder(X, state)out.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"机器翻译基础","slug":"Deep_learning/机器翻译","date":"2020-02-15T16:00:00.000Z","updated":"2020-02-15T16:00:00.000Z","comments":true,"path":"2020/02/16/Deep_learning/机器翻译/","link":"","permalink":"https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/","excerpt":"","text":"机器翻译机器翻译是指将一段文本从一种语言自动翻译到另一种语言。因为一段文本序列在不同语言中的长度不一定相同，所以我们使用机器翻译为例来介绍编码器—解码器和注意力机制的应用。 读取和预处理数据数据预处理将数据集清洗、转化为神经网络的输入minbatch 12345678910111213%导入模块import syssys.path.append('.')import collectionsimport d2lzh_pytorch as d2limport zipfileimport timeimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.utils import datafrom torch import optim 12345678def preprocess_raw(text): text = text.replace('\\u202f', ' ').replace('\\xa0', ' ') out = '' for i, char in enumerate(text.lower()): if char in (',', '!', '.') and i &gt; 0 and text[i-1] != ' ': out += ' ' out += char return out 分词1234567891011num_examples = 50000source, target = [], []for i, line in enumerate(text.split('\\n')): if i &gt; num_examples: break parts = line.split('\\t') if len(parts) &gt;= 2: source.append(parts[0].split(' ')) target.append(parts[1].split(' ')) source[0:3], target[0:3] 建立词典 123456def build_vocab(tokens): tokens = [token for line in tokens for token in line] return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)src_vocab = build_vocab(source)len(src_vocab) 载入数据 123456789101112131415161718192021def pad(line, max_len, padding_token): if len(line) &gt; max_len: return line[:max_len] return line + [padding_token] * (max_len - len(line))pad(src_vocab[source[0]], 10, src_vocab.pad)def build_array(lines, vocab, max_len, is_source): lines = [vocab[line] for line in lines] if not is_source: lines = [[vocab.bos] + line + [vocab.eos] for line in lines] array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines]) valid_len = (array != vocab.pad).sum(1) #第一个维度 return array, valid_lendef load_data_nmt(batch_size, max_len): # This function is saved in d2l. src_vocab, tgt_vocab = build_vocab(source), build_vocab(target) src_array, src_valid_len = build_array(source, src_vocab, max_len, True) tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False) train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len) train_iter = data.DataLoader(train_data, batch_size, shuffle=True) return src_vocab, tgt_vocab, train_iter encoder-decoder可以应用在对话系统、生成式任务中。 12345678910111213141516171819202122232425262728class Encoder(nn.Module): def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedError class Decoder(nn.Module): def __init__(self, **kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_outputs, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedError class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state) Seq2Seq 123456789101112131415161718192021222324252627282930313233343536373839class Seq2SeqEncoder(d2l.Encoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) self.num_hiddens=num_hiddens self.num_layers=num_layers self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout) def begin_state(self, batch_size, device): return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device)] def forward(self, X, *args): X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size) X = X.transpose(0, 1) # RNN needs first axes to be time # state = self.begin_state(X.shape[1], device=X.device) out, state = self.rnn(X) # The shape of out is (seq_len, batch_size, num_hiddens). # state contains the hidden state and the memory cell # of the last time step, the shape is (num_layers, batch_size, num_hiddens) return out, state class Seq2SeqDecoder(d2l.Decoder): def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens,vocab_size) def init_state(self, enc_outputs, *args): return enc_outputs[1] def forward(self, X, state): X = self.embedding(X).transpose(0, 1) out, state = self.rnn(X, state) # Make the batch to be the first dimension to simplify loss computation. out = self.dense(out).transpose(0, 1) return out, state Beamsearch","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"模型选择与过拟合与欠拟合","slug":"Deep_learning/模型选择与过拟合、欠拟合","date":"2020-02-15T16:00:00.000Z","updated":"2020-02-15T16:00:00.000Z","comments":true,"path":"2020/02/16/Deep_learning/模型选择与过拟合、欠拟合/","link":"","permalink":"https://shyshy903.github.io/2020/02/16/Deep_learning/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/","excerpt":"","text":"模型选择与过拟合与欠拟合训练误差与泛化误差训练误差（training error）指模型在训练数据集上表现出的误差。泛化误差（generalization error）指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。 机器学习模型应关注降低泛化误差。 模型选择验证集测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set） K折交叉验证由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是$K$折交叉验证（$K$-fold cross-validation）。在$K$折交叉验证中，我们把原始训练数据集分割成$K$个不重合的子数据集，然后我们做$K$次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他$K-1$个子数据集来训练模型。在这$K$次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这$K$次训练误差和验证误差分别求平均。 欠拟合与过拟合 一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）； 另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。 在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。 模型复杂度 图3.4 模型复杂度对欠拟合和过拟合的影响 影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。 多项式拟合 正常拟合 欠拟合 过拟合 权重衰减上一节中我们观察了过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。 L2正则化权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述$L_2$范数正则化，再解释它为何又称权重衰减。 $L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以3.1节（线性回归）中的线性回归损失函数 \\ell(w_1, w_2, b) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2为例，其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为 \\ell(w_1, w_2, b) + \\frac{\\lambda}{2n} \\|\\boldsymbol{w}\\|^2,其中超参数$\\lambda &gt; 0$。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为 \\begin{aligned} w_1 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ w_2 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right). \\end{aligned}可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。 权重衰减的pytorch实现123456789%matplotlib inlineimport torchimport torch.nn as nnimport numpy as npimport syssys.path.append(\".\")import d2lzh_pytorch as d2lprint(torch.__version__) 1.3.1 12345678910111213141516171819202122232425def fit_and_plot_pytorch(wd): # 对权重参数衰减。权重名称一般是以weight结尾 net = nn.Linear(num_inputs, 1) nn.init.normal_(net.weight, mean=0, std=1) nn.init.normal_(net.bias, mean=0, std=1) optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减 optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr) # 不对偏差参数衰减 train_ls, test_ls = [], [] for _ in range(num_epochs): for X, y in train_iter: l = loss(net(X), y).mean() optimizer_w.zero_grad() optimizer_b.zero_grad() l.backward() # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差 optimizer_w.step() optimizer_b.step() train_ls.append(loss(net(train_features), train_labels).mean().item()) test_ls.append(loss(net(test_features), test_labels).mean().item()) d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss', range(1, num_epochs + 1), test_ls, ['train', 'test']) print('L2 norm of w:', net.weight.data.norm().item()) dropout多层感知机的计算表达式为 h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right)这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i’$ h_i' = \\frac{\\xi_i}{1-p} h_i由于$E(\\xi_i) = 1-p$，因此 E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i即丢弃法不改变其输入的期望值。 隐藏层使用了丢弃法的多层感知机 dropout的pytorch实现12345678910111213net = nn.Sequential( d2l.FlattenLayer(), nn.Linear(num_inputs, num_hiddens1), nn.ReLU(), nn.Dropout(drop_prob1), nn.Linear(num_hiddens1, num_hiddens2), nn.ReLU(), nn.Dropout(drop_prob2), nn.Linear(num_hiddens2, 10) )for param in net.parameters(): nn.init.normal_(param, mean=0, std=0.01) 梯度消失与梯度爆炸当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（衰减）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。 随机初始化模型参数PyTorch的默认随机初始化随机初始化模型参数的方法有很多。使用torch.nn.init.normal_()使模型net的权重参数采用正态分布的随机初始化方式。不过，PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考源代码），因此一般不用我们考虑。 Xavier随机初始化还有一种比较常用的随机初始化方法叫作Xavier随机初始化[1]。假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布 U\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。 小结 正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。 权重衰减等价于$L_2$范数正则化，通常会使学到的权重参数的元素较接近0。 权重衰减可以通过优化器中的weight_decay超参数来指定。 可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。 我们可以通过使用丢弃法应对过拟合。 丢弃法只在训练模型时使用 深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。 我们通常需要随机初始化神经网络的模型参数，如权重参数。 12","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"循环神经网络","slug":"Deep_learning/循环神经网络","date":"2020-02-13T16:00:00.000Z","updated":"2020-02-13T16:00:00.000Z","comments":true,"path":"2020/02/14/Deep_learning/循环神经网络/","link":"","permalink":"https://shyshy903.github.io/2020/02/14/Deep_learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"循环神经网络简单循环神经网络的构造 裁剪梯度循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 g ，并设裁剪的阈值是 θ 。裁剪后的梯度 循环神经网络的pytorch实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torch.nn as nnimport timeimport mathimport syssys.path.append(\"/home/kesci/input\")import d2l_jay9460 as d2l(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)num_steps, batch_size = 35, 2X = torch.rand(num_steps, batch_size, vocab_size)state = NoneY, state_new = rnn_layer(X, state)print(Y.shape, state_new.shape)class RNNModel(nn.Module): def __init__(self, rnn_layer, vocab_size): super(RNNModel, self).__init__() self.rnn = rnn_layer self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) self.vocab_size = vocab_size self.dense = nn.Linear(self.hidden_size, vocab_size) def forward(self, inputs, state): # inputs.shape: (batch_size, num_steps) X = to_onehot(inputs, vocab_size) X = torch.stack(X) # X.shape: (num_steps, batch_size, vocab_size) hiddens, state = self.rnn(X, state) hiddens = hiddens.view(-1, hiddens.shape[-1]) # hiddens.shape: (num_steps * batch_size, hidden_size) output = self.dense(hiddens) return output, state def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char, char_to_idx): state = None output = [char_to_idx[prefix[0]]] # output记录prefix加上预测的num_chars个字符 for t in range(num_chars + len(prefix) - 1): X = torch.tensor([output[-1]], device=device).view(1, 1) (Y, state) = model(X, state) # 前向计算不需要传入模型参数 if t &lt; len(prefix) - 1: output.append(char_to_idx[prefix[t + 1]]) else: output.append(Y.argmax(dim=1).item()) return ''.join([idx_to_char[i] for i in output]) 12model = RNNModel(rnn_layer, vocab_size).to(device)predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx) 1234567891011121314151617181920212223242526272829303132333435363738def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes): loss = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=lr) model.to(device) for epoch in range(num_epochs): l_sum, n, start = 0.0, 0, time.time() data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样 state = None for X, Y in data_iter: if state is not None: # 使用detach函数从计算图分离隐藏状态 if isinstance (state, tuple): # LSTM, state:(h, c) state[0].detach_() state[1].detach_() else: state.detach_() (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size) y = torch.flatten(Y.T) l = loss(output, y.long()) optimizer.zero_grad() l.backward() grad_clipping(model.parameters(), clipping_theta, device) optimizer.step() l_sum += l.item() * y.shape[0] n += y.shape[0] if (epoch + 1) % pred_period == 0: print('epoch %d, perplexity %f, time %.2f sec' % ( epoch + 1, math.exp(l_sum / n), time.time() - start)) for prefix in prefixes: print(' -', predict_rnn_pytorch( prefix, pred_len, model, vocab_size, device, idx_to_char, char_to_idx)) 123456num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes) RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系 GRU 重置⻔有助于捕捉时间序列⾥短期的依赖关系； 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系 1234567891011num_hiddens=256num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']lr = 1e-2 # 注意调整学习率gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)model = d2l.RNNModel(gru_layer, vocab_size).to(device)d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes) LSTM 长短期记忆long short-term memory : 遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入 输出门:控制从记忆细胞到隐藏状态 记忆细胞：⼀种特殊的隐藏状态的信息的流动 1234567891011num_hiddens=256num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']lr = 1e-2 # 注意调整学习率lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)model = d2l.RNNModel(lstm_layer, vocab_size)d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes) 深度循环网络通过num_layers来进行控制 123456789101112num_hiddens=256num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']lr = 1e-2 # 注意调整学习率gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=2)model = d2l.RNNModel(gru_layer, vocab_size).to(device)d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes) 双向循环网络 通过参数bidirectional=True来进行控制 123456789101112num_hiddens=128num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e-2, 1e-2pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']lr = 1e-2 # 注意调整学习率gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=True)model = d2l.RNNModel(gru_layer, vocab_size).to(device)d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"文本预处理与语言模型","slug":"Deep_learning/文本预处理与语言模型","date":"2020-02-13T16:00:00.000Z","updated":"2020-02-13T16:00:00.000Z","comments":true,"path":"2020/02/14/Deep_learning/文本预处理与语言模型/","link":"","permalink":"https://shyshy903.github.io/2020/02/14/Deep_learning/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"文本预处理 读入文本 分词 建立字典，将每个词映射到一个唯一的索引（index） 将文本从词的序列转换为索引的序列，方便输入模型 读入文本1234567891011import collectionsimport redef read_time_machine(): with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f: lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f] return lineslines = read_time_machine()print('# sentences %d' % len(lines)) 分词将一个句子划分为若干个token 1234567891011def tokenize(sentences, token='word'): \"\"\"Split sentences into word or char tokens\"\"\" if token == 'word': return [sentence.split(' ') for sentence in sentences] elif token == 'char': return [list(sentence) for sentence in sentences] else: print('ERROR: unkown token type '+token)tokens = tokenize(lines)tokens[0:2] 建立字典123456789101112131415161718192021222324252627282930313233343536373839class Vocab(object): def __init__(self, tokens, min_freq=0, use_special_tokens=False): counter = count_corpus(tokens) # : self.token_freqs = list(counter.items()) self.idx_to_token = [] if use_special_tokens: # padding, begin of sentence, end of sentence, unknown self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3) self.idx_to_token += ['', '', '', ''] else: self.unk = 0 self.idx_to_token += [''] self.idx_to_token += [token for token, freq in self.token_freqs if freq &gt;= min_freq and token not in self.idx_to_token] self.token_to_idx = dict() for idx, token in enumerate(self.idx_to_token): self.token_to_idx[token] = idx def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens): if not isinstance(tokens, (list, tuple)): return self.token_to_idx.get(tokens, self.unk) return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices): if not isinstance(indices, (list, tuple)): return self.idx_to_token[indices] return [self.idx_to_token[index] for index in indices]def count_corpus(sentences): tokens = [tk for st in sentences for tk in st] return collections.Counter(tokens) # 返回一个字典，记录每个词的出现次数# 将词转化为索引for i in range(8, 10): print('words:', tokens[i]) print('indices:', vocab[tokens[i]]) 用现有工具包分词NLTK123456text = \"Mr. Chen doesn't agree with my suggestion.\"from nltk.tokenize import word_tokenizefrom nltk import datadata.path.append('/home/kesci/input/nltk_data3784/nltk_data')print(word_tokenize(text)) SPACY1234import spacynlp = spacy.load('en_core_web_sm')doc = nlp(text)print([token.text for token in doc]) 语言模型（基于统计的语言模型） n元语法 相邻采样在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻 随机采样下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size是每个小批量的样本数，num_steps是每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"Softmax与分类模型","slug":"Deep_learning/Softmax与分类模型","date":"2020-02-12T16:00:00.000Z","updated":"2020-02-12T16:00:00.000Z","comments":true,"path":"2020/02/13/Deep_learning/Softmax与分类模型/","link":"","permalink":"https://shyshy903.github.io/2020/02/13/Deep_learning/Softmax%E4%B8%8E%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"softmax回归前几节介绍的线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。 分类问题让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为$x_1, x_2, x_3, x_4$。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值$y_1, y_2, y_3$。 我们通常使用离散的数值来表示类别，例如$y_1=1, y_2=2, y_3=3$。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。 softmax回归模型softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的$w$）、偏差包含3个标量（带下标的$b$），且对每个输入计算$o_1, o_2, o_3$这3个输出： \\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\\\ o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\\\ o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3. \\end{aligned}图3.2用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_1, o_2, o_3$的计算都要依赖于所有的输入$x_1, x_2, x_3, x_4$，softmax回归的输出层也是一个全连接层。 softmax运算既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出$\\operatorname*{argmax}_i o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。 然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。 softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布： \\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3),其中 \\hat{y}_1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}_2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad \\hat{y}_3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}.容易看出$\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$且$0 \\leq \\hat{y}_1, \\hat{y}_2, \\hat{y}_3 \\leq 1$，因此$\\hat{y}_1, \\hat{y}_2, \\hat{y}_3$是一个合法的概率分布。这时候，如果$\\hat{y}_2=0.8$，不管$\\hat{y}_1$和$\\hat{y}_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到 \\operatorname*{argmax}_i o_i = \\operatorname*{argmax}_i \\hat y_i,因此softmax运算不改变预测类别输出。 单样本分类的矢量计算表达式为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为 \\boldsymbol{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\\\ w_{41} & w_{42} & w_{43} \\end{bmatrix},\\quad \\boldsymbol{b} = \\begin{bmatrix} b_1 & b_2 & b_3 \\end{bmatrix},设高和宽分别为2个像素的图像样本$i$的特征为 \\boldsymbol{x}^{(i)} = \\begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\\end{bmatrix},输出层的输出为 \\boldsymbol{o}^{(i)} = \\begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\\end{bmatrix},预测为狗、猫或鸡的概率分布为 \\boldsymbol{\\hat{y}}^{(i)} = \\begin{bmatrix}\\hat{y}_1^{(i)} & \\hat{y}_2^{(i)} & \\hat{y}_3^{(i)}\\end{bmatrix}.softmax回归对样本$i$分类的矢量计算表达式为 \\begin{aligned} \\boldsymbol{o}^{(i)} &= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{y}}^{(i)} &= \\text{softmax}(\\boldsymbol{o}^{(i)}). \\end{aligned}小批量样本分类的矢量计算表达式为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。假设softmax回归的权重和偏差参数分别为$\\boldsymbol{W} \\in \\mathbb{R}^{d \\times q}$和$\\boldsymbol{b} \\in \\mathbb{R}^{1 \\times q}$。softmax回归的矢量计算表达式为 \\begin{aligned} \\boldsymbol{O} &= \\boldsymbol{X} \\boldsymbol{W} + \\boldsymbol{b},\\\\ \\boldsymbol{\\hat{Y}} &= \\text{softmax}(\\boldsymbol{O}), \\end{aligned}其中的加法运算使用了广播机制，$\\boldsymbol{O}, \\boldsymbol{\\hat{Y}} \\in \\mathbb{R}^{n \\times q}$且这两个矩阵的第$i$行分别为样本$i$的输出$\\boldsymbol{o}^{(i)}$和概率分布$\\boldsymbol{\\hat{y}}^{(i)}$。 交叉熵损失函数前面提到，使用softmax运算后可以更方便地与离散标签计算误差。我们已经知道，softmax运算将输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本$i$，我们构造向量$\\boldsymbol{y}^{(i)}\\in \\mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\\boldsymbol{\\hat y}^{(i)}$尽可能接近真实的标签概率分布$\\boldsymbol{y}^{(i)}$。 我们可以像线性回归那样使用平方损失函数$|\\boldsymbol{\\hat y}^{(i)}-\\boldsymbol{y}^{(i)}|^2/2$。然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\\hat{y}^{(i)}_3$比其他两个预测值$\\hat{y}^{(i)}_1$和$\\hat{y}^{(i)}_2$大就行了。即使$\\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\\hat y^{(i)}_1=\\hat y^{(i)}_2=0.2$比$\\hat y^{(i)}_1=0, \\hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。 改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法： H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},其中带下标的$y_j^{(i)}$是向量$\\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}_{y^{(i)}}$为1，其余全为0，于是$H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。 假设训练数据集的样本数为$n$，交叉熵损失函数定义为 \\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),其中$\\boldsymbol{\\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\\ell(\\boldsymbol{\\Theta}) = -(1/n) \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\\ell(\\boldsymbol{\\Theta})$等价于最大化$\\exp(-n\\ell(\\boldsymbol{\\Theta}))=\\prod_{i=1}^n \\hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。 softmax的pytorch实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 加载各种包或者模块import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(r\"D:\\Documents\\learning\")# import d2lzh as d2lprint(torch.__version__)# 生成数据集batch_size = 256# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')# 定义网络模型num_inputs = 784num_outputs = 10class LinearNet(nn.Module): def __init__(self, num_inputs, num_outputs): super(LinearNet, self).__init__() self.linear = nn.Linear(num_inputs, num_outputs) def forward(self, x): # x 的形状: (batch, 1, 28, 28) y = self.linear(x.view(x.shape[0], -1)) return y # net = LinearNet(num_inputs, num_outputs)class FlattenLayer(nn.Module): def __init__(self): super(FlattenLayer, self).__init__() def forward(self, x): # x 的形状: (batch, *, *, ...) return x.view(x.shape[0], -1)from collections import OrderedDictnet = nn.Sequential( # FlattenLayer(), # LinearNet(num_inputs, num_outputs) OrderedDict([ ('flatten', FlattenLayer()), ('linear', nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以 )# 初始化模型参数init.normal_(net.linear.weight, mean=0, std=0.01)init.constant_(net.linear.bias, val=0)# 定义损失函数loss = nn.CrossEntropyLoss() # 下面是他的函数原型# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')# 定义优化函数 1.3.1 训练模型12345678910111213141516171819202122232425num_epochs = 5def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None): \"\"\"Train and evaluate a model with CPU.\"\"\" for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: with autograd.record(): y_hat = net(X) l = loss(y_hat, y).sum() l.backward() if trainer is None: sgd(params, lr, batch_size) else: trainer.step(batch_size) y = y.astype('float32') train_l_sum += l.asscalar() train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar() n += y.size test_acc = evaluate_accuracy(test_iter, net) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"多层感知机","slug":"Deep_learning/多层感知机","date":"2020-02-12T16:00:00.000Z","updated":"2020-02-12T16:00:00.000Z","comments":true,"path":"2020/02/13/Deep_learning/多层感知机/","link":"","permalink":"https://shyshy903.github.io/2020/02/13/Deep_learning/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"","text":"多层感知机我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。在本节中，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。 隐藏层多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。图3.3展示了一个多层感知机的神经网络图。 在图3.3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3.3中的多层感知机的层数为2。由图3.3可见，隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。 具体来说，给定一个小批量样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\\boldsymbol{H}$，有$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\\boldsymbol{W}_h \\in \\mathbb{R}^{d \\times h}$和 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，输出层的权重和偏差参数分别为$\\boldsymbol{W}_o \\in \\mathbb{R}^{h \\times q}$和$\\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times q}$。 我们先来看一种含单隐藏层的多层感知机的设计。其输出$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$的计算为 $$\\begin{aligned}\\boldsymbol{H} &amp;= \\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h,\\\\boldsymbol{O} &amp;= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o,\\end{aligned}$$ 也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到 $$\\boldsymbol{O} = (\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h)\\boldsymbol{W}_o + \\boldsymbol{b}_o = \\boldsymbol{X} \\boldsymbol{W}_h\\boldsymbol{W}_o + \\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o.$$ 从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\\boldsymbol{W}_h\\boldsymbol{W}_o$，偏差参数为$\\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。 激活函数上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。 ReLU函数ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为 $$\\text{ReLU}(x) = \\max(x, 0).$$ 可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。 1234567891011121314import syssys.path.insert(0, '..')import numpyimport matplotlib.pyplot as pltimport torchimport torch.nn as nnfrom torch.autograd import Variabledef xyplot(x_vals,y_vals,name): x_vals=x_vals.detach().numpy() # we can't directly use var.numpy() because varibles might y_vals=y_vals.detach().numpy() # already required grad.,thus using var.detach().numpy() plt.plot(x_vals,y_vals) plt.xlabel('x') plt.ylabel(name+'(x)') 123x=Variable(torch.arange(-8.0,8.0,0.1,dtype=torch.float32).reshape(int(16/0.1),1),requires_grad=True)y=torch.nn.functional.relu(x)xyplot(x,y,'relu') 12y.backward(torch.ones_like(x),retain_graph=True)xyplot(x,x.grad,\"grad of relu\") sigmod函数sigmod函数可将元素的值变为0，1之间 $$\\sigma(sigmod)= \\frac{1}{1+exp^(-x)}$$ 123x=Variable(torch.arange(-8.0,8.0,0.1,dtype=torch.float32).reshape(int(16/0.1),1),requires_grad=True)y=torch.sigmoid(x)xyplot(x,y,'sigmoid') 12y.backward(torch.ones_like(x),retain_graph=True)xyplot(x,x.grad,'grad of sigmoid') tanh 函数tanh函数可以将元素的值变为-1，1之间$$ tanh(x) = \\frac{1-exp^(-2x)}{1+exp^(-2x)}$$ 123x=Variable(torch.arange(-8.0,8.0,0.1,dtype=torch.float32).reshape(int(16/0.1),1),requires_grad=True)y=torch.tanh(x)xyplot(x,y,\"tanh\") 12y.backward(torch.ones_like(x),retain_graph=True)xyplot(x,x.grad,\"grad of tanh\") 关于激活函数的选择ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。 用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。 在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。 多层感知机的pytorch实现123456789import torchfrom torch import nnfrom torch.nn import initimport numpy as npimport syssys.path.append(\".\") import d2lzh_pytorch as d2lprint(torch.__version__) 1.3.11234567891011num_inputs, num_outputs, num_hiddens = 784, 10, 256 net = nn.Sequential( d2l.FlattenLayer(), nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.Linear(num_hiddens, num_outputs), ) for params in net.parameters(): init.normal_(params, mean=0, std=0.01) 1234567891011121314151617batch_size = 256train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)loss = torch.nn.CrossEntropyLoss()optimizer = torch.optim.SGD(net.parameters(), lr=0.5)num_epochs = 5d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)### 输出如下epoch 1, loss 0.0031, train acc 0.703, test acc 0.757epoch 2, loss 0.0019, train acc 0.824, test acc 0.822epoch 3, loss 0.0016, train acc 0.845, test acc 0.825epoch 4, loss 0.0015, train acc 0.855, test acc 0.811epoch 5, loss 0.0014, train acc 0.865, test acc 0.846","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"线性回归","slug":"Deep_learning/线性回归","date":"2020-02-11T16:00:00.000Z","updated":"2020-02-11T16:00:00.000Z","comments":true,"path":"2020/02/12/Deep_learning/线性回归/","link":"","permalink":"https://shyshy903.github.io/2020/02/12/Deep_learning/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","excerpt":"","text":"线性回归 模型$$y = wx + b$$ 损失函数$$\\ell(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\ell^{(i)}(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2.$$ 优化函数$$\\begin{aligned}w_1 &amp;\\leftarrow w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial w_1} = w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\w_2 &amp;\\leftarrow w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial w_2} = w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\b &amp;\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b) }{\\partial b} = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right).\\end{aligned}$$ 神经网络图（单层神经网络） 线性回归的pytorch实现生成数据集123456789import torchnum_inputs = 2num_examples = 1000true_w = [2, -3.4]true_b = 4.2features = torch.randn(num_examples, num_inputs)labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_blabels += torch.normal(mean=torch.zeros(labels.shape), std=0.01) 读取数据1234567from torch.utils import data as tdatabatch_size = 10# 将训练数据的特征和标签组合dataset = tdata.TensorDataset(features, labels)# 随机读取小批量data_iter = tdata.DataLoader(dataset, batch_size, shuffle=True) 123for X, y in data_iter: print(X, y) break tensor([[ 0.2115, 1.4861], [-0.2630, 0.8898], [ 0.8301, -2.6101], [ 1.5199, -0.5050], [-0.4478, 0.6990], [ 1.4203, 1.1574], [ 1.3185, 1.1949], [ 2.0129, 0.8379], [ 1.1585, -0.1882], [ 0.9050, 0.0398]]) tensor([-0.4229, 0.6551, 14.7292, 8.9412, 0.9225, 3.1058, 2.7778, 5.3856, 7.1542, 5.8629])定义模型先导入nn模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。我们先定义一个模型变量net，它是一个Sequential实例。在nn中，Sequential实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。 123from torch import nnnet = nn.Sequential() 12 # 全连接层是一个线性层，特征数为2，输出个数为1net.add_module('linear', nn.Linear(2, 1)) 初始化模型参数这里主要是初始化线性回归模型中的权重与偏差。使用nn.init模块如nn.init.normal(tensor, std=0.01)指定随机初始化将随机采样均值为0、标准差为0.01的正态分布。偏差初始化默认为0 12345678from torch.nn import initdef params_init(model): if isinstance(model, nn.Linear): init.normal_(tensor=model.weight.data, std=0.01) init.constant_(tensor=model.bias.data, val=0) net.apply(params_init) Sequential( (linear): Linear(in_features=2, out_features=1, bias=True) )定义损失函数与优化算法12345loss = nn.MSELoss() # 均方误差损失函数from torch import optimoptimizer = optim.SGD(net.parameters(), lr=0.03) # lr为学习率 训练模型1234567891011num_epochs = 5 # 初始化训练周期for epoch in range(1, num_epochs + 1): for X, y in data_iter: net.zero_grad() l = loss(net(X), y.reshape(batch_size, -1)) # -1表示自动计算列 l.backward() optimizer.step() with torch.no_grad(): l = loss(net(features), labels.reshape(num_examples, -1)) print('epoch %d, loss: %f' % (epoch, l.data.numpy())) epoch 1, loss: 0.000093 epoch 2, loss: 0.000094 epoch 3, loss: 0.000094 epoch 4, loss: 0.000093 epoch 5, loss: 0.00009312linear = net[0]true_w, linear.weight.data ([2, -3.4], tensor([[ 2.0001, -3.4001]]))1true_b, linear.bias.data (4.2, tensor([4.2001]))1net Sequential( (linear): Linear(in_features=2, out_features=1, bias=True) )","categories":[{"name":"deep_learning","slug":"deep-learning","permalink":"https://shyshy903.github.io/categories/deep-learning/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://shyshy903.github.io/tags/DL/"}]},{"title":"MYSQL安装配置","slug":"SQL/MySQL的安装与配置","date":"2020-01-15T16:00:00.000Z","updated":"2020-01-15T16:00:00.000Z","comments":true,"path":"2020/01/16/SQL/MySQL的安装与配置/","link":"","permalink":"https://shyshy903.github.io/2020/01/16/SQL/MySQL%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"","text":"MySQL的安装与配置下载软件与安装 到mySQL的官网：点击这里，看到下图，下载即可。 解压缩到你所需要的路径MySQL配置123456789101112131415161718192021222324252627[mysqld]# 设置3306端口port=3306# 设置mysql的安装目录basedir=&lt;你的路径&gt;\\mysql-8.0.11-winx64# 设置mysql数据库的数据的存放目录datadir=&lt;你的路径&gt;\\mysql-8.0.11-winx64\\\\data# 允许最大连接数max_connections=200# 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统max_connect_errors=10000# 服务端使用的字符集默认为UTF8character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODBwait_timeout=31536000interactive_timeout=31536000#sql_mode=ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION[mysql]# 设置mysql客户端默认字符集default-character-set=utf8[client]# 设置mysql客户端连接服务端时默认使用的端口port=3306default-character-set=utf8 环境变量配置 系统变量添加 1MYSQL_HOME : path 环境变量添加 1%MYSQL_HOME%\\bin 如果上面2个方法都不想使用，可以使用下面这个方法： MySQL初始化 CMD运行1$ mysqld --initialize --user=mysql --console 务必记住这个初始密码，一会需要用这个初始密码登录mysql；修改密码如果emm你把他点没了 你只要把datadir配置的那个data的文件删除了，然后重新执行初始化即可然后输入：1mysqld --install 再打开服务：1net start mysql 修改默认密码执行：1mysql -u root -p登录,输入初始密码，接下来修改密码：1ALTER USER \"root@\"localhost\" IDENTIFIED BY \"your password\" 其它如果你在数据库连接工具的时候出现错误，则可以再my.ini文件中的mysqld中加入：1$ default_authentication_plugin=mysql_native_password","categories":[{"name":"SQL","slug":"SQL","permalink":"https://shyshy903.github.io/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://shyshy903.github.io/tags/SQL/"}]},{"title":"SQL语言基础","slug":"SQL/SQL语言（一）","date":"2020-01-14T16:00:00.000Z","updated":"2020-01-14T16:00:00.000Z","comments":true,"path":"2020/01/15/SQL/SQL语言（一）/","link":"","permalink":"https://shyshy903.github.io/2020/01/15/SQL/SQL%E8%AF%AD%E8%A8%80%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"SQL语言简单来说，你需要在数据库上执行的操作大部分都要由SQL语句完成1SELECT * FROM websites但是，SQL语句并不对大小写敏感，同时我们在本例教程中，每个语句都加上分号1234567891011121314151617mysql&gt; use RUNOOB;Database changedmysql&gt; set names utf8;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT * FROM Websites;+----+--------------+---------------------------+-------+---------+| id | name | url | alexa | country |+----+--------------+---------------------------+-------+---------+| 1 | Google | https://www.google.cm/ | 1 | USA || 2 | 淘宝 | https://www.taobao.com/ | 13 | CN || 3 | 菜鸟教程 | http://www.runoob.com/ | 4689 | CN || 4 | 微博 | http://weibo.com/ | 20 | CN || 5 | Facebook | https://www.facebook.com/ | 3 | USA |+----+--------------+---------------------------+-------+---------+5 rows in set (0.01 sec)下面的大部分都是基于这个表格的 SQL SELECT 语句SELECT语句用于从数据库中选取数据，结果被存在一个结果表中，称为结果集 SELECT 语法123SELECT column_name,column_name FROM table_name; # 选出指定列SELECT * FROM table_name; # 选出所有的列 SQL SELECT DISTINCT 语句在表中，一个列可能会包含多个重复值，DISTINCT关键词用于返回唯一不同的值,就是会删掉重复的值 SQL SELECT DISTINCT 语法1SELECT DISTINCT column_name, column_name FROM table_name; SELECT WHERE子句WHERE子句用于提取那些阿玛尼组指定条件的记录 SQL WHERE 语法1SELECT column_name,column_name FROM table_name WHERE column_name operator value; 文本字段 vs 数值字段SQL中使用单引号来环绕文本数值，但是如果是数值就不用使用单引号了，如：12SELECT * FROM websites WHERE id = 1;SELECT * FROM websites WHERE country = 'CN'; WHERE子句的运算符 运算符 描述 &lt;&gt;或者!= 不等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 IN 指定针对某个列的可能值 SQL AND &amp; OR 运算符AND &amp; OR 运算符基于一个以上条件对记录进行过滤 如果第一个条件与第二个条件都成立，则ADN运算符显示一条记录 如果第一个条件与第二个条件有一个成立，则OR运算符显示一条记录 SQL AND &amp; OR 语法12SELECT * FROM Websites WHERE country = 'CN' AND alexa &gt; 50;SELECT * FROM websites WHERE country = 'USA' OR country = 'CN'; 结合AND 和 OR用法示例如下：12SELECT * FROM websites WHERE alexa &gt; 15 AND (country = 'CN' OR country = 'USA') SQL ORDER BY 关键词ORDER BY 关键词用于对于结果集按照一个列或者多个列进行排序 SQL ORDER BY 的语法ORDER BY关键词默认按照升序进行排列，如果需要降序，则可以使用DESC关键字12SELECT column_name, column_name FROM table_name ORDER BY column_name, column_name ASC|DESC; SQL ORDER BY(DESC)实例12SELECT * FROM websites ORDER BY alexa ;SELECT * FROM websites ORDER BY alexa DESC; ORDER BY 多列当有多列进行排序时，我们优先选择第一列1SELECT * FROM websites ORDER BY country,alexa; SQL INSERT INTO语句INSERT INTO语句向表中插入新纪录 SQL INSERT INTO语句的语法INSERT 语句有两种编写形式： 第一钟形式无需指定要插入数据的列名，只需要提供被插入的值即可：1INSERT INTO table_name VALUES (value1,value2,value3); 第二种形式需要指定列名及被插入的值1INSERT INTO table_name (column1, column2,column3) VALUES (value1,value2,value3) INSERT INTO实例假设我们需要向‘websites’表钟插入一个新行1INSERT INTO websites(name, url, alexa, country) VALUES('百度’, 'https://www.baidu.com/','4','CN'); 在指定的列插入数据下面的SQL语句将插入一个新行，但是只在’name’、‘url’、’country‘列插入数据1INSERT INTO websites(name, url, country) VALUES ('stackoverflow', 'http://stackoverflow.com/', 'IND') SQL UPDATE 语句UPDATE语句用于更新表中已经存在的记录 UPDATE语句的语法1UPDATE table_name SET column = value1, column2 = value2,... WHERE some_column = some_value; UPDATE实例1UPADATE Websites SET alexa = '500', country = 'USA' WHERE name = '菜鸟教程’; SQL DELETEDELET语句用于删除表中的行 SQL DELETE语法和实例从表中删除网站名是百度且国家为CN的网站1DELET FROM websites WHERE name = '百度' AND country = 'CN'; 删除所有数据12DELETE FROM table_name;DELETE * FROM table_name; 最后非常感谢菜鸟教程，本文借鉴了菜鸟教程的表格和相关代码：菜鸟教程: https://www.runoob.com/sql/sql-syntax.html","categories":[{"name":"SQL","slug":"SQL","permalink":"https://shyshy903.github.io/categories/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://shyshy903.github.io/tags/SQL/"}]},{"title":"《机器学习实战》《西瓜书》笔记（八）- K均值聚类","slug":"Machine_Learning/机器学习笔记（8-K均值聚类）","date":"2019-11-30T11:30:00.000Z","updated":"2020-02-11T08:48:55.575Z","comments":true,"path":"2019/11/30/Machine_Learning/机器学习笔记（8-K均值聚类）/","link":"","permalink":"https://shyshy903.github.io/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888-K%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%EF%BC%89/","excerpt":"","text":"K均值聚类算法伪代码： 1234567创建k个点作为起始质心（经常是随机选择）当任意一个点的簇分配结果发生改变时 对数据集中的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到距离其最近的簇 对每一个簇，计算簇中所有点的均值，并且将该值作为质心 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657\"\"\"函数说明：k-means聚类算法Parameters: dataSet - 用于聚类的数据集 k - 选取k个质心 distMeas - 距离计算方法,默认欧氏距离distEclud() createCent - 获取k个质心的方法,默认随机获取randCent() Returns: centroids - k个聚类的聚类结果 clusterAssment - 聚类误差\"\"\"def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): # 获取数据集样本数 m = np.shape(dataSet)[0] # 初始化一个（m,2）全零矩阵 clusterAssment = np.mat(np.zeros((m, 2))) # 创建初始的k个质心向量 centroids = createCent(dataSet, k) # 聚类结果是否发生变化的布尔类型 clusterChanged = True # 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化 while clusterChanged: # 聚类结果变化布尔类型置为False clusterChanged = False # 遍历数据集每一个样本向量 for i in range(m): # 初始化最小距离为正无穷，最小距离对应的索引为-1 minDist = float('inf') minIndex = -1 # 循环k个类的质心 for j in range(k): # 计算数据点到质心的欧氏距离 distJI = distMeas(centroids[j, :], dataSet[i, :]) # 如果距离小于当前最小距离 if distJI &lt; minDist: # 当前距离为最小距离，最小距离对应索引应为j(第j个类) minDist = distJI minIndex = j # 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法 if clusterAssment[i, 0] != minIndex: clusterChanged = True # 更新当前变化样本的聚类结果和平方误差 clusterAssment[i, :] = minIndex, minDist**2 # 打印k-means聚类的质心 # print(centroids) # 遍历每一个质心 for cent in range(k): # 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来 ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A == cent)[0]] # 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量 centroids[cent, :] = np.mean(ptsInClust, axis=0) # 返回k个聚类，聚类结果及误差 return centroids, clusterAssment 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187# -*- coding: utf-8 -*-\"\"\"Created on Thu Aug 2 21:20:03 2018@author: wzy\"\"\"import matplotlib.pyplot as pltimport numpy as np\"\"\"函数说明：将文本文档中的数据读入到python中Parameters: fileName - 文件名 Returns: dataMat - 数据矩阵\"\"\"def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = list(map(float, curLine)) dataMat.append(fltLine) return dataMat\"\"\"函数说明：数据向量计算欧式距离Parameters: vecA - 数据向量A vecB - 数据向量B Returns: 两个向量之间的欧几里德距离Modify: 2018-08-02\"\"\"def distEclud(vecA, vecB): return np.sqrt(np.sum(np.power(vecA - vecB, 2)))\"\"\"函数说明：随机初始化k个质心（质心满足数据边界之内）Parameters: dataSet - 输入的数据集 k - 选取k个质心 Returns: centroids - 返回初始化得到的k个质心向量\"\"\"def randCent(dataSet, k): # 得到数据样本的维度 n = np.shape(dataSet)[1] # 初始化为一个(k,n)的全零矩阵 centroids = np.mat(np.zeros((k, n))) # 遍历数据集的每一个维度 for j in range(n): # 得到该列数据的最小值,最大值 minJ = np.min(dataSet[:, j]) maxJ = np.max(dataSet[:, j]) # 得到该列数据的范围(最大值-最小值) rangeJ = float(maxJ - minJ) # k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值 # Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1). centroids[:, j] = minJ + rangeJ * np.random.rand(k, 1) # 返回初始化得到的k个质心向量 return centroids\"\"\"函数说明：k-means聚类算法Parameters: dataSet - 用于聚类的数据集 k - 选取k个质心 distMeas - 距离计算方法,默认欧氏距离distEclud() createCent - 获取k个质心的方法,默认随机获取randCent() Returns: centroids - k个聚类的聚类结果 clusterAssment - 聚类误差\"\"\"def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): # 获取数据集样本数 m = np.shape(dataSet)[0] # 初始化一个（m,2）全零矩阵 clusterAssment = np.mat(np.zeros((m, 2))) # 创建初始的k个质心向量 centroids = createCent(dataSet, k) # 聚类结果是否发生变化的布尔类型 clusterChanged = True # 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化 while clusterChanged: # 聚类结果变化布尔类型置为False clusterChanged = False # 遍历数据集每一个样本向量 for i in range(m): # 初始化最小距离为正无穷，最小距离对应的索引为-1 minDist = float('inf') minIndex = -1 # 循环k个类的质心 for j in range(k): # 计算数据点到质心的欧氏距离 distJI = distMeas(centroids[j, :], dataSet[i, :]) # 如果距离小于当前最小距离 if distJI &lt; minDist: # 当前距离为最小距离，最小距离对应索引应为j(第j个类) minDist = distJI minIndex = j # 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法 if clusterAssment[i, 0] != minIndex: clusterChanged = True # 更新当前变化样本的聚类结果和平方误差 clusterAssment[i, :] = minIndex, minDist**2 # 打印k-means聚类的质心 # print(centroids) # 遍历每一个质心 for cent in range(k): # 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来 ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A == cent)[0]] # 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量 centroids[cent, :] = np.mean(ptsInClust, axis=0) # 返回k个聚类，聚类结果及误差 return centroids, clusterAssment \"\"\"函数说明：绘制数据集Parameters: fileName - 文件名 Returns: None\"\"\"def plotDataSet(filename): # 导入数据 datMat = np.mat(loadDataSet(filename)) # 进行k-means算法其中k为4 myCentroids, clustAssing = kMeans(datMat, 4) clustAssing = clustAssing.tolist() myCentroids = myCentroids.tolist() xcord = [[], [], [], []] ycord = [[], [], [], []] datMat = datMat.tolist() m = len(clustAssing) for i in range(m): if int(clustAssing[i][0]) == 0: xcord[0].append(datMat[i][0]) ycord[0].append(datMat[i][1]) elif int(clustAssing[i][0]) == 1: xcord[1].append(datMat[i][0]) ycord[1].append(datMat[i][1]) elif int(clustAssing[i][0]) == 2: xcord[2].append(datMat[i][0]) ycord[2].append(datMat[i][1]) elif int(clustAssing[i][0]) == 3: xcord[3].append(datMat[i][0]) ycord[3].append(datMat[i][1]) fig = plt.figure() ax = fig.add_subplot(111) # 绘制样本点 ax.scatter(xcord[0], ycord[0], s=20, c='b', marker='*', alpha=.5) ax.scatter(xcord[1], ycord[1], s=20, c='r', marker='D', alpha=.5) ax.scatter(xcord[2], ycord[2], s=20, c='c', marker='&gt;', alpha=.5) ax.scatter(xcord[3], ycord[3], s=20, c='k', marker='o', alpha=.5) # 绘制质心 ax.scatter(myCentroids[0][0], myCentroids[0][1], s=100, c='k', marker='+', alpha=.5) ax.scatter(myCentroids[1][0], myCentroids[1][1], s=100, c='k', marker='+', alpha=.5) ax.scatter(myCentroids[2][0], myCentroids[2][1], s=100, c='k', marker='+', alpha=.5) ax.scatter(myCentroids[3][0], myCentroids[3][1], s=100, c='k', marker='+', alpha=.5) plt.title('DataSet') plt.xlabel('X') plt.show()if __name__ == '__main__': plotDataSet('testSet.txt') 二分K均值聚类算法伪代码： 1234567将所有点看成一个簇当簇数目小于K时 对每一个簇 计算总误差 在给定的簇上面进行K-均值聚类（k&#x3D;2) 计算将该簇一分为二之后的总误差 选择使得误差最小的那个簇进行划分操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970\"\"\"函数说明：二分k-means聚类算法Parameters: dataSet - 用于聚类的数据集 k - 选取k个质心 distMeas - 距离计算方法,默认欧氏距离distEclud() Returns: centList - k个聚类的聚类结果 clusterAssment - 聚类误差\"\"\"def biKmeans(dataSet, k, distMeas=distEclud): # 获取数据集的样本数 m = np.shape(dataSet)[0] # 初始化一个元素均值0的(m, 2)矩阵 clusterAssment = np.mat(np.zeros((m, 2))) # 获取数据集每一列数据的均值，组成一个列表 centroid0 = np.mean(dataSet, axis=0).tolist()[0] # 当前聚类列表为将数据集聚为一类 centList = [centroid0] # 遍历每个数据集样本 for j in range(m): # 计算当前聚为一类时各个数据点距离质心的平方距离 clusterAssment[j, 1] = distMeas(np.mat(centroid0), dataSet[j, :])**2 # 循环，直至二分k-Means值达到k类为止 while (len(centList) &lt; k): # 将当前最小平方误差置为正无穷 lowerSSE = float('inf') # 遍历当前每个聚类 for i in range(len(centList)): # 通过数组过滤筛选出属于第i类的数据集合 ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, 0].A == i)[0], :] # 对该类利用二分k-means算法进行划分，返回划分后的结果以及误差 centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 计算该类划分后两个类的误差平方和 sseSplit = np.sum(splitClustAss[:, 1]) # 计算数据集中不属于该类的数据的误差平方和 sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, 0].A != i)[0], 1]) # 打印这两项误差值 print('sseSplit = %f, and notSplit = %f' % (sseSplit, sseNotSplit)) # 划分第i类后总误差小于当前最小总误差 if (sseSplit + sseNotSplit) &lt; lowerSSE: # 第i类作为本次划分类 bestCentToSplit = i # 第i类划分后得到的两个质心向量 bestNewCents = centroidMat # 复制第i类中数据点的聚类结果即误差值 bestClustAss = splitClustAss.copy() # 将划分第i类后的总误差作为当前最小误差 lowerSSE = sseSplit + sseNotSplit # 数组过滤选出本次2-means聚类划分后类编号为1数据点，将这些数据点类编号变为 # 当前类个数+1， 作为新的一个聚类 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # 同理，将划分数据中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号 # 连续不出现空缺 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit # 打印本次执行2-means聚类算法的类 print('the bestCentToSplit is %d' % bestCentToSplit) # 打印被划分的类的数据个数 print('the len of bestClustAss is %d' % len(bestClustAss)) # 更新质心列表中变化后的质心向量 centList[bestCentToSplit] = bestNewCents[0, :] # 添加新的类的质心向量 centList.append(bestNewCents[1, :]) # 更新clusterAssment列表中参与2-means聚类数据点变化后的分类编号，及数据该类的误差平方 clusterAssment[np.nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # 返回聚类结果 return centList, clusterAssment 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260# -*- coding: utf-8 -*-\"\"\"Created on Fri Aug 3 13:53:40 2018@author: wzy\"\"\"import matplotlib.pyplot as pltimport numpy as np\"\"\"函数说明：将文本文档中的数据读入到python中Parameters: fileName - 文件名 Returns: dataMat - 数据矩阵\"\"\"def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = list(map(float, curLine)) dataMat.append(fltLine) return dataMat\"\"\"函数说明：数据向量计算欧式距离Parameters: vecA - 数据向量A vecB - 数据向量B Returns: 两个向量之间的欧几里德距离\"\"\"def distEclud(vecA, vecB): return np.sqrt(np.sum(np.power(vecA - vecB, 2)))\"\"\"函数说明：随机初始化k个质心（质心满足数据边界之内）Parameters: dataSet - 输入的数据集 k - 选取k个质心 Returns: centroids - 返回初始化得到的k个质心向量\"\"\"def randCent(dataSet, k): # 得到数据样本的维度 n = np.shape(dataSet)[1] # 初始化为一个(k,n)的全零矩阵 centroids = np.mat(np.zeros((k, n))) # 遍历数据集的每一个维度 for j in range(n): # 得到该列数据的最小值,最大值 minJ = np.min(dataSet[:, j]) maxJ = np.max(dataSet[:, j]) # 得到该列数据的范围(最大值-最小值) rangeJ = float(maxJ - minJ) # k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值 # Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1). centroids[:, j] = minJ + rangeJ * np.random.rand(k, 1) # 返回初始化得到的k个质心向量 return centroids\"\"\"函数说明：k-means聚类算法Parameters: dataSet - 用于聚类的数据集 k - 选取k个质心 distMeas - 距离计算方法,默认欧氏距离distEclud() createCent - 获取k个质心的方法,默认随机获取randCent() Returns: centroids - k个聚类的聚类结果 clusterAssment - 聚类误差\"\"\"def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): # 获取数据集样本数 m = np.shape(dataSet)[0] # 初始化一个（m,2）全零矩阵 clusterAssment = np.mat(np.zeros((m, 2))) # 创建初始的k个质心向量 centroids = createCent(dataSet, k) # 聚类结果是否发生变化的布尔类型 clusterChanged = True # 只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不发生变化 while clusterChanged: # 聚类结果变化布尔类型置为False clusterChanged = False # 遍历数据集每一个样本向量 for i in range(m): # 初始化最小距离为正无穷，最小距离对应的索引为-1 minDist = float('inf') minIndex = -1 # 循环k个类的质心 for j in range(k): # 计算数据点到质心的欧氏距离 distJI = distMeas(centroids[j, :], dataSet[i, :]) # 如果距离小于当前最小距离 if distJI &lt; minDist: # 当前距离为最小距离，最小距离对应索引应为j(第j个类) minDist = distJI minIndex = j # 当前聚类结果中第i个样本的聚类结果发生变化：布尔值置为True，继续聚类算法 if clusterAssment[i, 0] != minIndex: clusterChanged = True # 更新当前变化样本的聚类结果和平方误差 clusterAssment[i, :] = minIndex, minDist**2 # 打印k-means聚类的质心 # print(centroids) # 遍历每一个质心 for cent in range(k): # 将数据集中所有属于当前质心类的样本通过条件过滤筛选出来 ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A == cent)[0]] # 计算这些数据的均值(axis=0:求列均值)，作为该类质心向量 centroids[cent, :] = np.mean(ptsInClust, axis=0) # 返回k个聚类，聚类结果及误差 return centroids, clusterAssment\"\"\"函数说明：二分k-means聚类算法Parameters: dataSet - 用于聚类的数据集 k - 选取k个质心 distMeas - 距离计算方法,默认欧氏距离distEclud() Returns: centList - k个聚类的聚类结果 clusterAssment - 聚类误差\"\"\"def biKmeans(dataSet, k, distMeas=distEclud): # 获取数据集的样本数 m = np.shape(dataSet)[0] # 初始化一个元素均值0的(m, 2)矩阵 clusterAssment = np.mat(np.zeros((m, 2))) # 获取数据集每一列数据的均值，组成一个列表 centroid0 = np.mean(dataSet, axis=0).tolist()[0] # 当前聚类列表为将数据集聚为一类 centList = [centroid0] # 遍历每个数据集样本 for j in range(m): # 计算当前聚为一类时各个数据点距离质心的平方距离 clusterAssment[j, 1] = distMeas(np.mat(centroid0), dataSet[j, :])**2 # 循环，直至二分k-Means值达到k类为止 while (len(centList) &lt; k): # 将当前最小平方误差置为正无穷 lowerSSE = float('inf') # 遍历当前每个聚类 for i in range(len(centList)): # 通过数组过滤筛选出属于第i类的数据集合 ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, 0].A == i)[0], :] # 对该类利用二分k-means算法进行划分，返回划分后的结果以及误差 centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 计算该类划分后两个类的误差平方和 sseSplit = np.sum(splitClustAss[:, 1]) # 计算数据集中不属于该类的数据的误差平方和 sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, 0].A != i)[0], 1]) # 打印这两项误差值 print('sseSplit = %f, and notSplit = %f' % (sseSplit, sseNotSplit)) # 划分第i类后总误差小于当前最小总误差 if (sseSplit + sseNotSplit) &lt; lowerSSE: # 第i类作为本次划分类 bestCentToSplit = i # 第i类划分后得到的两个质心向量 bestNewCents = centroidMat # 复制第i类中数据点的聚类结果即误差值 bestClustAss = splitClustAss.copy() # 将划分第i类后的总误差作为当前最小误差 lowerSSE = sseSplit + sseNotSplit # 数组过滤选出本次2-means聚类划分后类编号为1数据点，将这些数据点类编号变为 # 当前类个数+1， 作为新的一个聚类 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # 同理，将划分数据中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号 # 连续不出现空缺 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit # 打印本次执行2-means聚类算法的类 print('the bestCentToSplit is %d' % bestCentToSplit) # 打印被划分的类的数据个数 print('the len of bestClustAss is %d' % len(bestClustAss)) # 更新质心列表中变化后的质心向量 centList[bestCentToSplit] = bestNewCents[0, :] # 添加新的类的质心向量 centList.append(bestNewCents[1, :]) # 更新clusterAssment列表中参与2-means聚类数据点变化后的分类编号，及数据该类的误差平方 clusterAssment[np.nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # 返回聚类结果 return centList, clusterAssment\"\"\"函数说明：绘制数据集Parameters: fileName - 文件名 k - 选取k个质心 Returns: None\"\"\"def plotDataSet(filename, k): # 导入数据 datMat = np.mat(loadDataSet(filename)) # 进行k-means算法其中k为4 centList, clusterAssment = biKmeans(datMat, k) clusterAssment = clusterAssment.tolist() xcord = [[], [], []] ycord = [[], [], []] datMat = datMat.tolist() m = len(clusterAssment) for i in range(m): if int(clusterAssment[i][0]) == 0: xcord[0].append(datMat[i][0]) ycord[0].append(datMat[i][1]) elif int(clusterAssment[i][0]) == 1: xcord[1].append(datMat[i][0]) ycord[1].append(datMat[i][1]) elif int(clusterAssment[i][0]) == 2: xcord[2].append(datMat[i][0]) ycord[2].append(datMat[i][1]) fig = plt.figure() ax = fig.add_subplot(111) # 绘制样本点 ax.scatter(xcord[0], ycord[0], s=20, c='b', marker='*', alpha=.5) ax.scatter(xcord[1], ycord[1], s=20, c='r', marker='D', alpha=.5) ax.scatter(xcord[2], ycord[2], s=20, c='c', marker='&gt;', alpha=.5) # 绘制质心 for i in range(k): ax.scatter(centList[i].tolist()[0][0], centList[i].tolist()[0][1], s=100, c='k', marker='+', alpha=.5) # ax.scatter(centList[0].tolist()[0][0], centList[0].tolist()[0][1], s=100, c='k', marker='+', alpha=.5) # ax.scatter(centList[1].tolist()[0][0], centList[1].tolist()[0][1], s=100, c='k', marker='+', alpha=.5) # ax.scatter(centList[2].tolist()[0][0], centList[2].tolist()[0][1], s=100, c='k', marker='+', alpha=.5) plt.title('DataSet') plt.xlabel('X') plt.show() if __name__ == '__main__': datMat = np.mat(loadDataSet('testSet2.txt')) centList, myNewAssments = biKmeans(datMat, 3) plotDataSet('testSet2.txt', 3)","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（七）- SVM","slug":"Machine_Learning/机器学习笔记（7 -SVM)","date":"2019-11-30T07:30:00.000Z","updated":"2020-02-11T08:48:55.572Z","comments":true,"path":"2019/11/30/Machine_Learning/机器学习笔记（7 -SVM)/","link":"","permalink":"https://shyshy903.github.io/2019/11/30/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%20-SVM)/","excerpt":"","text":"SVM（支持向量机）支持向量（support vector)就是离分隔超平面最近的那些点。最大化支持向量到分割面的距离。 核函数（kneral function)核函数就是将数据转化成易于分类器理解的一种形式，目前相对流行的一种核函数：径向基函数 径向基函数把数据从一个特征空间映射到另一个特征空间$k(x,y)=exp(\\frac{-||x-y||^2}{2\\sigma^2})$其中$\\sigma$是确定到达率，是函数值跌倒0的速度参数 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447# -*- coding: utf-8 -*-\"\"\"Created on Wed Jul 25 11:04:01 2018k1越大会过拟合@author: wzy\"\"\"import matplotlib.pyplot as pltimport numpy as npimport random\"\"\"类说明：维护所有需要操作的值Parameters: dataMatIn - 数据矩阵 classLabels - 数据标签 C - 松弛变量 toler - 容错率 kTup - 包含核函数信息的元组，第一个参数存放该核函数类别，第二个参数存放必要的核函数需要用到的参数 Returns: None\"\"\"class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # 数据矩阵 self.X = dataMatIn # 数据标签 self.labelMat = classLabels # 松弛变量 self.C = C # 容错率 self.tol = toler # 矩阵的行数 self.m = np.shape(dataMatIn)[0] # 根据矩阵行数初始化alphas矩阵，一个m行1列的全零列向量 self.alphas = np.mat(np.zeros((self.m, 1))) # 初始化b参数为0 self.b = 0 # 根据矩阵行数初始化误差缓存矩阵，第一列为是否有效标志位，第二列为实际的误差E的值 self.eCache = np.mat(np.zeros((self.m, 2))) # 初始化核K self.K = np.mat(np.zeros((self.m, self.m))) # 计算所有数据的核K for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)\"\"\"函数说明：通过核函数将数据转换更高维空间Parameters: X - 数据矩阵 A - 单个数据的向量 kTup - 包含核函数信息的元组 Returns: K - 计算的核K\"\"\"def kernelTrans(X, A, kTup): # 读取X的行列数 m, n = np.shape(X) # K初始化为m行1列的零向量 K = np.mat(np.zeros((m, 1))) # 线性核函数只进行内积 if kTup[0] == 'lin': K = X * A.T # 高斯核函数，根据高斯核函数公式计算 elif kTup[0] == 'rbf': for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = np.exp(K / (-1 * kTup[1] ** 2)) else: raise NameError('核函数无法识别') return K \"\"\"函数说明：读取数据Parameters: fileName - 文件名 Returns: dataMat - 数据矩阵 labelMat - 数据标签\"\"\"def loadDataSet(fileName): # 数据矩阵 dataMat = [] # 标签向量 labelMat = [] # 打开文件 fr = open(fileName) # 逐行读取 for line in fr.readlines(): # 去掉每一行首尾的空白符，例如'\\n','\\r','\\t',' ' # 将每一行内容根据'\\t'符进行切片 lineArr = line.strip().split('\\t') # 添加数据(100个元素排成一行) dataMat.append([float(lineArr[0]), float(lineArr[1])]) # 添加标签(100个元素排成一行) labelMat.append(float(lineArr[2])) return dataMat, labelMat\"\"\"函数说明：计算误差Parameters: oS - 数据结构 k - 标号为k的数据 Returns: Ek - 标号为k的数据误差\"\"\"def calcEk(oS, k): # multiply(a,b)就是个乘法，如果a,b是两个数组，那么对应元素相乘 # .T为转置 fXk = float(np.multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) # 计算误差项 Ek = fXk - float(oS.labelMat[k]) # 返回误差项 return Ek\"\"\"函数说明：随机选择alpha_jParameters: i - alpha_i的索引值 m - alpha参数个数 Returns: j - alpha_j的索引值\"\"\"def selectJrand(i, m): j = i while(j == i): # uniform()方法将随机生成一个实数，它在[x, y)范围内 j = int(random.uniform(0, m)) return j\"\"\"函数说明：内循环启发方式2Parameters: i - 标号为i的数据的索引值 oS - 数据结构 Ei - 标号为i的数据误差 Returns: j - 标号为j的数据的索引值 maxK - 标号为maxK的数据的索引值 Ej - 标号为j的数据误差\"\"\"def selectJ(i, oS, Ei): # 初始化 maxK = -1 maxDeltaE = 0 Ej = 0 # 根据Ei更新误差缓存 oS.eCache[i] = [1, Ei] # 对一个矩阵.A转换为Array类型 # 返回误差不为0的数据的索引值 validEcacheList = np.nonzero(oS.eCache[:, 0].A)[0] # 有不为0的误差 if(len(validEcacheList) &gt; 1): # 遍历，找到最大的Ek for k in validEcacheList: # 不计算k==i节省时间 if k == i: continue # 计算Ek Ek = calcEk(oS, k) # 计算|Ei - Ek| deltaE = abs(Ei - Ek) # 找到maxDeltaE if(deltaE &gt; maxDeltaE): maxK = k maxDeltaE = deltaE Ej = Ek # 返回maxK，Ej return maxK, Ej # 没有不为0的误差 else: # 随机选择alpha_j的索引值 j = selectJrand(i, oS.m) # 计算Ej Ej = calcEk(oS, j) # 返回j，Ej return j, Ej\"\"\"函数说明：计算Ek,并更新误差缓存Parameters: oS - 数据结构 k - 标号为k的数据的索引值 Returns: None\"\"\"def updateEk(oS, k): # 计算Ek Ek = calcEk(oS, k) # 更新误差缓存 oS.eCache[k] = [1, Ek] \"\"\"函数说明：修剪alpha_jParameters: aj - alpha_j值 H - alpha上限 L - alpha下限 Returns: aj - alpha_j值Modify: 2018-07-24\"\"\"def clipAlpha(aj, H, L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj\"\"\"函数说明：优化的SMO算法Parameters: i - 标号为i的数据的索引值 oS - 数据结构 Returns: 1 - 有任意一对alpha值发生变化 0 - 没有任意一对alpha值发生变化或变化太小\"\"\"def innerL(i, oS): # 步骤1：计算误差Ei Ei = calcEk(oS, i) # 优化alpha,设定一定的容错率 if((oS.labelMat[i] * Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i] * Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): # 使用内循环启发方式2选择alpha_j,并计算Ej j, Ej = selectJ(i, oS, Ei) # 保存更新前的alpha值，使用深层拷贝 alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() # 步骤2：计算上界H和下界L if(oS.labelMat[i] != oS.labelMat[j]): L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L == H\") return 0 # 步骤3：计算eta eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] if eta &gt;= 0: print(\"eta &gt;= 0\") return 0 # 步骤4：更新alpha_j oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta # 步骤5：修剪alpha_j oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) # 更新Ej至误差缓存 updateEk(oS, j) if(abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print(\"alpha_j变化太小\") return 0 # 步骤6：更新alpha_i oS.alphas[i] += oS.labelMat[i] * oS.labelMat[j] * (alphaJold - oS.alphas[j]) # 更新Ei至误差缓存 updateEk(oS, i) # 步骤7：更新b_1和b_2: b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, i] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, j] # 步骤8：根据b_1和b_2更新b if(0 &lt; oS.alphas[i] &lt; oS.C): oS.b = b1 elif(0 &lt; oS.alphas[j] &lt; oS.C): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 \"\"\"函数说明：完整的线性SMO算法Parameters: dataMatIn - 数据矩阵 classLabels - 数据标签 C - 松弛变量 toler - 容错率 maxIter - 最大迭代次数 kTup - 包含核函数信息的元组 Returns: oS.b - SMO算法计算的b oS.alphas - SMO算法计算的alphas\"\"\"def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup = ('lin', 0)): # 初始化数据结构 oS = optStruct(np.mat(dataMatIn), np.mat(classLabels).transpose(), C, toler, kTup) # 初始化当前迭代次数 iter = 0 entrieSet = True alphaPairsChanged = 0 # 遍历整个数据集alpha都没有更新或者超过最大迭代次数，则退出循环 while(iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entrieSet)): alphaPairsChanged = 0 # 遍历整个数据集 if entrieSet: for i in range(oS.m): # 使用优化的SMO算法 alphaPairsChanged += innerL(i, oS) print(\"全样本遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter, i, alphaPairsChanged)) iter += 1 # 遍历非边界值 else: # 遍历不在边界0和C的alpha nonBoundIs = np.nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0] for i in nonBoundIs: alphaPairsChanged += innerL(i, oS) print(\"非边界遍历:第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter, i, alphaPairsChanged)) iter += 1 # 遍历一次后改为非边界遍历 if entrieSet: entrieSet = False # 如果alpha没有更新，计算全样本遍历 elif(alphaPairsChanged == 0): entrieSet = True print(\"迭代次数:%d\" % iter) # 返回SMO算法计算的b和alphas return oS.b, oS.alphas\"\"\"函数说明：测试函数Parameters: k1 - 使用高斯核函数的时候表示到达率 Returns: None\"\"\"def testRbf(k1 = 1.3): # 加载训练集 dataArr, labelArr = loadDataSet('testSetRBF.txt') # 根据训练集计算b, alphas b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 100, ('rbf', k1)) datMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() # 获得支持向量 svInd = np.nonzero(alphas.A &gt; 0)[0] sVs = datMat[svInd] labelSV = labelMat[svInd] print(\"支持向量个数:%d\" % np.shape(sVs)[0]) m, n = np.shape(datMat) errorCount = 0 for i in range(m): # 计算各个点的核 kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1)) # 根据支持向量的点计算超平面，返回预测结果 predict = kernelEval.T * np.multiply(labelSV, alphas[svInd]) + b # 返回数组中各元素的正负号，用1和-1表示，并统计错误个数 if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 # 打印错误率 print('训练集错误率:%.2f%%' % ((float(errorCount) / m) * 100)) # 加载测试集 dataArr, labelArr = loadDataSet('testSetRBF2.txt') errorCount = 0 datMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() m, n = np.shape(datMat) for i in range(m): # 计算各个点的核 kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1)) # 根据支持向量的点计算超平面，返回预测结果 predict = kernelEval.T * np.multiply(labelSV, alphas[svInd]) + b # 返回数组中各元素的正负号，用1和-1表示，并统计错误个数 if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 # 打印错误率 print('训练集错误率:%.2f%%' % ((float(errorCount) / m) * 100)) \"\"\"函数说明：数据可视化Parameters: dataMat - 数据矩阵 labelMat - 数据标签 Returns: None\"\"\"def showDataSet(dataMat, labelMat): # 正样本 data_plus = [] # 负样本 data_minus = [] for i in range(len(dataMat)): if labelMat[i] &gt; 0: data_plus.append(dataMat[i]) else: data_minus.append(dataMat[i]) # 转换为numpy矩阵 data_plus_np = np.array(data_plus) # 转换为numpy矩阵 data_minus_np = np.array(data_minus) # 正样本散点图（scatter） # transpose转置 plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1]) # 负样本散点图（scatter） plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1]) # 显示 plt.show() if __name__ == '__main__': testRbf()","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（五）- 朴素贝叶斯","slug":"Machine_Learning/机器学习笔记（5-朴素贝叶斯）","date":"2019-11-28T07:30:00.000Z","updated":"2020-02-11T08:48:55.558Z","comments":true,"path":"2019/11/28/Machine_Learning/机器学习笔记（5-朴素贝叶斯）/","link":"","permalink":"https://shyshy903.github.io/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%89/","excerpt":"","text":"贝叶斯贝叶斯定理$p(c|x) = \\frac{p(x|c)p(c)}{p(x)}$其中c为类别，x为实例具有特征${x_1,x_2,…x_i}$如果 $p(c_1|x) &gt; p(c_2|x)$，那么就属于类别c1,反之属于c2 朴素贝叶斯的一般过程 收集数据，可以使用RSS源 准备数据，需要使用数值型或者布尔型 分析数据，大量特征时，可以绘制直方图 训练算法，计算不同的独立特征的条件概率 测试算法，计算错误率 使用算法，封装贝叶斯分类器 文本分类准备数据，从文本中构建词向量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172\"\"\"函数说明：创建实验样本Parameters: None Returns: postingList - 实验样本切分的词条 classVec - 类别标签向量\"\"\"def loadDataSet(): # 切分的词条 postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] # 类别标签向量，1代表侮辱性词汇，0代表不是 classVec = [0, 1, 0, 1, 0, 1] # 返回实验样本切分的词条、类别标签向量 return postingList, classVec\"\"\"函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表Parameters: dataSet - 整理的样本数据集 Returns: vocabSet - 返回不重复的词条列表，也就是词汇表\"\"\"def createVocabList(dataSet): # 创建一个空的不重复列表 # set是一个无序且不重复的元素集合 vocabSet = set([]) for document in dataSet: # 取并集 vocabSet = vocabSet | set(document) return list(vocabSet)\"\"\"函数说明：根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0Parameters: vocabList - createVocabList返回的列表 inputSet - 切分的词条列表 Returns: returnVec - 文档向量，词集模型\"\"\"def setOfWords2Vec(vocabList, inputSet): # 创建一个其中所含元素都为0的向量 returnVec = [0] * len(vocabList) # 遍历每个词条 for word in inputSet: if word in vocabList: # 如果词条存在于词汇表中，则置1 # index返回word出现在vocabList中的索引 # 若这里改为+=则就是基于词袋的模型，遇到一个单词会增加单词向量中德对应值 returnVec[vocabList.index(word)] = 1 else: print(\"the word: %s is not in my Vocabulary\" % word) # 返回文档向量 return returnVec&gt;&gt; imort bayes&gt;&gt; listOposts, listClasses = bayes.loadDataSet()&gt;&gt; myVocabList = bayes.createVocabList(listOpists)&gt;&gt; bayes.setOfWords2Vec(myVocabList, listOpists)[0,0,1,0.........] 训练算法，从词向量计算概率$p(c|w) = \\frac{p(w|c)p(c)}{p(w)}$$p(c) = \\frac{类别c的实例数}{总的实例数}$$p(w|c) = p(w_0,w_1,w_2,w_3,w_4 |c_i)$$p(w|c) = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)…p(w_n|c_i)$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144计算每个类别中的文档数目对每篇训练文档 对每个类被 如果词条出现在文档中，增加该词条的计数值 增加所有词条的计数值 对每个类别 对每个词条： 将该词条的数目除以总的词条数目1得到条件概率 返回每个类别的条件概率\"\"\"函数说明：朴素贝叶斯分类器训练函数Parameters: trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵 trainCategory - 训练类标签向量，即loadDataSet返回的classVec Returns: p0Vect - 侮辱类的条件概率数组 p1Vect - 非侮辱类的条件概率数组 pAbusive - 文档属于侮辱类的概率\"\"\"def trainNB0(trainMatrix, trainCategory): # 计算训练文档数目 numTrainDocs = len(trainMatrix) # 计算每篇文档的词条数目 numWords = len(trainMatrix[0]) # 文档属于侮辱类的概率 pAbusive = sum(trainCategory)/float(numTrainDocs) # 创建numpy.zeros数组，词条出现数初始化为0 # p0Num = np.zeros(numWords) # p1Num = np.zeros(numWords) # 创建numpy.ones数组，词条出现数初始化为1,拉普拉斯平滑 p0Num = np.ones(numWords) p1Num = np.ones(numWords) # 分母初始化为0 # p0Denom = 0.0 # p1Denom = 0.0 # 分母初始化为2，拉普拉斯平滑 p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): # 统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)... if trainCategory[i] == 1: # 统计所有侮辱类文档中每个单词出现的个数 p1Num += trainMatrix[i] # 统计一共出现的侮辱单词的个数 p1Denom += sum(trainMatrix[i]) # 统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)... else: # 统计所有非侮辱类文档中每个单词出现的个数 p0Num += trainMatrix[i] # 统计一共出现的非侮辱单词的个数 p0Denom += sum(trainMatrix[i]) # 每个侮辱类单词分别出现的概率 # p1Vect = p1Num / p1Denom # 取对数，防止下溢出 p1Vect = np.log(p1Num / p1Denom) # 每个非侮辱类单词分别出现的概率 # p0Vect = p0Num / p0Denom # 取对数，防止下溢出 p0Vect = np.log(p0Num / p0Denom) # 返回属于侮辱类的条件概率数组、属于非侮辱类的条件概率数组、文档属于侮辱类的概率 return p0Vect, p1Vect, pAbusive``` ### 测试算法，改进为了避免$p(w|c) = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)...p(w_n|c_i)$中出现某一项为0的情况，这里采用对数矫正：$ln(a*b) = lna + lnb$```python\"\"\"函数说明：朴素贝叶斯分类器分类函数Parameters: vec2Classify - 待分类的词条数组 p0Vec - 侮辱类的条件概率数组 p1Vec - 非侮辱类的条件概率数组 pClass1 - 文档属于侮辱类的概率 Returns: 0 - 属于非侮辱类 1 - 属于侮辱类Modify: 2018-07-21\"\"\"def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): # 对应元素相乘 # p1 = reduce(lambda x,y:x*y, vec2Classify * p1Vec) * pClass1 # p0 = reduce(lambda x,y:x*y, vec2Classify * p0Vec) * (1.0 - pClass1) # 对应元素相乘，logA*B = logA + logB所以这里是累加 p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) # print('p0:', p0) # print('p1:', p1) if p1 &gt; p0: return 1 else: return 0\"\"\"函数说明：测试朴素贝叶斯分类器Parameters: None Returns: NoneModify: 2018-07-21\"\"\"def testingNB(): # 创建实验样本 listOPosts, listclasses = loadDataSet() # 创建词汇表,将输入文本中的不重复的单词进行提取组成单词向量 myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: # 将实验样本向量化若postinDoc中的单词在myVocabList出现则将returnVec该位置的索引置1 # 将6组数据list存储在trainMat中 trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) # 训练朴素贝叶斯分类器 p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listclasses)) # 测试样本1 testEntry = ['love', 'my', 'dalmation'] # 测试样本向量化返回这三个单词出现位置的索引 thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) if classifyNB(thisDoc, p0V, p1V, pAb): # 执行分类并打印结果 print(testEntry, '属于侮辱类') else: # 执行分类并打印结果 print(testEntry, '属于非侮辱类') # 测试样本2 testEntry = ['stupid', 'garbage'] # 将实验样本向量化 thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) if classifyNB(thisDoc, p0V, p1V, pAb): # 执行分类并打印结果 print(testEntry, '属于侮辱类') else: # 执行分类并打印结果 print(testEntry, '属于非侮辱类')","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（六）- logist回归","slug":"Machine_Learning/机器学习笔记（6- Logistic回归）","date":"2019-11-28T07:30:00.000Z","updated":"2020-02-11T08:48:55.568Z","comments":true,"path":"2019/11/28/Machine_Learning/机器学习笔记（6- Logistic回归）/","link":"","permalink":"https://shyshy903.github.io/2019/11/28/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886-%20Logistic%E5%9B%9E%E5%BD%92%EF%BC%89/","excerpt":"","text":"logist回归最佳回归系数与Sigmod函数$\\sigma = 1/(1 + e^{-z})$$z = w_0x_0+ w_1x_1+ w_2x_2+ ….w_nx_n$$z = w^TX$ 梯度上升法$w = w + α* grad f(w)$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778\"\"\"函数说明：梯度上升算法测试函数 求函数f(x) = -x^2+4x的极大值Parameters: NoneReturns: None\"\"\"def Gradient_Ascent_test(): # f(x)的导数 def f_prime(x_old): return -2 * x_old + 4 # 初始值，给一个小于x_new的值 x_old = -1 # 梯度上升算法初始值，即从(0, 0)开始 x_new = 0 # 步长，也就是学习速率，控制更新的幅度 alpha = 0.01 # 精度，也就是更新阈值 presision = 0.00000001 while abs(x_new - x_old) &gt; presision: x_old = x_new # 利用上面的公式 x_new = x_old + alpha * f_prime(x_old) # 打印最终求解的极值近似值 print(x_new)\"\"\"函数说明：sigmoid函数Parameters: inX - 数据 Returns: sigmoid函数\"\"\"def sigmoid(inX): return 1.0 / (1 + np.exp(-inX))\"\"\"函数说明：梯度上升法Parameters: dataMath - 数据集 classLabels - 数据标签 Returns: weights.getA() - 求得的权重数组（最优参数） weights_array - 每次更新的回归系数\"\"\"def gradAscent(dataMath, classLabels): # 转换成numpy的mat(矩阵) dataMatrix = np.mat(dataMath) # 转换成numpy的mat(矩阵)并进行转置 labelMat = np.mat(classLabels).transpose() # 返回dataMatrix的大小，m为行数，n为列数 m, n = np.shape(dataMatrix) # 移动步长，也就是学习效率，控制更新的幅度 alpha = 0.01 # 最大迭代次数 maxCycles = 500 weights = np.ones((n, 1)) weights_array = np.array([]) for k in range(maxCycles): # 梯度上升矢量化公式 h = sigmoid(dataMatrix * weights) error = labelMat - h weights = weights + alpha * dataMatrix.transpose() * error # numpy.append(arr, values, axis=None):就是arr和values会重新组合成一个新的数组，做为返回值。 # 当axis无定义时，是横向加成，返回总是为一维数组 weights_array = np.append(weights_array, weights) weights_array = weights_array.reshape(maxCycles, n) # 将矩阵转换为数组，返回权重数组 # mat.getA()将自身矩阵变量转化为ndarray类型变量 return weights.getA(), weights_array 绘制决策边界12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455\"\"\"函数说明：绘制数据集Parameters: weights - 权重参数数组 Returns: None\"\"\"def plotBestFit(weights): # 加载数据集 dataMat, labelMat = loadDataSet() # 转换成numpy的array数组 dataArr = np.array(dataMat) # 数据个数 # 例如建立一个4*2的矩阵c，c.shape[1]为第一维的长度2， c.shape[0]为第二维的长度4 n = np.shape(dataMat)[0] # 正样本 xcord1 = [] ycord1 = [] # 负样本 xcord2 = [] ycord2 = [] # 根据数据集标签进行分类 for i in range(n): if int(labelMat[i]) == 1: # 1为正样本 xcord1.append(dataArr[i, 1]) ycord1.append(dataArr[i, 2]) else: # 0为负样本 xcord2.append(dataArr[i, 1]) ycord2.append(dataArr[i, 2]) # 新建图框 fig = plt.figure() # 添加subplot ax = fig.add_subplot(111) # 绘制正样本 ax.scatter(xcord1, ycord1, s=20, c='red', marker='s', alpha=.5) # 绘制负样本 ax.scatter(xcord2, ycord2, s=20, c='green', alpha=.5) # x轴坐标 x = np.arange(-3.0, 3.0, 0.1) # w0*x0 + w1*x1 * w2*x2 = 0 # x0 = 1, x1 = x, x2 = y y = (-weights[0] - weights[1] * x) / weights[2] ax.plot(x, y) # 绘制title plt.title('BestFit') # 绘制label plt.xlabel('x1') plt.ylabel('y2') # 显示 plt.show() 随机梯度上升法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113\"\"\"函数说明：改进的随机梯度上升法Parameters: dataMatrix - 数据数组 classLabels - 数据标签 numIter - 迭代次数 Returns: weights - 求得的回归系数数组（最优参数） weights_array - 每次更新的回归系数\"\"\"def stocGradAscent1(dataMatrix, classLabels, numIter=150): # 返回dataMatrix的大小，m为行数，n为列数 m, n = np.shape(dataMatrix) # 参数初始化 weights = np.ones(n) weights_array = np.array([]) for j in range(numIter): dataIndex = list(range(m)) for i in range(m): # 每次都降低alpha的大小 alpha = 4/(1.0+j+i)+0.01 # 随机选择样本 randIndex = int(random.uniform(0, len(dataIndex))) # 随机选择一个样本计算h h = sigmoid(sum(dataMatrix[randIndex] * weights)) # 计算误差 error = classLabels[randIndex] - h # 更新回归系数 weights = weights + alpha * error * dataMatrix[randIndex] # 添加返回系数到数组中当axis为0时，数组是加在下面（列数要相同） weights_array = np.append(weights_array, weights, axis=0) # 删除已使用的样本 del(dataIndex[randIndex]) # 改变维度 weights_array = weights_array.reshape(numIter*m, n) # 返回 return weights, weights_array\"\"\"函数说明：绘制回归系数与迭代次数的关系Parameters: weights_array1 - 回归系数数组1 weights_array2 - 回归系数数组2 Returns: None\"\"\"def plotWeights(weights_array1, weights_array2): # 设置汉字格式为14号简体字 font = FontProperties(fname=r\"C:\\Windows\\Fonts\\simsun.ttc\", size=14) # 将fig画布分隔成1行1列，不共享x轴和y轴，fig画布的大小为（20, 10） # 当nrows=3，ncols=2时，代表fig画布被分为6个区域，axs[0][0]代表第一行第一个区域 fig, axs = plt.subplots(nrows=3, ncols=2, sharex=False, sharey=False, figsize=(20, 10)) # x1坐标轴的范围 x1 = np.arange(0, len(weights_array1), 1) # 绘制w0与迭代次数的关系 axs[0][0].plot(x1, weights_array1[:, 0]) axs0_title_text = axs[0][0].set_title(u'改进的梯度上升算法，回归系数与迭代次数关系', FontProperties=font) axs0_ylabel_text = axs[0][0].set_ylabel(u'w0', FontProperties=font) plt.setp(axs0_title_text, size=20, weight='bold', color='black') plt.setp(axs0_ylabel_text, size=20, weight='bold', color='black') # 绘制w1与迭代次数的关系 axs[1][0].plot(x1, weights_array1[:, 1]) axs1_ylabel_text = axs[1][0].set_ylabel(u'w1', FontProperties=font) plt.setp(axs1_ylabel_text, size=20, weight='bold', color='black') # 绘制w2与迭代次数的关系 axs[2][0].plot(x1, weights_array1[:, 2]) axs2_title_text = axs[2][0].set_title(u'迭代次数', FontProperties=font) axs2_ylabel_text = axs[2][0].set_ylabel(u'w2', FontProperties=font) plt.setp(axs2_title_text, size=20, weight='bold', color='black') plt.setp(axs2_ylabel_text, size=20, weight='bold', color='black') # x2坐标轴的范围 x2 = np.arange(0, len(weights_array2), 1) # 绘制w0与迭代次数的关系 axs[0][1].plot(x2, weights_array2[:, 0]) axs0_title_text = axs[0][1].set_title(u'梯度上升算法，回归系数与迭代次数关系', FontProperties=font) axs0_ylabel_text = axs[0][1].set_ylabel(u'w0', FontProperties=font) plt.setp(axs0_title_text, size=20, weight='bold', color='black') plt.setp(axs0_ylabel_text, size=20, weight='bold', color='black') # 绘制w1与迭代次数的关系 axs[1][1].plot(x2, weights_array2[:, 1]) axs1_ylabel_text = axs[1][1].set_ylabel(u'w1', FontProperties=font) plt.setp(axs1_ylabel_text, size=20, weight='bold', color='black') # 绘制w2与迭代次数的关系 axs[2][1].plot(x2, weights_array2[:, 2]) axs2_title_text = axs[2][1].set_title(u'迭代次数', FontProperties=font) axs2_ylabel_text = axs[2][1].set_ylabel(u'w2', FontProperties=font) plt.setp(axs2_title_text, size=20, weight='bold', color='black') plt.setp(axs2_ylabel_text, size=20, weight='bold', color='black') plt.show() if __name__ == '__main__': # 测试简单梯度上升法 # Gradient_Ascent_test() # 加载数据集 dataMat, labelMat = loadDataSet() # 训练权重 weights2, weights_array2 = gradAscent(dataMat, labelMat) # 新方法训练权重 weights1, weights_array1 = stocGradAscent1(np.array(dataMat), labelMat) # 绘制数据集中的y和x的散点图 # plotBestFit(weights) # print(gradAscent(dataMat, labelMat)) plotWeights(weights_array1, weights_array2) 梯度下降法$w = w - α*gradf(w)$","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（四）- 决策树","slug":"Machine_Learning/机器学习笔记（4-DecisionTree)","date":"2019-11-27T07:30:00.000Z","updated":"2020-02-11T08:48:55.554Z","comments":true,"path":"2019/11/27/Machine_Learning/机器学习笔记（4-DecisionTree)/","link":"","permalink":"https://shyshy903.github.io/2019/11/27/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%884-DecisionTree)/","excerpt":"","text":"决策树决策树本质上是一种流程图，长方形代表判断模块，椭圆代表终止模块，左右箭头指引节点的上下分支决策树相比较于KNN，其重要的原因就是其数据形式非常容易理解，而KNN的数据形式所包含的内在含义却不是很容易理解 优点计算复杂度不高，可以处理不相关特征数据，对中间数据的缺省值不敏感缺点会产生过度匹配问题 信息论划分数据集12345678910\"\"\"划分数据集的伪代码\"\"\"检测数据集是否属于同一类： if so return 类标签 else: 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子类 调用函数createBranch并增加返回节点到结果上 return 分支节点 决策树的流程 收集数据：可以使用任何方法 准备数据：树构造算法适用于标称型数据，如果数据是数值型数据，需要先把数据进行离散化 分析数据：构造树完成，进行检查 训练算法：构造树的数据结构 测试算法：使用经验树计算错误率 使用算法 信息增益与信息熵划分数据集的最大原则是，将无序的数据变得更加有序信息增益：划分数据集前后信息发生的变化称为信息增益计算每个特征划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择如何计算信息增益？集合信息的度量方式称为香农熵熵定义为信息的期望值：符号$x_i$的信息定义为$l(x_i) = -log_2p(x_i)$，p(x_i)是选择该分类的概率则信息熵为：$H = -\\sum\\nolimits_{i=1}^{n}p(x_i)log_2p(x_i)$,其中n是分类的数目 计算信息熵的源代码123456789101112131415161718192021222324252627282930313233343536373839\"\"\"用于计算给定的信息熵\"\"\"\"\"\"函数说明：计算给定数据集的经验熵（香农熵） H = -SUM（kp*Log2（kp））Parameters: dataSet - 数据集 Returns: shannonEnt - 经验熵（香农熵）\"\"\"def calcShannonEnt(dataSet): # 返回数据集的行数 numEntires = len(dataSet) # 保存每个标签（Label）出现次数的“字典” labelCounts = &#123;&#125; # 对每组特征向量进行统计 for featVec in dataSet: # 按行进行遍历 # 提取标签（Label）信息 currentLabel = featVec[-1] # 取每一行最后一列特征值 # 如果标签（Label）没有放入统计次数的字典，添加进去 if currentLabel not in labelCounts.keys(): # 创建一个新的键值对，键为currentLabel值为0 labelCounts[currentLabel] = 0 # Label计数 labelCounts[currentLabel] += 1 # 经验熵（香农熵） shannonEnt = 0.0 # 计算香农熵 for key in labelCounts: # 选择该标签（Label）的概率 prob = float(labelCounts[key]) / numEntires # 利用公式计算 shannonEnt -= prob*log(prob, 2) # 返回经验熵（香农熵） return shannonEnt 创建数据集 12345678def createDataSet()： dataSet = [[1, ,1 , 'yes'],[1, 1, 'yes'],[1, 0, 'no'],[0, 1, 'no'],[0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels&gt;&gt; reload(trees.py)&gt;&gt; myDat, labels = trees.createDataSet()&gt;&gt; trees.calcShannonEnt(mydata) 划分数据集的算法与代码一般可用二分法划分数据集，这里采用ID3算法进行划分数据集香农熵可用来度量数据集的无序度，数据无序度越大，香农熵值越大。分类算法除了需要测量信息熵还需要一个度量数据划分准确度的熵值，这就像在二维坐标中画直线进行划分平面坐标系、 按照给定特征划分数据集1234567891011121314151617181920212223\"\"\"函数：按照给定的特征划分数据集输入： dataSet,数据集 axis, 划分数据集的特征 value, 需要返回的特征值Return: retDataSet,划分的数据集\"\"\"def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: # 按行遍历数据集 if featVec[axis] == value: # 去掉特征为axis的数据集 reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) # 扩展列表元素 retDataSet.append(reducedFeatVec) # 添加嵌套列表 return retDataSet&gt;&gt; import DecisionTree as DT&gt;&gt; mydat, labels = DT.createDataSet()&gt;&gt; DT.splitDataSet(mydat, 0, 0)&gt;&gt; DT.splitDataSet(mydat, 0, 1) 选择最好的数据集划分方式1234567891011121314151617181920212223242526272829\"\"\"函数：选择最好的数据集划分方式Para: dataSet:数据集return: bestFeature:最好的特征的索引值\"\"\"def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 # 特征的个数-1 baseEntropy = calcShannonEnt(dataSet) # 计算数据集的香农熵 bestInfoGain = 0.0 ; bestFeature = -1 # 最大信息增益和最优划分特征的索引值 for i in range(numFeatures): # 遍历特征 featList = [example[i] for example in dataSet] # 列表生成式生成特征所有的取值 uniqueVals = set(featList) # 删去重复值 newEntropy = 0.0 # 香农熵 for values in uniqueVals: # 遍历特征值 subDataSet = splitDataSet(dataset, i , value) # 划分数据集 prob = len(subDataSet) / float(len(dataSet)) # 计算概率 newEntropy += prob * calcShannonEnt(subDataSet) # 计算经验条件熵 infoGain = baseEntropy - newEntropy # 信息增益值 print(\"第%d个特征的增益为%.3f\" % (i, infoGain)) # 打印每个特征的信息增益，取正 if(infoGain &gt; baseEntropy): # 找到对应最大信息增益的特征 baseInfoGain = infoGain bestFeature = i return bestFeature&gt;&gt; DecisionTree.chooseBestFeaturToSplit(mydat) 递归构建决策树 当特征值多与两个的时候，就可能存在大于两个分支的数据集划分，这时我们就通过递归调用来实现它！递归结束的条件程序遍历完所有划分数据的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例都具有相同的分类，则得到一个叶子节点或者终止快。任何到达叶子结点所属的分类必然属于叶子节点的类。 12345678910111213141516171819202122\"\"\"函数说明：统计classList中出现次数最多的元素（类标签）Parameters: classList - 类标签列表 Returns: sortedClassCount[0][0] - 出现次数最多的元素（类标签） \"\"\" def majorityCnt(classList): classCount = &#123;&#125; # 统计classList中每个元素出现的次数 for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 # 根据字典的值降序排序 # operator.itemgetter(1)获取对象的第1列的值 sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True) # 返回classList中出现次数最多的元素 return sortedClassCount[0][0] 创建决策树的代码 1234567891011121314151617181920212223242526272829303132\"\"\"函数说明：创建决策树（ID3算法） 递归有两个终止条件：1、所有的类标签完全相同，直接返回类标签 2、用完所有标签但是得不到唯一类别的分组，即特征不够用，挑选出现数量最多的类别作为返回Parameters: dataSet - 训练数据集 labels - 分类属性标签 featLabels - 存储选择的最优特征标签 Returns: myTree - 决策树\"\"\"def createTree(dataSet, labels, featLabels): classList = [example[-1] for example in dataSet] # 取分类标签（是否放贷：yes or no） if classList.count(classList[0]) == len(classList): # 如果类别完全相同则停止继续划分 return classList[0] if len(dataSet[0]) == 1: # 遍历完所有特征时返回出现次数最多的类标签 return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 选择最优特征 bestFeatLabel = labels[bestFeat] # 最优特征的标签 featLabels.append(bestFeatLabel) myTree = &#123;bestFeatLabel:&#123;&#125;&#125; # 根据最优特征的标签生成树 del(labels[bestFeat]) # 删除已经使用的特征标签 featValues = [example[bestFeat] for example in dataSet] # 得到训练集中所有最优解特征的属性值 uniqueVals = set(featValues) # 去掉重复的属性值 for value in uniqueVals: # 遍历特征，创建决策树 myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels) return myTree&gt;&gt; my tree = DecisionTree.createTree(mydata,labels)&gt;&gt; my tree 测试算法与分类器测试算法构造分类器12345678910111213141516171819202122\"\"\"函数：使用决策树的分类函数para: inputree, featLabels, testVecreturn: classlabel,分类器\"\"\"def classify(inputTree, featLabels, testVec): # 获取决策树结点 firstStr = next(iter(inputTree)) # 下一个字典 secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key], featLabels, testVec) else: classLabel = secondDict[key] return classLabel 使用算法12345678910111213141516171819202122232425262728\"\"\"函数说明：存储决策树Parameters: inputTree - 已经生成的决策树 filename - 决策树的存储文件名 Returns: None\"\"\" def storeTree(inputTree, filename): with open(filename, 'wb') as fw: pickle.dump(inputTree, fw)\"\"\"函数说明：读取决策树Parameters: filename - 决策树的存储文件名 Returns: pickle.load(fr) - 决策树字典\"\"\" def grabTree(filename): fr = open(filename, 'rb') return pickle.load(fr)","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"色彩搭配与设计","slug":"设计/关于颜色","date":"2019-11-26T15:30:00.000Z","updated":"2019-11-26T15:30:00.000Z","comments":true,"path":"2019/11/26/设计/关于颜色/","link":"","permalink":"https://shyshy903.github.io/2019/11/26/%E8%AE%BE%E8%AE%A1/%E5%85%B3%E4%BA%8E%E9%A2%9C%E8%89%B2/","excerpt":"","text":"ColorMaterial design中的color 123456789101112131415161718192021222324&lt;!--Material Colors--&gt; &lt;color name=\"materialColorRed\"&gt;#f44336&lt;/color&gt; &lt;color name=\"materialColorPink\"&gt;#e91e63&lt;/color&gt; &lt;color name=\"materialColorPurple\"&gt;#9c27b0&lt;/color&gt; &lt;color name=\"materialColorDeepPurple\"&gt;#673ab7&lt;/color&gt; &lt;color name=\"materialColorIndigo\"&gt;#3f51b5&lt;/color&gt; &lt;color name=\"materialColorBlue\"&gt;#2196f3&lt;/color&gt; &lt;color name=\"materialColorLightBlue\"&gt;#03a9f4&lt;/color&gt; &lt;color name=\"materialColorCyan\"&gt;#00bcd4&lt;/color&gt; &lt;color name=\"materialColorTeal\"&gt;#009688&lt;/color&gt; &lt;color name=\"materialColorGreen\"&gt;#4caf50&lt;/color&gt; &lt;color name=\"materialColorLightGreen\"&gt;#8bc34a&lt;/color&gt; &lt;color name=\"materialColorLime\"&gt;#cddc39&lt;/color&gt; &lt;color name=\"materialColorYellow\"&gt;#FFeb3b&lt;/color&gt; &lt;color name=\"materialColorAmber\"&gt;#FFc107&lt;/color&gt; &lt;color name=\"materialColorOrange\"&gt;#FF9800&lt;/color&gt; &lt;color name=\"materialColorDeepOrange\"&gt;#FF5722&lt;/color&gt; &lt;color name=\"materialColorBrown\"&gt;#795548&lt;/color&gt; &lt;color name=\"materialColorGrey\"&gt;#9e9e9e&lt;/color&gt; &lt;color name=\"materialColorBlueGrey\"&gt;#607d8b&lt;/color&gt; &lt;!--Text Colors--&gt; &lt;color name=\"primaryText\"&gt;#212121&lt;/color&gt; &lt;color name=\"secondaryText\"&gt;#757575&lt;/color&gt; &lt;color name=\"dividerColor\"&gt;#bdbdbd&lt;/color&gt; 色彩的巧妙搭配 几个色彩网站推荐 拼色网站1： https://colordrop.io/拼色网站2：http://www.peise.net/tools/web/#RGB色值对照表 ：https://tool.oschina.net/commons?type=3 一个免费图片网站https://unsplash.com/","categories":[{"name":"设计","slug":"设计","permalink":"https://shyshy903.github.io/categories/%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"color","slug":"color","permalink":"https://shyshy903.github.io/tags/color/"}]},{"title":"《机器学习实战》《西瓜书》笔记（三）- KNN","slug":"Machine_Learning/机器学习笔记（3-KNN)","date":"2019-11-26T07:30:00.000Z","updated":"2020-02-11T08:48:55.551Z","comments":true,"path":"2019/11/26/Machine_Learning/机器学习笔记（3-KNN)/","link":"","permalink":"https://shyshy903.github.io/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883-KNN)/","excerpt":"","text":"《机器学习实战》《西瓜书》笔记（三）- KNNKNN原理 输入带有标签的训练集 输入没有标签的新数据 算法将输入数据的特征与训练集的数据的特征进行比较 求新数据与样本集中数据的距离 算法提取样本集中最相似数据（最近邻）的分类标签，只选择前K个最相似的数据 选取k个相似数据频率最多的分类属性作为新数据的分类属性 KNN伪代码 计算已知类别数据集中点与当前点的距离 按照距离从小到大递增排序 选取与当前点距离最小的k个点 确定前k个点所在类别出现的频率 返回前k个点出现频率最高的类别作为当前点的预测分类 KNN源代码123456789101112131415161718192021222324252627282930def classify0(inX, dataSet, labels, k): # numpy函数shape[0]返回dataSet的行数 dataSetSize = dataSet.shape[0] # 将inX重复dataSetSize次并排成一列 diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet # 二维特征相减后平方（用diffMat的转置乘diffMat） sqDiffMat = diffMat**2 # sum()所有元素相加，sum(0)列相加，sum(1)行相加 sqDistances = sqDiffMat.sum(axis=1) # 开方，计算出距离 distances = sqDistances**0.5 # argsort函数返回的是distances值从小到大的--索引值 sortedDistIndicies = distances.argsort() # 定义一个记录类别次数的字典 classCount = &#123;&#125; # 选择距离最小的k个点 for i in range(k): # 取出前k个元素的类别 voteIlabel = labels[sortedDistIndicies[i]] # 字典的get()方法，返回指定键的值，如果值不在字典中返回0 # 计算类别次数 classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # python3中用items()替换python2中的iteritems() # key = operator.itemgetter(1)根据字典的值进行排序 # key = operator.itemgetter(0)根据字典的键进行排序 # reverse降序排序字典 sortedClassCount = sorted(classCount.items(),\\ key = operator.itemgetter(1), reverse = True) # 返回次数最多的类别，即所要分类的类别 return sortedClassCount[0][0] 越预测数据所在的分类在终端中的交互解释器执行： 1&gt;&gt; classify([1.0, 1.0], group, lables, 5) 即可测试得到一个结果 简单的一个示例数据的准备在KNN的模块中添加一个实现数据样本的函数，代码如下所示： 123456import numpy as npimport operatordef creatDataSet(): group = np.array([1, 1], [1, 1.1], [0, 0], [0, 0.1]) labels = ['A', 'A', 'B', 'B'] return group, labels 在交互解释器中执行: 1234&gt;&gt; import KNN&gt;&gt; group, lables = KNN.creatDataSet()&gt;&gt; KNN.classify0([0,0], group, labels, 3)&gt;&gt; B 可生成一个初步应用于KNN的4维2列的数据集 如何测试分类器使用分类器的错误率，即分类器给出的错误结果除以测试执行的综述来判断分类器的好坏 示例1 KNN进行约会匹配执行流程 收集数据 ： 导入文本文件 准备数据: 使用python解析 分析数据： matplotlib绘图 训练算法：k-近邻不适用 测试算法 使用算法准备数据123456789101112131415161718192021222324252627282930313233343536373839404142\"\"\"函数说明：打开解析文件，对数据进行分类，1代表不喜欢，2代表魅力一般，3代表极具魅力Parameters: filename - 文件名 Returns: returnMat - 特征矩阵 classLabelVector - 分类label向量\"\"\"def file2matrix(filename): # 打开文件 fr = open(filename) # 读取文件所有内容 arrayOlines = fr.readlines() # 得到文件行数 numberOfLines = len(arrayOlines) # 返回的NumPy矩阵numberOfLines行，3列 returnMat = np.zeros((numberOfLines, 3)) # 创建分类标签向量 classLabelVector = [] # 行的索引值 index = 0 # 读取每一行 for line in arrayOlines: # 去掉每一行首尾的空白符，例如'\\n','\\r','\\t',' ' line = line.strip() # 将每一行内容根据'\\t'符进行切片,本例中一共有4列 listFromLine = line.split('\\t') # 将数据的前3列进行提取保存在returnMat矩阵中，也就是特征矩阵 returnMat[index,:] = listFromLine[0:3] # 根据文本内容进行分类1：不喜欢；2：一般；3：喜欢 if listFromLine[-1] == 'didntLike': classLabelVector.append(1) elif listFromLine[-1] == 'smallDoses': classLabelVector.append(2) elif listFromLine[-1] == 'largeDoses': classLabelVector.append(3) index += 1 # 返回标签列向量以及特征矩阵 return returnMat, classLabelVector 分析数据，使用matplotlib进行画图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970\"\"\"函数说明：可视化数据Parameters: datingDataMat - 特征矩阵 datingLabels - 分类Label Returns: None Modify: 2018-07-13\"\"\"def showdatas(datingDataMat, datingLabels): # 设置汉字格式为14号简体字 font = FontProperties(fname=r\"C:\\Windows\\Fonts\\simsun.ttc\", size=14) # 将fig画布分隔成1行1列，不共享x轴和y轴，fig画布的大小为（13，8） # 当nrows=2，ncols=2时，代表fig画布被分为4个区域，axs[0][0]代表第一行第一个区域 fig, axs = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(13, 8)) # 获取datingLabels的行数作为label的个数 # numberOfLabels = len(datingLabels) # label的颜色配置矩阵 LabelsColors = [] for i in datingLabels: # didntLike if i == 1: LabelsColors.append('black') # smallDoses if i == 2: LabelsColors.append('orange') # largeDoses if i == 3: LabelsColors.append('red') # 画出散点图，以datingDataMat矩阵第一列为x，第二列为y，散点大小为15, 透明度为0.5 axs[0][0].scatter(x=datingDataMat[:,0], y=datingDataMat[:,1], color=LabelsColors, s=15, alpha=.5) # 设置标题，x轴label， y轴label axs0_title_text = axs[0][0].set_title(u'每年获得的飞行常客里程数与玩视频游戏所消耗时间占比', FontProperties=font) axs0_xlabel_text = axs[0][0].set_xlabel(u'每年获得的飞行常客里程数', FontProperties=font) axs0_ylabel_text = axs[0][0].set_ylabel(u'玩视频游戏所消耗时间占比', FontProperties=font) plt.setp(axs0_title_text, size=9, weight='bold', color='red') plt.setp(axs0_xlabel_text, size=7, weight='bold', color='black') plt.setp(axs0_ylabel_text, size=7, weight='bold', color='black') # 画出散点图，以datingDataMat矩阵第一列为x，第三列为y，散点大小为15, 透明度为0.5 axs[0][1].scatter(x=datingDataMat[:,0], y=datingDataMat[:,2], color=LabelsColors, s=15, alpha=.5) # 设置标题，x轴label， y轴label axs1_title_text = axs[0][1].set_title(u'每年获得的飞行常客里程数与每周消费的冰淇淋公升数', FontProperties=font) axs1_xlabel_text = axs[0][1].set_xlabel(u'每年获得的飞行常客里程数', FontProperties=font) axs1_ylabel_text = axs[0][1].set_ylabel(u'每周消费的冰淇淋公升数', FontProperties=font) plt.setp(axs1_title_text, size=9, weight='bold', color='red') plt.setp(axs1_xlabel_text, size=7, weight='bold', color='black') plt.setp(axs1_ylabel_text, size=7, weight='bold', color='black') # 画出散点图，以datingDataMat矩阵第二列为x，第三列为y，散点大小为15, 透明度为0.5 axs[1][0].scatter(x=datingDataMat[:,1], y=datingDataMat[:,2], color=LabelsColors, s=15, alpha=.5) # 设置标题，x轴label， y轴label axs2_title_text = axs[1][0].set_title(u'玩视频游戏所消耗时间占比与每周消费的冰淇淋公升数', FontProperties=font) axs2_xlabel_text = axs[1][0].set_xlabel(u'玩视频游戏所消耗时间占比', FontProperties=font) axs2_ylabel_text = axs[1][0].set_ylabel(u'每周消费的冰淇淋公升数', FontProperties=font) plt.setp(axs2_title_text, size=9, weight='bold', color='red') plt.setp(axs2_xlabel_text, size=7, weight='bold', color='black') plt.setp(axs2_ylabel_text, size=7, weight='bold', color='black') # 设置图例 didntLike = mlines.Line2D([], [], color='black', marker='.', markersize=6, label='didntLike') smallDoses = mlines.Line2D([], [], color='orange', marker='.', markersize=6, label='smallDoses') largeDoses = mlines.Line2D([], [], color='red', marker='.', markersize=6, label='largeDoses') # 添加图例 axs[0][0].legend(handles=[didntLike, smallDoses, largeDoses]) axs[0][1].legend(handles=[didntLike, smallDoses, largeDoses]) axs[1][0].legend(handles=[didntLike, smallDoses, largeDoses]) # 显示图片 plt.show() 准备数据，归一化12345678910111213141516171819202122232425262728293031\"\"\"函数说明：对数据进行归一化Parameters: dataSet - 特征矩阵 Returns: normDataSet - 归一化后的特征矩阵 ranges - 数据范围 minVals - 数据最小值 Modify: 2018-07-13\"\"\"def autoNorm(dataSet): # 获取数据的最小值 minVals = dataSet.min(0) # 获取数据的最大值 maxVals = dataSet.max(0) # 最大值和最小值的范围 ranges = maxVals - minVals # shape(dataSet)返回dataSet的矩阵行列数 normDataSet = np.zeros(np.shape(dataSet)) # numpy函数shape[0]返回dataSet的行数 m = dataSet.shape[0] # 原始值减去最小值（x-xmin） normDataSet = dataSet - np.tile(minVals, (m, 1)) # 差值处以最大值和最小值的差值（x-xmin）/（xmax-xmin） normDataSet = normDataSet / np.tile(ranges, (m, 1)) # 归一化数据结果，数据范围，最小值 return normDataSet, ranges, minVals 测试算法，作为完整程序验证分类器1234567891011121314151617181920212223242526272829303132333435363738\"\"\"函数说明：分类器测试函数Parameters: None Returns: normDataSet - 归一化后的特征矩阵 ranges - 数据范围 minVals - 数据最小值 Modify: 2018-07-13\"\"\"def datingClassTest(): # 打开文件名 filename = \"datingTestSet.txt\" # 将返回的特征矩阵和分类向量分别存储到datingDataMat和datingLabels中 datingDataMat, datingLabels = file2matrix(filename) # 取所有数据的10% hoRatio越小，错误率越低 hoRatio = 0.10 # 数据归一化，返回归一化数据结果，数据范围，最小值 normMat, ranges, minVals = autoNorm(datingDataMat) # 获取normMat的行数 m = normMat.shape[0] # 10%的测试数据的个数 numTestVecs = int(m * hoRatio) # 分类错误计数 errorCount = 0.0 for i in range(numTestVecs): # 前numTestVecs个数据作为测试集，后m-numTestVecs个数据作为训练集 # k选择label数+1（结果比较好） classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:],\\ datingLabels[numTestVecs:m], 4) print(\"分类结果:%d\\t真实类别:%d\" % (classifierResult, datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print(\"错误率:%f%%\" % (errorCount/float(numTestVecs)*100)) 使用算法123456789101112131415161718192021222324252627282930313233\"\"\"函数说明：通过输入一个人的三围特征，进行分类输出Parameters: None Returns: None Modify: 2018-07-14\"\"\"def classifyPerson(): # 输出结果 resultList = ['讨厌', '有些喜欢', '非常喜欢'] # 三维特征用户输入 percentTats = float(input(\"玩视频游戏所消耗时间百分比：\")) ffMiles = float(input(\"每年获得的飞行常客里程数：\")) iceCream = float(input(\"每周消费的冰淇淋公升数：\")) # 打开的文件名 filename = \"datingTestSet.txt\" # 打开并处理数据 datingDataMat, datingLabels = file2matrix(filename) # 训练集归一化 normMat, ranges, minVals = autoNorm(datingDataMat) # 生成NumPy数组，测试集 inArr = np.array([percentTats, ffMiles, iceCream]) # 测试集归一化 norminArr = (inArr - minVals) / ranges # 返回分类结果 classifierResult = classify0(norminArr, normMat, datingLabels, 4) # 打印结果 print(\"你可能%s这个人\" % (resultList[classifierResult - 1]))","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（二）- 模型评估与选择","slug":"Machine_Learning/机器学习笔记（2）","date":"2019-11-26T07:30:00.000Z","updated":"2020-02-11T08:48:55.547Z","comments":true,"path":"2019/11/26/Machine_Learning/机器学习笔记（2）/","link":"","permalink":"https://shyshy903.github.io/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89/","excerpt":"","text":"《机器学习实战》《西瓜书》笔记（二）- 模型评估与选择经验误差与过拟合 错误率与精度错误率是分类错误的样本数占样本总数的比例，精度是分类正确的样本数占样本总数的比例。 误差与经验误差学习器的实际预测输出与样本的真实输出之间的差异称为“误差”，学习器在训练集上的误差称为训练误差/经验误差，在新样本上的误差称为“泛化误差”。 过拟合与欠拟合然而，当学习器把训练样本学得”太 好”了的时候，很可能巳经把训练样本自身的一些特点当作了所有潜在样本都 会具有的一般性质，这样就会导致泛化性能下降，叫做 过拟合（overfitting)训练样本的一般性质尚未学好叫做 欠拟合 评估方法通常我们使用测试集上样本产生的泛化误差作为评估标准，就是以“测试误差”来代表学习器的“泛化误差” 划分数据集(D)与测试机(T)的常用方法留出法“留出法”直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为训练集T。训练/测试集的划分要尽可能保持数据分布的一致性，例如在分类任务中至少要保持样本的类别比例相似。如果从采样的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”。单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。通常将2/3~4/5的样本用于训练，剩余样本用于测试。 交叉验证法先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从D中通过分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，通常又称为“k折交叉验证”。与留出法相似，将数据集划分为k个子集同样存在多种划分方式，为减少因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值。假定数据集中包含m个样本，若令k=m，则得到了交叉验证法的一个特例：留一法。 自助法为减少训练样本规模不同造成的影响，同时比较高效地进行实验估计。自助法（bootstrapping）以自助采样法为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集D’：每次随机从D中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集D’。显然，D中有一部分样本在D’中多次出现，而一部分样本不出现。通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D’中。于是将D’用作训练集，D/D’用作测试集。这样的测试结果，亦称“包外估计”。没有留出法，和交叉验证法常用 调参与最终模型大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模 型的性能往往有显著差别.因此，在进行模型评估与选择时，除了要对适用学习 算法进行选择，还需对算法参数进行设定，这就是通常所说的”参数调节”或 简称”调参” (parameter tuning).现实中常用的做法?是对每个参数选定一个 范围和变化步长，例如在 [0 ，0.2] 范围内以 0.05 为步长，则实际要评估的候选参 数值有 5 个，最终是从这 5 个候选值中产生选定值.显然，这样选定的参数值往 往不是”最佳”值，但这是在计算开销和性能估计之间进行折中的结果，通过 这个折中，学习过程才变得可行.事实上，即便在进行这样的折中后，调参往往 仍很困难.可以简单估算一下:假定算法有 3 个参数，每个参数仅考虑 5 个候选 值，这样对每一组训练/测试集就有 53 = 125 个模型需考察另外，需注意的是，我们通常把学得模型在实际使用中遇到的数据称为测 试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为”验 证集” (validation set). 例如，在研究对比不同算法的泛化性能时，我们用测试 集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分 为训练集和验证集，基于验证集上的性能来进行模型选择和调参. 性能度量对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure). 错误率与精度错误率是分类错误的样本数占样本总数的比例，精度是分类正确的样本数占样本总数的比例。 查准率、查全率、F1对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反倒(true negative) 、 假反例(false negative)四种情形，令 TP、 FP、 TN、 FN 分别表示其对应的 样例数，则显然有 TP+FP+TN+FN=样例总数.分类结果的”泪淆矩 阵” (co时usion matrix),如下表所示 准率和查全率是一对矛盾的度量。查准率-查全率曲线，简称“P-R曲线”。在进行比较时，若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者。如果两个学习器的P-R曲线交叉，难以一般性断言两者孰优孰劣。比较合理的判据是比较P-R曲线下面积的大小。 为综合考虑查准率、查全率的性能度量，“平衡点”即“查准率=查全率”时的取值，更常用的是F1度量。 当对查准率和查全率的重视程度有所不同，F1度量的一般形式Fβ. β>0度量了查全率对查准率的相对重要性。β=1时退化为标准的F1，β>1时查全率有更大影响，β","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"《机器学习实战》《西瓜书》笔记（一）","slug":"Machine_Learning/机器学习笔记(1)","date":"2019-11-26T07:00:00.000Z","updated":"2020-02-11T08:48:55.543Z","comments":true,"path":"2019/11/26/Machine_Learning/机器学习笔记(1)/","link":"","permalink":"https://shyshy903.github.io/2019/11/26/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)/","excerpt":"","text":"《机器学习实战》《西瓜书》笔记（一）机器学习的相关概念我们要做的其实是让机器他有自己学习的能力，也就我们要做的应该machine learning的方向。讲的比较拟人化一点，所谓machine learning的方向，就是你就写段程序，然后让机器人变得了很聪明，他就能够有学习的能力。接下来，你就像教一个婴儿、教一个小孩一样的教他，你并不是写程序让他做到这件事，你是写程序让它具有学习的能力。然后接下来，你就可以用像教小孩的方式告诉它。假设你要叫他学会做语音辨识，你就告诉它这段声音是“Hi”，这段声音就是“How are you”，这段声音是“Good bye”。希望接下来它就学会了，你给它一个新的声音，它就可以帮你产生语音辨识的结果。用数学的语义去理解，机器需要一个函数对输入进行自主判断输出，以完成回归预测、分类、聚类等实际任务 监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。 无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类 半监督学习介于监督学习与无监督学习之间 增强学习机器为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的 开发机器学习应用程序的主要步骤 收集数据 准备输入数据（Python语言） 分析输入数据（数据处理、降维等方法） 训练算法（无监督学习不需要训练算法） 测试算法 执行算法 机器学习的基本术语这组记录的集合称为一个 “数据集” (data set)其中每条记录是关于一个事件或对象(这里是一个西瓜)的描述，称为一个 “示例” (instance) 或”样本” (sample)反映事件或对象在某方面的表现或性质的事项，例如”色泽” “根蒂” “敲声”，称为”)副主” (attribute) 或 “特征”(feature)属性上的取值，例如”青绿” “乌黑”，称为”)副主值” (attribute va1ue)属性张成的空间称为”属性空间” (attribute space)”样本空间” (samp1e space)或”输入空间”例如我们把”色泽” “根蒂” “敲声”作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此我们也把…个示例称为一个 “特征向量” (feature vector).eg:$D = {x_1,x_2,….x_m} $ 样本包含m个实例$x_i = {x_{i1}…..x_{id}}$ 是d维样本空间的一个向量((色泽:青绿;根蒂二蜷缩; 敲声=浊响)，好瓜)” .这里关于示例结果的信息，例如”好瓜”，称为 “标记” (labe1); 拥有了标记信息的示例，则称为”样例” (examp1e). 假设空间归纳是从特殊到一般的“泛化”过程，演绎是从一般到特殊的“特化”过程。学习的目的是“泛化”，即通过对训练集中瓜的学习已获得对没见过的瓜进行判断的能力。学习过程看作一个在所以假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确实，假设空间及其规模大小就却确定了。 我们用 m 表示这 个假设.这样，若”色泽” “根蒂” “敲声”分别有3、 2、 2 种可能取值，则我 们面临的假设空间规模大小为 4 x 3 x 3 + 1 = 37. 归纳偏好现在有三个与训练集一致的假设，但与他们对应的模型在面临新样本的时候，却会产生不同的输出。根据仅有的训练样本无法判断三个假设中哪个“更好”。对于一个具体的学习算法而言，它必须要产生一个模型，这时，学习算法本身的“偏好”起到关键左右。例如，若算法喜欢“尽可能特殊”的模型，则会有相应的模型产生。机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。任何一个有效的机器学习算法必有其归纳偏好，否则产生的模型每次在进行预测时随机抽选训练集上的等效假设，学得模型结果不一，显然没有意义。 归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进 行选择的启发式或”价值观”那么，有没有一般性的原则来引导算法确立 “正确的”偏好呢? “奥卡姆剃刀” (Occam’s razor)是一种常用的、自然科学 研究中最基本的原则，即”若有多个假设与观察一致，则选最简单的那个”如 果采用这个原则，并且假设我们认为”更平滑”意味着”更简单” (例如曲线 A 更易于描述，其方程式是 $y = x2+ 6x + 1$ ，而曲线 B 则要复杂得多)，则在 图1.3 中我们会自然地偏好”平滑”的曲线 A. 生成式模型与判别式模型 产生式模型从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。 判别式模型寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。 区别： 假设有样本输入值（或者观察值）x，类别标签（或者输出值）y 判别式模型评估对象是最大化条件概率p(y|x)并直接对其建模， 生成式模型评估对象是最大化联合概率p(x,y)并对其建模。 其实两者的评估目标都是要得到最终的类别标签Y， 而Y=argmax p(y|x)，不同的是判别式模型直接通过解在满足训练样本分布下的最优化问题得到模型参数，主要用到拉格朗日乘算法、梯度下降法，常见的判别式模型如最大熵模型、CRF、LR、SVM等； 而生成式模型先经过贝叶斯转换成Y = argmax p(y|x) = argmax p(x|y)*p(y)，然后分别学习p(y)和p(x|y)的概率分布，主要通过极大似然估计的方法学习参数，如NGram、HMM、Naive Bayes。 优缺点： 生成模型： 优点：1）实际上带的信息要比判别模型丰富，研究单类问题比判别模型灵活性强2）模型可以通过增量学习得到3）生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。 缺点：1）学习过程比较复杂。2）实践中多数情况下判别模型效果更好。 判别模型： 优点：1）分类边界更灵活，比使用纯概率方法或生产模型得到的更高级.2）准确率往往较生成模型高。3）不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。 缺点：1）不能反映训练数据本身的特性。","categories":[{"name":"Machine_Learning","slug":"Machine-Learning","permalink":"https://shyshy903.github.io/categories/Machine-Learning/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://shyshy903.github.io/tags/ML/"}]},{"title":"Java基础（5）","slug":"java/shyjava(5)","date":"2019-11-25T16:00:00.000Z","updated":"2019-11-25T16:00:00.000Z","comments":true,"path":"2019/11/26/java/shyjava(5)/","link":"","permalink":"https://shyshy903.github.io/2019/11/26/java/shyjava(5)/","excerpt":"","text":"方法方法的定义12345678910111213141516171819202122MethodDeclaration: MethodHeader MethodBodyMethodHeader: Modifiersopt ResultType Identifier(FormalParameterListopt) ThrowsoptModifiers: public protected private static abstract final synchronized native strictfpResultType: Type voidMethodBody: &#123; statements &#125;public static int max(int num1, int num2) &#123; int result = 0; if(num1 &gt; num2) result = num1; else result = num2; return result;&#125; 方法签名(Method Signature)指方法名称、参数类型、参数数量和返回类型。一个类中不能包含签名相同或仅返回类型不同的多个方法。 方法头中声明的变量称为形参(formal parameter)。当调用方法时，可向形参传递一个值，这个值称为实参(actual parameter / argument)。形参可以使用final进行修饰，表示方法内部不允许修改该参数。 形参不允许有默认值，最后一个可为变长参数（可用…或数组定义，参见第7章数组）。方法不允许static局部变量。 方法可以有一个返回值(return value)。如果方法没有返回值，返回值类型为void，但构造函数确实没有返回值。 方法的调用 声明方法只给出方法的定义。要执行方法，必须调用(call/invoke)方法。 如果方法有返回值，通常将方法调用作为一个值来处理（可放在一个表达式里）。1234int large = max(3, 4) * 2; System.out.println(max(3,4));如果方法没有返回值，方法调用必须是一条语句。System.out.println(“Welcome to Java!”); 当调用方法时，程序控制权转移至被调用的方法。当执行return语句或到达方法结尾时，程序控制权转移至调用者。 调用当前类中的静态方法：可直接用“方法名”，也可用”类名.方法名“；实例函数中也可用” 方法名“或”this.方法名“调用。 调用其它类中的静态方法：必须用”类名.方法名“或”对象.方法名“调用；子类实例函数也可用” super.方法名“调用父类方法。 所有静态方法提倡用”类名.方法名“调用。如Math.sin(3.0) 1234567891011121314public class TestMax &#123; public static void main(String[] args) &#123; int i = 5; int j = 2; int k = max(i, j); System.out.println(\"The maximum between \" + i + \" and \" + j + \" is \" + k); &#125; public static int max(int num1, int num2) &#123; int result; result = (num1 &gt; num2) ?num1:num2; return result ; &#125;&#125; 调用程序栈每当调用一个方法时，系统将参数、局部变量存储在一个内存区域中，这个内存区域称为调用堆栈(call stack)。当方法结束返回到调用者时，系统自动释放相应的调用栈。 方法的参数传递 如果方法声明中包含形参，调用方法时，必须提供实参。实参的类型必须与形参的类型兼容：如父类形参可用子类实参。实参顺序必须与形参的顺序一致。 1234567public static void nPrintln(String message, int n) &#123; for (int i = 0; i &lt; n; i++) System.out.println(message);&#125;nPrintln(“Hello”, 3); //正确nPrintln(3, “Hello”); //错误 当调用方法时，基本数据类型的实参值的副本被传递给方法的形参。方法内部对形参的修改不影响实参值。(Call by value)对象类型的参数是引用调用（Call by reference） 12345678910111213141516171819202122public class TestPassByValue &#123; public static void main(String[] args) &#123; int num1 = 1; int num2 = 2; System.out.println(\"调用swap方法之前：num1 = \" + num1 + \"，num2 = \" + num2); swap(num1, num2); System.out.println(\"调用swap方法之后：num1 = \" + num1 + \"，num2 = \" + num2); &#125; public static void swap(int n1, int n2) &#123; System.out.println(\"\\t在swap方法内：\"); System.out.println(\"\\t\\t交换之前：n1 = \" + n1 + \"，n2 = \" + n2); int temp = n1; n1 = n2; n2 = temp; System.out.println(\"\\t\\t交换之后：n1 = \" + n1 + \"，n2 = \" + n2); &#125;&#125; 方法的重载 方法重载(overloading)是指方法名称相同，但方法签名不同的方法，仅返回类型不同的方法不可重载。一个类中可以包含多个重载的方法。 当调用方法时，Java编译器会根据实参的个数和类型寻找最合适的方法进行调用。 调用时匹配成功的方法可能多于一个，则会产生编译二义性错误，称为歧义调用(ambiguous invocation）重载示例123456789101112131415161718192021222324252627282930313233343536373839public class TestMethodOverloading &#123; /** Return the max between two int values */ public static int max(int num1, int num2) &#123; public class TestMethodOverloading &#123; /** Return the max between two int values */ public static int max(int num1, int num2) &#123; return (num1 &gt; num2) ？num1:num2; &#125; /** Return the max between two double values */ public static double max(double num1, double num2) &#123; return (num1 &gt; num2) ？num1:num2; &#125; /** Return the max among three double values */ public static double max(double num1, double num2, double num3) &#123; return max(max(num1, num2), num3); &#125; &#125; &#125;&#125;public class TestMethodOverloading &#123; /** Main method */ public static void main(String[ ] args) &#123; // Invoke the max method with int parameters System.out.println(\"The maximum between 3 and 4 is \" + max(3, 4)); // Invoke the max method with the double parameters System.out.println(\"The maximum between 3.0 and 5.4 is \" + max(3.0, 5.4)); // Invoke the max method with three double parameters System.out.println(\"The maximum between 3.0, 5.4, and 10.14 is \" + max(3.0, 5.4, 10.14)); &#125; &#125; 有歧义的重载123456789101112131415161718public class AmbiguousOverloading &#123; public static void main(String[ ] args) &#123; //System.out.println(max(1, 2)); //该调用产生歧义 &#125; //以下任一函数的参数都相容（都能自动转换），编译无法确定用哪个函数 public static double max(int num1, double num2) &#123; if (num1 &gt; num2) return num1; else return num2; &#125; public static double max(double num1, int num2) &#123; if (num1 &gt; num2) return num1; else return num2; &#125;&#125; 局部变量的作用域方法内部声明的变量称为局部变量(local variable)。局部变量的作用域(scope)指程序中可以使用该变量的部分。局部变量的生命期和其作用域相同。局部变量的作用域从它的声明开始，直到包含该变量的程序块结束。局部变量在使用前必须先赋值。在方法中，可以在不同的非嵌套程序块中以相同的名称多次声明局部变量。但不能在嵌套的块中以相同的名称多次声明局部变量：无法访问外部块变量。在for语句的初始动作部分声明的变量，作用域是整个循环。在for语句循环体中声明的变量，作用域从变量声明开始到循环体结束12345678910111213141516171819public class TestLocalVariable &#123; public static void method1( ) &#123; int x = 1; int y = 1; for (int i = 1; i &lt; 10; i++) &#123; x += i; &#125; for (int i = 1; i &lt; 10; i++) &#123; y += i; //正确：两个循环未嵌套，都可用i &#125; &#125; //错误，变量i在嵌套的语句块中声明 public static void method2( ) &#123;// int i = 1;// int sum = 0;// for (int i = 1; i &lt; 10; i++) &#123;//java不允许函数的局部变量或参数的作用域被覆盖// sum += i; //无法访问外部块局部变量// &#125; &#125;&#125;全局变量的声明与使用123456public class args &#123; public static String username; // 全局变量 public static String password; //全局变量&#125;&gt;&gt; args.username&gt;&gt; args.password","categories":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/tags/java/"}]},{"title":"Python学习笔记","slug":"python/shypython","date":"2019-11-22T16:00:00.000Z","updated":"2020-02-11T08:48:55.606Z","comments":true,"path":"2019/11/23/python/shypython/","link":"","permalink":"https://shyshy903.github.io/2019/11/23/python/shypython/","excerpt":"","text":"Shypython-learn-notes1. python 数据类型1.1 变量1.1.1 算术运算符 123456- 加减乘除+、-、*、/- 取余、取整、取绝对值 %、//、abs()- 最小、最大值 min()、max()- 复数 complex(re,im)- 取共轭 c.conjugate()- 返回商和余数 divmod(x,y) 1.1.2 布尔运算符 123- 小于、大于 &lt; 、 &gt;- 等于、不等于 == 、 != - 与、或、非 and 、or 、not 1.1.3 赋值运算符 1234- a=a+b is a+=b- a=a-b is a-=b- a=a*/b is a*/b- a=a**(//)b is a**(//)=b 1.1.4 位运算符 123- 与或 &amp; 、 | - 异或、取反 ^ 、~ - 左位移、右位移 &lt;&lt; 、 &gt;&gt; 1.1.5 转义符 1234567891011121314- 续行符 \\- 反斜杠符号 \\\\- 引号 \\'- 响铃 \\a- 退格 \\b- 转义 \\e- 空 \\000- 换行 \\n- 纵向制表符 \\v- 横向制表符 \\t- 回车 \\r- 换页 \\f- 八进制 \\oyy- 十六进制 \\xyy 1.2 字符串(不可变类型)1.2.1 切片操作 12345# 当索引为正数时，从0开始，当索引为负数时，从-1开始（从右往左）- newstr = s[a:b:c] 从索引a开始到b ,每隔c取一个值，左开右闭- newstr = s[0:] - newstr = s[:] 和上面的式子等价- newstr = s[::-1] 实现字符串的逆序 1.2.2 字符串运算及方法 1234567891011121314151617str = 'I LOVE PYTHON!!'- 标准化字符串 r'str' 或者 repr(str)- x in s 子字符串 - s1 + s2 字符串连接 - s*n 字符串副本的拼接 - s[i] 字符串索引 - str.index('s') 获得字符串s字符的索引位置- len(s) 字符串长度 - ord(str) 字符串的编码 - chr(number) 返回某个编码得到的字符 - str.spilt(', ') 字符串的分割 ,返回值是一个列表- chr.join(list) 字符串编码的连接 , \" \".join(list)str = \"www.runoob.com\"- print(str.upper()) 把所有字符中的小写字母转换成大写字母- print(str.lower()) 把所有字符中的大写字母转换成小写字母- print(str.capitalize()) 把第一个字母转化为大写字母，其余小写- print(str.title()) 把每个单词的第一个字母转化为大写，其余小写 1.2.3深入研究字符串的方法 123456str.find(x) 返回x的第一个字符出现的索引位置str.count(x) 返回x出现的次数str.replace('top','bot') 返回一个修改的副本str.spilt()tabel = str.maketrans('xyz','uvw') ; str.translate(table) 返回一个映射后的副本str.strip() 返回字符串的一个副本，并且消除前后空格 1.3 列表（可变类型）1.3.1 list的内置方法 123456789101112lst = [1,3,a,[4,5],6]- list.append(x) 在尾部增加一个元素- list.insert(x,i) 在索引i处添加一个元素- list.index(x) 获得元素x的索引- list.remove(x) 删除列表的原色- list.pop(i) 弹出索引为i的元素并在列表中删除它- list.clear() 清楚列表- list.count(x) 返回列表x出现的次数- list.sort() 对列表直接排序， 区别于排序函数 sorted()- list.reverse() 对列表进行反转- len(list)- for item in list: 对列表的遍历 1.3.2 列表和字符串的相互转化 123456789101112131415161718192021222324252627282930 1. str &gt;&gt;&gt;list str1 = \"12345\"list1 = list(str1)print list1 str2 = \"123 sjhid dhi\"list2 = str2.split() #or list2 = str2.split(\" \")print list2 str3 = \"www.google.com\"list3 = str3.split(\".\")print list3 输出为：['1', '2', '3', '4', '5']['123', 'sjhid', 'dhi']['www', 'google', 'com']2. list &gt;&gt;&gt;strstr4 = \"\".join(list3)print str4str5 = \".\".join(list3)print str5str6 = \" \".join(list3)print str6输出为：wwwgooglecomwww.google.comwww google com 1.3 元组类型（不可变类型）1.3.1 元组的运算及操作 12# 元组为不可修改的字符串tuple = (2019,'a',(b,c),'science') 1.3.2 元组与列表的转换 12tuple = tuple(list)list = list(touple) 1.4 集合类型（消除关系重复元素）1234567891011121314myset = ['nature','science']set.add(x)set.remove(X)set.discard(X)set.clear()set.pop()len(set)in / not inset.issubset(set2) 判断set是否是set2的子集，返回bool类型set.isuperset(set2) set.union(set2) 计算并集set.intersection(set2) 计算交集set.difference(set2) 计算差集set.symmetric_difference(set2) 计算对称差集 1.5 字典类型（键值对）12345678910111213141516171819mydict = &#123;'a':1,'b':2,'c':3&#125;len(dict)str(dict)dict('a') 访问字典中键为a的值dict.clear()dict.items() 以列表形式返回可遍历的（键，值）元素数组dict.keys() 以列表形式返回一个字典中的所有键dict.values() 以字典形式返回一个字典中的所有值``` ## 2. 语句类型### 2.1 if 语句```python if &lt;条件&gt;： passelif &lt;条件&gt;： passelse: pass 2.2 while语句12345while &lt;条件&gt;： pass# 死循环while 1: pass 2.3 for 语句1234567891011121314151617for item in list: pass# 在for循环中使用内置函数 range()range(a,b,c) 返回一个数字区间的所有整数# 简单的冒泡排序算法Sfor i in range(len(n)-1): for j in range(len(n)-i-1): if n[j] &gt; n[i]: n[j], n[j+1] = n[j+1], n[j]# 在for循环中使用内置函数zip()zip(x,y) 将多序列生成一个新的序列，每个序列的元素以元组形式存储数据for t1,t2 in zip(x,y): print(t1,t2) 2.4 控制语句123break 跳出循环continue 终止当前一次循环、继续进行下一次循环pass 什么都不做，保持结构完整性 3. 格式化输入与输出3.1 格式化输入12a = input(&lt;info&gt;)a = input(repr(str)) 3.2 格式化输出3.2.1 print语句 12345678print(a, b) 同行以空格隔开输出print(a, b,s ep=',') 以逗号隔开进行输出print(s, ewp='\\n') 以换行隔开进行输出print(name, end='!') 每个输出都要添加!print(' i love %s ' % s) print 格式化输出# 对字符串的每个元素进行换行输出for item in list: print(item) 3.2.2 字符串方法format() 123print('&#123;0&#125; : &#123;1&#125; : &#123;2&#125;'.format(hour, minute, second))# 按照对齐格式化进行排列print('&#123;0:3&#125;,&#123;1:5&#125;'.format(12, 534)) :后面的内容为只等的格式，表示占位数，如果不够，在前面用空格补齐 3.2.3 数据输出的格式类型 123456789# a表示占位，不够，按照原长度打印，多了左侧空格补齐，小数点后面为保留几位小数，f位数据类型print('i love %a.bf', num)- b 以二进制形式输出- c 输出证书值对应的unicode字符- d 以十进制形式输出数值- e 以科学计数法形式输出- o 以八进制形式输出- x 以小写形式的十六进制输出- X 以大写形式的十六进制输出 4. 函数4.1 函数定义及调用函数12345678910111213141516- def funname(para1, para2,*，para3...): 函数体# 在参数列表中使用（*），代表调用函数时，在（*）后面的参数都必须指定参数名称，如下funname(para1, para2，para3=2...) 调用函数- def fun(strname, age=32)： 后面的参数位默认形参，默认形参必须放在后面- 对元组和列表进行解包def fun(*person):fun('shy','21')mylist = ['shy','21']fun(*mylist)- 对字典进行解包定义参数及调用def fun(**person)print('姓名',person['name'],'年纪',person['age'])fun('shy','21')mydict = &#123;'name':'shy','age':21&#125;fun(**mydict) 4.2 函数类型4.2.1 python内置函数 下图python3.8官方文档给出的内置函数库： 官方中文文档的连接：https://docs.python.org/zh-cn/3/library/functions.html 4.2.2 匿名函数与可迭代函数 匿名函数1234567891011121314151617181920# 匿名函数lambda para1, para2... : 表达式r = lambda x,y:x*y# 匿名函数与reduce函数的组合应用reduce(fun, seq, initial) #用序列值依次调用funfrom funtools import reducea = reduce(lambda x,y:x + y, range(1,101)) #实现求1~100的和# 匿名函数与map函数的组合应用map(fun, seq[,seq,]) #将seq内部的元素作为参数依次调用t = map(lambda x:x**2,[1,2,3,4,5]) #返回一个map对象print(list(t)) #打印值为[1,4,9,16,25]y =map(lambda x,y:x+y,[1,2,3],[4,5,6])print(list(t)) # 打印值为[5,7,9]# 匿名函数与filter函数的组合应用filter(fun or none, seq) #将序列对象依次放到fun中，如果返回true就留下t = filter(lambda x:x%2==0, [1,2,3,4,5,6])print(list(t)) # 打印值为[2,4,6] 可迭代函数1234# 每个生成器对象会有一个__next__()方法t = filter(lambda x:x%2==0, [1,2,3,4,5,6])print(t.__next__()) # 打印2print(t.__next__()) # 打印4 4.2.3 生成器函数与工厂函数 生成器函数123456789# 生成器与迭代器不同，迭代器的内容存在内存里，用next函数遍历，生成器用完立即销毁def Reverse(data): for idx in range(len(data)-1,-1,-1): yield data[idx] # 生成器函数用yield返回for c in Reverse('Python'): print(c, end = ' ') # 打印 n o h t y P# 生成器表达式mylist = [x*x for x in range(3)] # 使用生成器表达式返回一个对象 工厂函数1234567891011121314151617181920212223242526# 闭合函数def wrapperfun(strname): def recorder(age)： print(strname,age) return recorderfun = wrapperfun('shy')fun(37) # 打印 shy 37# 装饰器属性：在原有的函数包一个函数，不改变原代码的基础上，添加新功能def checkParams(fn): # 装饰器函数 def wrapper(strname): if isinstance(strname.(str))： return fn(strname) # 判断字符串类型 print ('error') return return wrapperdef wrapperfun(strname): def recorder(age)： print(strname,age) return recorderwrapperfun2 = checkParams(wrapperfun)fun = wrapperfun('shy')fun(37) # 打印 shy 37fun = wrapperfun2(37) # 输入不合法 @修饰符1234567891011121314151617def checkParams(fn): # 装饰器函数 def wrapper(strname): if isinstance(strname.(str))： return fn(strname) # 判断字符串类型 print ('error') return return wrapper@checkParams # 使用装饰器函数对wrapperfun函数进行修饰def wrapperfun(strname): def recorder(age)： print(strname,age) return recorderfun = wrapperfun('shy')fun(37) # 打印 shy 37fun = wrapperfun2(37) # 输入不合法 4.4.4 偏函数与递归函数 偏函数123# 偏函数是重新定义一个函数，并指定了默认参数值from funtools import partialpartial(fun, *args, **keywords) 递归函数123# 递归函数是自己调用自己的函数，所有的函数调用都是压栈的过程，所以耗内存，栈空间有限，如果程序栈空间地址写满，程序最后会崩溃def fun(n): return n*fun(n-1) 4.4.5 eval 和 exec函数 eval函数123456# exec执行不返回结果，eval执行返回结果dic = &#123;&#125;dic['b'] = 3exec('a=4',dic)print(dic.keys()) #打印dict_keys(['a','__builtins__','b'])# 使用这两个函数第一个参数一定是可执行代码 4.3 变量的作用域 4.3.1 global语句 123456# global 可以把局部声明为全局a = 6def func(): global a a = 5print(a) # 打印5 4.3.2 nonlocal语句 123456789101112# nonlocal可以把全局往下一作用域调用a = 6def func(): a = 7 def nested(): nonlocal a a+=1 nested() print('本地:',a) #打印 8func()print('全局',a) # 打印6print(a) # 打印5 5. 面向对象的程序设计5.1 类的结构12345678910111213141516class MyClass： def __init__(self,属性): # 构造函数 self.属性 = 属性 pass def getname(self): return self.name ......myc = MyClass(属性) # 初始化实例对象，构造函数的属性a = myc.getname() # 类还具有一些内置属性__name__ 名称__doc__ 类的文档字符串__nodule__ 类的模块if __name__ == '__main__': 5.2 类方法1234@classmthod # 声明为类方法，可以直接用类名进行调用，当然初始化实例对象进行调用也是正确的def fun(cls):@staticmethod # 等同于普通函数，只是被封装在类中，独立于整个类def fun() # 同上的调用方式，且参数没有限制要求 5.3 类的私有化属性123456789101112131415# 私有化属性方法，在类的属性前面加双下划线，同时会提供一个私有化属性的访问函数，不可以被修改,但可以针对具体实例进行对象修改clas MyClass: __Occupation = 'scientist' def __init__(sefl,name,age)； def getOccupation(self): return self.__Occupation# 使用装饰器函数实现类的私有化clas MyClass: __Occupation = 'scientist' def __init__(sefl,name,age)； @property # 装饰为属性，使类的私有化属性也可以被访问 def getOccupation(self): return self.__Occupation 5.4 类的继承5.4.1 继承结构体 12class DerivedClass(FatherClass1, FatherClass2)： pass 123456789101112131415161718192021class Record: \"\"\" A record class \"\"\" __Ocuupation = \"scientist\" def __init__(self, name, age): self.name = name self.age = age def showrecode(self): print(\"Occupation:\",self.getOccupation()) def getOccupation(self): return self.__Occupationclass GirlRecord(Record): \"\"\" A girlrecord class \"\"\" def showrecode(self): Record.showrecode(self) print(\"the girl:\", self.name, \"age:\", self.age)myc = GirlRecord(\"Anaa\", 21)myc.showrecode() 5.4.2 super()函数 123456789101112131415161718192021222324252627282930313233# 保障了调用父类方法时只调用一次class Record: \"\"\" A record class \"\"\" __Ocuupation = \"scientist\" def __init__(self, name, age): self.name = name self.age = age def showrecode(self): print(\"Occupation:\",self.getOccupation()) def getOccupation(self): return self.__Occupationclass GirlRecord(Record): \"\"\" A girlrecord class \"\"\" def showrecode(self): super().showrecode(self) print(\"the girl:\", self.name, \"age:\", self.age)class MaleRecord(Record): \"\"\" A girlrecord class \"\"\" def showrecode(self): super().showrecode(self) print(\"the girl:\", self.name, \"age:\", self.age) class ThisRecord(GirlRecord, MaleRecord): \"\"\" A girlrecord class \"\"\" def showrecode(self): super().showrecode(self) print(\"the girl:\", self.name, \"age:\", self.age)myc = ThisRecord(\"Anaa\", 21)myc.showrecode() 5.5 类相关的内置函数 判断实例（isinstance)1isinstance(object, class_name) 判断字类（issubclass)1issubclass(class1, class2) 判断类实例中是否包含某一个属性（hasattr)1hasattr(object, name) 获得类实例中的某一个属性（getattr)1getattr(object, name[,default]) 设置类实例中的某一个属性值（setattr)1setattr(object, name, value) 5.6 重载运算符123456789101112131415161718192021222324252627class MyClass: \"\"\" A record class\"\"\" def __init__(self, name, age): self.name = name self.age = age def __str__(self): # 将值转化为字符串进行输出 retrun \"name:\"+self.name;\"age:\"+str(self.age) __repr__ = __str__ # 转化为解释器读取的形式 def __lt__(self, record): #重载比较运算符 if self.age &lt; record.age: return True else: return False def __add__(self, record): #重载加号运算符 return MyClass(self.name, self.age+record.age)myc = MyClass(\"A\", 42)myc1 = MyClass(\"B\", 23)print(repr(myc)) print(myc)print(str(myc))print(myc&lt;myc1)print(myc+myc1) 运算符重载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 方法名 运算符和表达式 说明__add__(self,rhs) self + rhs 加法__sub__(self,rhs) self - rhs 减法__mul__(self,rhs) self * rhs 乘法__truediv__(self,rhs) self / rhs 除法__floordiv__(self,rhs) self //rhs 地板除__mod__(self,rhs) self % rhs 取模(求余)__pow__(self,rhs) self **rhs 幂运算合赋值算术运算符的重载:方法名 运算符和表达式 说明__iadd__(self,rhs) self += rhs 加法__isub__(self,rhs) self -= rhs 减法__imul__(self,rhs) self *= rhs 乘法__itruediv__(self,rhs) self /= rhs 除法__ifloordiv__(self,rhs) self //=rhs 地板除__imod__(self,rhs) self %= rhs 取模(求余)__ipow__(self,rhs) self **=rhs 幂运算比较算术运算符的重载:方法名 运算符和表达式 说明__lt__(self,rhs) self &lt; rhs 小于__le__(self,rhs) self &lt;= rhs 小于等于__gt__(self,rhs) self &gt; rhs 大于__ge__(self,rhs) self &gt;= rhs 大于等于__eq__(self,rhs) self == rhs 等于__ne__(self,rhs) self != rhs 不等于 位运算符重载方法名 运算符和表达式 说明__and__(self,rhs) self &amp; rhs 位与__or__(self,rhs) self | rhs 位或__xor__(self,rhs) self ^ rhs 位异或 __lshift__(self,rhs) self &lt;&lt;rhs 左移 __rshift__(self,rhs) self &gt;&gt;rhs 右移反向位运算符重载方法名 运算符和表达式 说明__and__(self,lhs) lhs &amp; rhs 位与__or__(self,lhs) lhs | rhs 位或__xor__(self,lhs) lhs ^ rhs 位异或__lshift__(self,lhs) lhs &lt;&lt;rhs 左移__rshift__(self,lhs) lhs &gt;&gt;rhs 右移复合赋值位相关运算符重载方法名 运算符和表达式 说明__iand__(self,rhs) self &amp; rhs 位与__ior__(self,rhs) self | rhs 位或__ixor__(self,rhs) self ^ rhs 位异或__ilshift__(self,rhs) self &lt;&lt;rhs 左移__irshift__(self,rhs) self &gt;&gt;rhs 右移一元运算符的重载方法名 运算符和表达式 说明__neg__(self) - self 负号__pos__(self) + self 正号__invert__(self) ~ self 取反 6 错误异常与文件读写6.1 错误异常捕捉 6.1.1 异常语句结构 123456789101112try: passexcept(ZeroDivisionError, ValueError): print('错误')except: print('其它异常’)except Exception as e: # 捕捉未知异常 print(e)else: pass # 没有异常发生时执行finally: pass # 最终处理语句，无论是否有异常，都要执行这个语句 6.1.2 异常类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 异常名称 异常解释AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性xIOError 输入/输出异常；基本上是无法打开文件ImportError 无法引入模块或包；基本上是路径问题或名称错误IndentationError 语法错误（的子类） ；代码没有正确对齐IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5]KeyError 试图访问字典里不存在的键KeyboardInterrupt Ctrl+C被按下NameError 使用一个还未被赋予对象的变量SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了）TypeError 传入对象类型与要求的不符合UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量，导致你以为正在访问它ValueError 传入一个调用者不期望的值，即使值的类型是正确BaseException 所有异常的基类SystemExit 解释器请求退出KeyboardInterrupt 用户中断执行(通常是输入^C)Exception 常规错误的基类StopIteration 迭代器没有更多的值GeneratorExit 生成器(generator)发生异常来通知退出StandardError 所有的内建标准异常的基类ArithmeticError 所有数值计算错误的基类FloatingPointError 浮点计算错误OverflowError 数值运算超出最大限制ZeroDivisionError 除(或取模)零 (所有数据类型)AssertionError 断言语句失败AttributeError 对象没有这个属性EOFError 没有内建输入,到达EOF 标记EnvironmentError 操作系统错误的基类IOError 输入/输出操作失败OSError 操作系统错误WindowsError 系统调用失败ImportError 导入模块/对象失败LookupError 无效数据查询的基类IndexError 序列中没有此索引(index)KeyError 映射中没有这个键MemoryError 内存溢出错误(对于Python 解释器不是致命的)NameError 未声明/初始化对象 (没有属性)UnboundLocalError 访问未初始化的本地变量ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象RuntimeError 一般的运行时错误NotImplementedError 尚未实现的方法SyntaxError Python 语法错误IndentationError 缩进错误TabError Tab 和空格混用SystemError 一般的解释器系统错误TypeError 对类型无效的操作ValueError 传入无效的参数UnicodeError Unicode 相关的错误UnicodeDecodeError Unicode 解码时的错误UnicodeEncodeError Unicode 编码时错误UnicodeTranslateError Unicode 转换时错误Warning 警告的基类DeprecationWarning 关于被弃用的特征的警告FutureWarning 关于构造将来语义会有改变的警告OverflowWarning 旧的关于自动提升为长整型(long)的警告PendingDeprecationWarning 关于特性将会被废弃的警告RuntimeWarning 可疑的运行时行为(runtime behavior)的警告SyntaxWarning 可疑的语法的警告UserWarning 用户代码生成的警告 6.2 文件读写与导入6.2.1 语句结构 123456789101112131415161718192021222324f = open('new/test.txt','a+',encoding='utf-8')f.write('\\n今天天气很好')f.close()f = open('new/test.txt','rb')w=f.read().decode('utf-8')print(w)f = open('workfile', 'w')with open('workfile') as f: read_data = f.read()f.close()文件常见的读写模式w 以写方式打开，W 文件若存在，首先要清空，然后（重新）创建a 以追加模式打开 (从 EOF 开始, 必要时创建新文件)r+ 以读写模式打开w+ 以读写模式打开 (参见 w )a+ 以读写模式打开 (参见 a )rb 以二进制读模式打开wb 以二进制写模式打开 (参见 w )ab 以二进制追加模式打开 (参见 a )rb+ 以二进制读写模式打开 (参见 r+ )wb+ 以二进制读写模式打开 (参见 w+ )ab+ 以二进制读写模式打开 (参见 a+ ) 6.2.1 文件读写方法 123456789101112131415161718192021222324252627f.read()f.read(size)f.readline() # 读取一行f.readlines() # 读取所有行，每一行存储为列表的每一个元素for line in f: print(line, end='')&gt;&gt;&gt; f.write('This is a test\\n')15f.tell() # 返回一个整数，给出文件对象在文件中的当前位置，表示为二进制模式下时从文件开始的字节数，以及文本模式下的不透明数字。# 要改变文件对象的位置，请使用 f.seek(offset, whence)。 通过向一个参考点添加 offset 来计算位置； # 参考点由 whence 参数指定。 # whence 的 0 值表示从文件开头起算，1 表示使用当前文件位置，2 表示使用文件末尾作为参考点。 # whence 如果省略则默认值为 0，即使用文件开头作为参考点。&gt;&gt;&gt;&gt;&gt;&gt; f = open('workfile', 'rb+')&gt;&gt;&gt; f.write(b'0123456789abcdef')16&gt;&gt;&gt; f.seek(5) # Go to the 6th byte in the file5&gt;&gt;&gt; f.read(1)b'5'&gt;&gt;&gt; f.seek(-3, 2) # Go to the 3rd byte before the end13&gt;&gt;&gt; f.read(1)b'd' 6.3 常见文件类型的读取方式 csv文件1234567with open('C:/users/lenovo/desktop/student_score.csv','r') as f: for line in f.readlines(): #逐行读取 print(line)# 更加高效的是使用panda读取数据，存为一个矩阵的形式import panda as pddata = pd.read_csv(sys.argv[1]) txt文件123456789with open('my_file.txt') as f: for line in f: #逐行读取 print(line.strip()) #使用strip删除空格和空行，否则会有\\n在最后# 更加高效的是使用numpy读取数据,转化一个数组import numpy as npdataset = np.loadtxt('路径')dataset.shape( ) # 查看数组维度newset = reshape(dataset, (100,3)) # 转化为100行 3列的数组 excle文件123456789101112131415import numpy as npimport xlrd #使用库函数workbook = xlrd.open_workbook('C:/users/lenovo/desktop/student_score.xlsx') #读取路径sheet = workbook.sheet_by_name('Sheet1') #读取excel中的第一个sheetdata_name = sheet.col_values(0) #按列读取，读取第一列#data_name1 = sheet.row_values(0) #按行读取，读取第一行data_st_ID = sheet.col_values(1)data_st_score = sheet.col_values(2)# 更加高效的是使用panda读取数据import pandas as pdtest_df = pd.read_excel(r'G:\\test.xlsx') mat文件123456789101112131415import numpy as np import os os.chdir(r'F:/data')import scipy.io as sciodata = scio.loadmat('97.mat')print(data)de = data['X097_DE_time']读取的结果是一个字典or from mat4py import loadmatimport numpy as npdata = np.array(loadmat('test.mat')['dictKey']).astype('float') 6.3 使用json保存结构化数据123&gt;&gt;&gt; import json&gt;&gt;&gt; json.dumps([1, 'simple', 'list'])'[1, \"simple\", \"list\"]' 6.3.1 将数据保存为json文件 1234model=&#123;&#125; #数据with open(\"./hmm.json\",'w',encoding='utf-8') as json_file: json.dump(model,json_file,ensure_ascii=False) 6.3.2 从json文件读取数据 1234567891011121314151617model=&#123;&#125; #存放读取的数据with open(\"./hmm.json\",'r',encoding='utf-8') as json_file: model=json.load(json_file)读取返回的为python字典or import json str_file = './960x540/config.json'with open(str_file, 'r') as f: print(\"Load str file from &#123;&#125;\".format(str_file)) r = json.load(f)print(type(r))print(r)print(r['under_game_score_y']) 7. python标准库 也可以查看标准库文档https://docs.python.org/zh-cn/3/tutorial/index.htmlhttps://docs.python.org/zh-cn/3/library/index.html","categories":[{"name":"python","slug":"python","permalink":"https://shyshy903.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://shyshy903.github.io/tags/python/"}]},{"title":"Java基础（4）","slug":"java/shyjava(4)","date":"2019-11-22T16:00:00.000Z","updated":"2019-11-22T16:00:00.000Z","comments":true,"path":"2019/11/23/java/shyjava(4)/","link":"","permalink":"https://shyshy903.github.io/2019/11/23/java/shyjava(4)/","excerpt":"","text":"循环while循环12345int i = 0;while (i &lt; 100) &#123; System.out.println(“Welcome to Java!”); i++; //必须有语句改变循环条件&#125; do while循环123456\"\"\"循环体至少执行一次\"\"\"do statement or block while (loop-continuation-condition); for 循环12345678910111213for (int i = 0; i &lt; 100; i++) &#123; System.out.println(“Welcome to Java!”);&#125;for循环头中的每个部分可以是零个或多个以逗句分隔的表达式。for (int i = 0, j = 0; i + j &lt; 10; i++, j++) &#123; System.out.println(“Welcome to Java!”);&#125;如果for循环中的loop-continuation-condition被省略，则隐含为真。for (;;) &#123; while(true) &#123; //do something 等价于 //do something&#125; &#125; break 与 continue12break //跳出循环continue //跳出当前循环，继续循环 JDK1.5 增强的for循环12345678910111213JDK 1.5引入新的for循环，可以不用下标就可以依次访问数组元素。语法：for(elementType value : arrayRefVar) &#123;&#125;例如for(int i = 0; i &lt; myList.length; i++) &#123; sum += myList[i];&#125;for(double value : myList) &#123; sum += value;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/tags/java/"}]},{"title":"Java基础（3）","slug":"java/shyjava(3)","date":"2019-11-21T16:00:00.000Z","updated":"2019-11-21T16:00:00.000Z","comments":true,"path":"2019/11/22/java/shyjava(3)/","link":"","permalink":"https://shyshy903.github.io/2019/11/22/java/shyjava(3)/","excerpt":"","text":"Shy-Learnjava（3）基础3 数学函数、字符与字符串3.1 数学函数Math是final类：在java.lang.Math中，所有数学函数都是静态方法123456789101112# Math类中定义了常用的数学常量，如PI : 3.14159265358979323846E : 2.7182818284590452354# 方法:注意都是静态函数# 三角函数sin, cos, tan, asin, acos, atan,toRadians,toDigrees# 指数exp, log, log10，pow, sqrt# 取整ceil, floor, round# 其它min, max, abs, random（[0.0,1.0)) Math.random方法生成[0.0,1.0)之间的double类型的随机数123456如：（int）(Math.random( )*10); //[0,10)50+(int)(Math.random( )*50); //[50,100)一般地a+(int)(Math.random( )*b) //返回[a, a+b)a+(int)(Math.random( )*（b+1）) //返回[a, a+b]编写生成随机字符的方法123456789101112131415161718Java中每个字符对应一个Unicode编码从0000到FFFF。 在生成一个随机字符，就是产生一个从0到65535之间的随机数。 所以, 计算表达式为：(int)(Math.random( ) * (65535 + 1)) 。英文大、小写字母的Unicode是一串连续的整数，如‘a’的统一码是: (int)‘a’=97由于char类型可自动地被转换为int类型，所以我们可以对应使用如下整数值：‘a’=97, ‘b’=98， …, ‘z’=122因此，随机生成从‘a’-‘z’之间的字符就等于生成‘a’-‘z’之间的随机数，可用‘a’+（int）(Math.Random( ) * (‘z’-’a’+1))将上面讨论一般化，按如下表达式，可以生成任意2个字符ch1和ch2（ch1&lt;ch2）之间的随机字符(char)(ch1+(int)(Math.rabdom()*(ch2-ch1+1))) 3.2 字符数据类型Unicode和ASCII码123456789Java对字符采用16位Unicode编码，因此char类型的大小为二个字节16位的Unicode用以\\u开头的4位16进制数表示，范围从’\\u0000’到’\\uffff’,不能少写位数Unicode包括ASCII码，从’\\u0000’到’\\u007f’对应128个ASCII字符JAVA中的ASCII字符也可以用Unicode表示，例如char letter = ‘A’；char letter = ‘\\u0041’；//等价，\\u后面必须写满4位16进制数++和--运算符也可以用在char类型数据上，运算结果为该字符之后或之前的字符，例如下面的语句显示字符bchar ch = ‘a’;System.out.println(++ch); //显示b特殊字符转义12345和C++一样，采用反斜杠(\\)后面加上一个字符或者一些数字位组成转义序列，一个转义序列被当做一个字符如\\n \\t \\b \\r \\f \\\\ \\' \\\"如果想打印带””的信息 He said “Java is fun “System.out.println(“He said \\”Java is fun \\””);字符型和数据型的转换1234567891011121314151617181920char类型数据可以转换成任意一种数值类型，反之亦然。将整数转换成char类型数据时，只用到该数据的低16位，其余被忽略。例如char ch = （char）0xAB0041； System.out.println(ch); //显示A要将浮点数转成char时，先把浮点数转成int型，然后将整数转换成char char ch = （char）65.25； System.out.println(ch); //显示A当一个char型转换成数值型时，这个字符的Unicode码就被转换成某种特定数据类型 int i = （int）‘A’； System.out.println(i); //显示65如果转换结果适用于目标变量（不会有精度损失），可以采用隐式转换；否则必须强制类型转换 int i = ‘A’； byte b = （byte）‘\\uFFF4’; //取低8位二进制数所有数值运算符都可以用在char型操作数上， 如果另一个操作数是数值，那么char型操作数就自动转换为数值； 如果另外一个操作数是字符串，那么char型操作数会自动转换成字符串再和另外一个操作数字符串相连 int i = ‘2’+ ‘3’; System.out.println（i）； // i为50+51=101 int j = 2 + ‘a’； //j = 99 System.out.println(j + “ is the Unicode of ”+ (char)j);//99 is the Unicode of c字符的比较测试12345678910111213两个字符可以通过关系运算符进行比较，如同比较二个数值：通过字符的Unicode值进行比较Java为每个基本类型实现了对应的包装类，char类型的包装类是Character类。注意包装类对象为引用类型，不是值类型Character类的作用将char类型的数据封装成对象包含处理字符的方法和常量方法isDigit方法判断一个字符是否是数字isLetter方法判断一个字符是否是字母isLetterOrDigit方法判断一个字符是否是字母或数字isLowerCase方法判断一个字符是否是小写isUpperCase方法判断一个字符是否是大写toLowerCase方法将一个字符转换成小写toUpperCase方法将一个字符转换成大写 3.3 String 类，一个final类123456789101112java.lang.String表示一个固定长度的字符序列，实例化后字符不能改。构造函数长度(length)获取字符(charAt)连接(concat)截取(substring)比较(equals, equalsIgnoreCase, compareTo, startWith),endWith, regionMatch)转换(toLowerCase, toUpperCase, trim, replace)查找(indexOf, lastIndexOf)字符串和数组间转换(getChars, toCharArray), getChars返回void,超出长度就异常字符串和数字间转换(valueOf) String类对象的构造12345678910111213141516从字面值创建字符串String newString = new String(stringLiteral);例如：String message = new String(\"Welcome to Java\");由于字符串经常使用，java提供了创建字符串的简写形式。String newString = stringLiteral;例如：String m1 = “Welcome”; //m1和m2中的字符都是不可修改的String m2 = “Welcome”; //故m1和m2可以优化引用同一常量：m1==m2String m3 = \"Wel\" +\"come\";//m1==m2==m3 String m4 = \"Wel\" +new String(\"come\"); //m1!=m4字符串对象创建之后，其内容是不可修改的。String s = “java”;s = “HTML”;String t =s; 字符串的比较1234567891011121314151617181920212223242526272829303132333435363738394041424344equals方法用于比较两个字符串是否包含相同的内容（字符序列）:两个字符串内容相同，返回true两个字符串内容不同，返回false比较字符串内容不能直接比较二个引用变量，比较二个引用变量只是判断这二个引用变量是否指向同一个对象equalsIngnoeCase忽略大小写比较内容是否相同regionMatch比较部分内容是否相同startsWith判断是否以某个字符串开始endsWith判断是否以某个字符串结束compareTo方法用于比较两个字符串的大小，即第一个不同字符的差值。s1.compareTo(s2)的返回值:当两个字符串相同时，返回０当s1按字典排序在s2之前，返回小于０的值当s1按字典排序在s2之后，返回大于０的值String s0 = \"Java\";String s1 = \"Welcome to \" + s0;String s2 = \"Welcome to Java\";String s3 = \"welcome to java\";String s6 = \"Welcome to Java\";// equals用于比较两个字符串的内容是否相同System.out.println(\"s1.equals(s2) is \" + s1.equals(s2)); //true// equalsIgnoreCase忽略大小写System.out.println(\"s2.equals(s3) is \" + s2.equals(s3)); //falseSystem.out.println(\"s2.equalsIgnoreCase(s3) is \" + s2.equalsIgnoreCase(s3)); //true// regionMatches比较部分字符串: 给定两个串的起始位置和长度System.out.println(\"s2.regionMatches(11, s0, 0, 4) is \" + s2.regionMatches(11, s0, 0, 4)); //trueSystem.out.println(\"s3.regionMatches(11, s0, 0, 4) is \" + s3.regionMatches(11, s0, 0, 4));//falseSystem.out.println(\"s3.regionMatches(true, 11, s0, 0, 4) is \" + s3.regionMatches(true, 11, s0, 0, 4));//true,忽略大小写String s0 = \"Java\";String s1 = \"Welcome to \" + s0;String s2 = \"Welcome to Java\";String s3 = \"welcome to java\";String s6 = \"Welcome to Java\";// startsWith判断是否以某个字符串开始// endsWith判断是否以某个字符串结束System.out.println(\"s2.startsWith(s0) is \" + s2.startsWith(s0));//falseSystem.out.println(\"s2.endsWith(s0) is \" + s2.endsWith(s0)); //true // compareTo根据字典排序比较两个字符串String s4 = \"abc\";String s5 = \"abe\";System.out.println(\"s4.compareTo(s5) is \" + s4.compareTo(s5));//-2字符串方法12345678910111213141516171819202122232425262728293031323334353637383940414243调用length( )方法可以获取字符串的长度。例如：message.length( )返回15charAt(index)方法可以获取指定位置的字符。index必须在0到s.length()-1之间。例如：message.charAt(0)返回字符’W’concat方法用于连接两个字符串。例如：String s3 = s1.concat(s2);使用加号(+)连接两个字符串。例如：String s3 = s1 + s2;s1 + s2 + s3 等价于s1.concat(s2).concat(s3)连接操作返回一个新的字符串：因为String类型的实例不可修改。substring用于截取字符串的一部分，返回新字符串。public String substring(int beginIndex, int endIndex)返回字符串的子串。子串从beginIndex开始，直到endIndex-1public String substring(int beginIndex)返回字符串的子串。子串从beginIndex开始，直到字符串的结尾。toLowerCase将字符串转换成小写形式，得到新串toUpperCase将字符串转换成大写形式，得到新串trim删除两端的空格，得到新串replace字符替换，得到新串String s0 = \"Java\";String s1 = \" Welcome to Java \";// toLowerCase将字符串转换成小写形式System.out.println(\"s1.toLowerCase() is \" + s1.toLowerCase()); // toUpperCase将字符串转换成大写形式System.out.println(\"s1.toUpperCase() is \" + s1.toUpperCase()); // trim删除两端的空格System.out.println(\"s1.trim() is \" + s1.trim( )); // replace字符替换System.out.println(“s1.replace(s0, \\”HTML\\“) is ” + s1.replace(s0, “HTML”)); //Welcome to HTML// indexOf返回字符串中字符或字符串匹配的位置，返回-1表示未找到。\"Welcome to Java\".indexOf('W') returns 0\"Welcome to Java\".indexOf('x') returns -1\"Welcome to Java\".indexOf('o‘,5) returns 9\"Welcome to Java\".indexOf(\"come\") returns 3\"Welcome to Java\".indexOf(\"Java\", 5) returns 11\"Welcome to Java\".indexOf(\"java\", 5) returns -1\"Welcome to Java\".lastIndexOf('a') returns 1412345678910111213141516171819202122232425262728293031323334353637383940414243444546toCharArray将字符串转换成字符数组 String s = “Java”; char[ ] charArray = s.toCharArray( );// charArray.length=4将字符数组转换成字符串使用String的构造函数，可同时初始化 new String(new char[ ] &#123;‘J’,‘a’,‘v’,‘a’&#125; );使用valueOf方法 String.valueOf(new char[ ] &#123;‘J’,‘a’,‘v’,‘a’&#125;); String.valueOf(2.34);valueOf方法将基本数据类型转换为字符串。例如 String s1 = String.valueOf(1.0); //“１.0” String s2 = String.valueOf(true); //“true”字符串转换为基本类型 Double.parseDouble(str) Integer.parseInt(str) Boolean.parseBoolean(str)回文是指顺读和倒读都一样的词语。例如“mom”, “dad”, ”noon”都是回文。编写程序，判断一个字符串是否是回文。public class CheckPalindrome &#123; public static boolean isPalindrome(String s) &#123; // The index of the first character in the string int low = 0; // The index of the last character in the string int high = s.length( ) - 1; while (low &lt; high) &#123; if (s.charAt(low) != s.charAt(high)) return false; // Not a palindrome low++; high--; &#125; return true; // The string is a palindrome &#125;&#125;public class CheckPalindrome &#123; /** Main method */ public static void main(String[] args) &#123; // Prompt the user to enter a string String s = JOptionPane.showInputDialog(\"Enter a string:\"); String output = \"\"; if (isPalindrome(s)) output = s + \" is a palindrome\"; else output = s + \" is not a palindrome\"; // Display the result JOptionPane.showMessageDialog(null, output); &#125;&#125;StringBuilder与StringBuffer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768String类一旦初始化完成，字符串就是不可修改的。StringBuilder与StringBuffer(final类）初始化后还可以修改字符串。StringBuffer修改缓冲区的方法是同步的，更适合多任务环境。StringBuilder在单任务模式下与StringBuffer工作机制类似。由于可修改字符串， StringBuilder 与StringBuffer 增加了String类没有的一些函数，例如：append、insert、delete、replace、reverse、setCharAt等。仅以StringBuilder为例：StringBuilder stringMy=new StringBuilder( );StringMy.append(“Welcome to”); StringMy.append(“ Java”);StringBuffer用于处理可变内容的字符串。append方法在字符串的结尾追加数据insert方法在指定位置上插入数据reverse方法翻转字符串replace方法替换字符toString方法返回String对象capacity方法返回缓冲区的容量length方法返回缓冲区中字符的个数setLength方法设置缓冲区的长度charAt方法返回指定位置的字符setCharAt方法设置指定位置的字符所有对StringBuffer对象内容进行修改的方法，都返回指向相同StringBuffer对象的引用StringBuffer bf = new StringBuffer();StringBuffer bf1 = bf.append(\"Welcome \"); StringBuffer bf2 = bf.append(\"to \");StringBuffer bf3 = bf.append(\"Java\");assert(bf==bf1 &amp;&amp; bf==bf2 &amp;&amp; bf == bf3);因此以上语句可以直接写成：bf.append(\"Welcome \").append(\"to \").append(\"Java\");// 追加StringBuffer bf = new StringBuffer();bf.append(“Welcome”);bf.append(‘ ‘);bf.append(“to ”);bf.append(“Java”);System.out.println(bf.toString()); //Welcome to Java//插入bf.insert(11,”HTML and ”) //Welcome to HTML and JAVA//删除bf.delete(8,11); //Welcome Javabf.deleteCharAt(8);//Welcome o Javabf.reverse(); //avaJ ot emocleWbf.replace（11，15，“HTML”）;//Welcome to HTMLbf.setCharAt(0,’w’);//welcome to javatoString(): 从缓冲区返回字符串capacity()：返回缓冲区容量。length &lt;= capacity 当字符串长度超过缓冲区容量，capacity会自动增加length()：返回缓冲区中字符数量setLength(newLength)：设置缓冲区长度charAt(index)：返回下标为index的字符// 编写程序，检查回文，并忽略不是字母和数字的字符。解决方案创建一个新的StringBuffer，将字符串的字母和数字添加到StringBuffer中，返回过滤后的String对象。翻转过滤后的字符串，并与过滤后的字符串进行比较，如果内容相同则是回文。public static boolean isPalindrome(String s) &#123; // Create a new string that is the reversal of s String s2 = reverse(s); // Compare if the reversal is the same as the original string return s2.equals(s); &#125;public static String reverse(String s) &#123; StringBuffer strBuf = new StringBuffer(s); strBuf.reverse(); return strBuf.toString(); &#125; 3.4 格式化控制台输入输出1234567891011121314151617181920212223JDK1.5提供了格式化控制台输出方法System.out.printf(format, item1, item2, …);格式化字符串String.format(format, item1, item2, …);格式描述符%b 布尔值%c 字符%d 十进制整数%f 浮点数%e 科学计数法%s 字符串String.format(“格式$：%1$d,%2$s”, 99,“abc”); //结果”格式$：99，abc“public class TestPrintf &#123; public static void main(String[] args) &#123; System.out.printf(\"boolean : %6b\\n\", false); System.out.printf(\"boolean : %6b\\n\", true); System.out.printf(\"character : %4c\\n\", 'a'); System.out.printf(\"integer : %6d, %6d\\n\", 100, 200); System.out.printf(\"double : %7.2f\\n\", 12.345); System.out.printf(\"String : %7s\\n\", \"hello\"); &#125;&#125;","categories":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/tags/java/"}]},{"title":"Java基础（2）","slug":"java/shyjava(2)","date":"2019-11-20T16:00:00.000Z","updated":"2019-11-20T16:00:00.000Z","comments":true,"path":"2019/11/21/java/shyjava(2)/","link":"","permalink":"https://shyshy903.github.io/2019/11/21/java/shyjava(2)/","excerpt":"","text":"Shy-Learnjava（2）基础2 选择2.1 布尔类型和逻辑运算符12345678boolean类型的值有真(true)或假(false)。关系运算符: &lt;, &lt;=, &gt;, &gt;=, ==, !=关系运算符的计算结果是boolean类型boolean类型不能与其它数据类型混合运算布尔运算符: !, &amp;&amp;, ||, ^, &amp;, | &amp;&amp; , ||为条件逻辑运算符: (x&gt;0) &amp;&amp; (x&lt;9)&amp;，|为无条件逻辑运算符，同时也是位操作符^ 异或 2.2 if语句12345678910111213141516171819202122232425262728293031323334353637if (radius &gt;= 0) &#123; area = radius * radius * PI; System.out.println(\"The area for the circle of radius \" + radius + \" is \" + area);&#125;if (radius &gt;= 0) &#123; area = radius * radius * 3.14159; System.out.println(\"The area for the circle of radius \" + radius + \" is \" + area);&#125; else &#123; System.out.println(\"Negative input\"); &#125;if (score &gt; 90.0) grade = ‘A’;else if (score &gt;= 80.0) grade = ‘B’;else if (scroe &gt;= 70.0) grade = ‘C’;else if (score &gt;= 60.0) grade = ‘D’;else grade = ‘F’// 高手的ifif (number % 2 == 0) even = true;else even = false;//新手等价于even = (number % 2 == 0);//高手if (even == true) System.out.println(“It is even.”);等价于if (even) System.out.println(“It is even.”); 2.3 条件语句 swith语句 1234567891011switch(expression) &#123; case value1 : statement(s) break; case value2 : statement(s) break; … default : statement(s)&#125; 条件表达式 12max = (num1 &gt; num2) ? num1 : num2; 2.4 操作符的优先级和表达式规则123456789101112131415161718括号优先级最高，如果括号有嵌套，内部括号优先执行。如果没有括号，则根据操作符的优先级和结合规则确定执行顺序。如果相邻的操作符有相同的优先级，则根据结合规则确定执行顺序。除赋值运算符之外的二元运算符都是左结合的。赋值运算符和?:运算符是右结合的。例如：a+b-c+d 等价于 ((a+b)-c)+da=b+=c=5 等价于 a=(b+=(c=5))操作符的优先级和结合规则只规定了操作符的执行顺序。操作数从左至右进行运算。二元操作符左边的操作数比右边的操作数优先运算。例如：int a = 0;int x = a + (++a);x的结果为1int a = 0;int x = (++a) + a;x的结果为2 表达式规则123456789101112规则可能的情况下，从左向右计算所有子表达式根据运算符的优先级进行运算优先级相同的运算符，根据结合方向进行运算3 + 4 * 4 &gt; 5 * (4 + 3) - 1 的执行顺序为： 3 + 4 * 4 &gt; 5 * (4 + 3) - 1 3 + 4 * 4 &gt; 5 * 7 – 1 3 + 16 &gt; 5 * 7 – 1 3 + 16 &gt; 35 – 1 19 &gt; 35 – 1 19 &gt; 34 false","categories":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/tags/java/"}]},{"title":"Java基础（1）","slug":"java/shyjava(1)","date":"2019-11-19T16:00:00.000Z","updated":"2019-11-19T16:00:00.000Z","comments":true,"path":"2019/11/20/java/shyjava(1)/","link":"","permalink":"https://shyshy903.github.io/2019/11/20/java/shyjava(1)/","excerpt":"","text":"从今天开始复习java Shy-Learnjava（1）基础0 编程风格 注释 类和方法前使用文档注释 方法步骤前使用行注释。 命名 变量和方法名使用小写，如果有多个单词，第一个单词首字母小写，其它单词首字母大写。 类名的每个单词的首字母大写。 常量使用大写，单词间以下划线分隔。 缩进、空格、块样式（在eclipse中使用ctrl+shift+f） 1 基本程序设计 编写一个程序12345678public class ComputeArea&#123; public static void main(String[] args)&#123; double raidus; double area; area = radius * raidus * 3.14 System.out.println(\"The area of the circle of raius\" + radius + \"is\" + area); &#125;&#125; 1.1 标准输入与输出 标准输入12System.out //标准输出流类OutputStrem的对象System.in //标准输入流类InputStrem的对象 Scanner类1234567891011import java.util.ScannerScanner input = new Scanner(System.in)double d = input.nextDouble();// 方法有nextByte()nextShort()nextInt()nextLong()nextFloat()nextDouble()next() // 读入一个字符串 1.2 标识符、常量与变量 标识符命名规则 标识符是由字母、数字、下划线(_)、美元符号($)组成的字符序列。 标识符必须以字母、下划线(_)、美元符号($)开头。不能以数字开头。标识符不能是保留字。 标识符不能为true、false或null等事实上的保留字（参见英文维基网） 标识符可以为任意长度，但编译通常只接受前128字符 例如：$2, area, radius, showMessageDialog是合法的标识符；2A, d+4是非法的标识符 java保留字1234567891011abstract default if package this assert do goto private throw boolean double implements protected throws break else import public transient(非序列化)byte enum instanceof return truecase extends int short trycatch false interface static voidchar final long strictfp(严格浮点) volatile class finally native(本地方法) super whileconst float new switch continue for null synchronized java常量123456final datatype CONSTANT_NAME = value;//注意常量的声明和初始化必须同时完成final double PI = 3.14159;// 避免重复输入// 便于程序修改// 便于程序阅读 1.3 赋值语句与基本表达式 赋值语句 123456789101112131415// 赋值语句右读i = j = k = 1;//不要认为i, j, k的值不变，volatile类型的变量值可变k = 1;j = k; i = j;//语法datatype variable = expression;//例如：int x = 1; //某些变量在申明时必须同时初始化：final int m=0;int x = 1, y = 2;//局部变量在使用前必须赋值。int x, y; //若是成员变量，x, y有默认值=0y = x + 1; //局部变量无默认值则错error 1.4 java 数据类型 数值数据类型 123456789101112131415161718192021222324252627282930313233整数byte 8位带符号整数(-128 到 127)short 16位带符号整数(-32768 到 32767)int 32位带符号整数(-2147483648 到 2147483647)long 64位带符号整数(-9223372036854775808 到9223372036854775807)浮点数float 32位浮点数(负数 -3.4×1038到-1.4×10-45 正数 1.4×10-45到3.4×1038 )double 64位浮点数(负数 -1.8×10308到-4.9×10-324 正数 4.9×10-324到1.8×10308)加(+)、减(-)、乘(*)、除(/)、求余(%)：注意+，-的优先级较低int a = 34 + 1; // 35double b = 34.0 – 0.1; // 33.9long c = 300 * 30; // 9000double d = 1.0 / 2.0; // 0.5: 此处为浮点除int e = 1 / 2; // 0: 此处为整除byte f = 20 % 3; // 2: 取余数整数相除的结果还是整数，省略小数部分。int i = 5 / 2 // 2int j = -5 / 2 // -2字面值是直接出现在程序中的常量值。int i = 34;long k = 100000L; 整数字面值以0开头表示八进制，如035；以0x或0X开头表示十六进制，如0x1D,0X1d；以1-9开头表示十进制，如29后缀字母：以l或L结尾表示long类型，如29L；其它表示int类型。浮点数字面值浮点数是包含小数点的十进制数，后跟可选的指数部分。如 18. 1.8e1 .18E2后缀字母：以d或D结尾或者无后缀表示double类型；以f或F结尾表示float类型 操作运算符 12345678910111213常用简洁操作符, 结果均为右值。 操作符 举例 等价于 += i += 8 i = i + 8 -= f -= 8.0 f = f - 8.0 *= i *= 8 i = i * 8 /= i /= 8 i = i / 8 %= i %= 8 i = i % 8递增和递减运算符：++, --。结果均为右值。前缀表示先加(减)1后使用后缀表示先使用后加(减) 1 int i =10; //i=++i + ++i; 结果为23 int newNum= 10 * i++; //newNum = 100, i = 11 int newNum= 10 * ++i; //newNum = 110, i = 11 数值类型转换 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647如果二元操作符的两个操作数的数据类型不同，那么根据下面的规则对操作数进行转换：如果有一个操作数是double类型，另一个操作数转换为double类型。否则，如果有一个操作数是float类型，另一个操作数转换为float类型。否则，如果有一个操作数是long类型，另一个操作数转换为long类型。否则，两个操作数都转换为int类型。数据转换总是向较大范围的数据类型转换，避免精度损失long k = i * 3 + 4; //i变成int参与右边表达式计算，计算结果转longdouble d = i * 3.1 + k / 2; //i转double， k/2转double将值赋值给较大取值范围的变量时，自动进行类型转换。byte → char→ short → int → long → float → double 将值赋值给较小取值范围的变量时，必须使用强制类型转换(type casting)。语法： (datatype)variableName 例如： float f = (float)10.1; // 10.1是double类型 int i = (int)f; // 10 int j = (int)-f; // -10``- 字符数据类型```javachar表示16位的单个Unicode字符。char类型的字面值以两个单引号界定的单个Unicode字符。如:'男','女'可以用\\uxxxx形式表示， xxxx为十六进制。如:'\\u7537', '\\u5973'转义字符表示：\\n \\t \\b \\r \\f \\\\ \\' \\\"例如：char letter = 'A';char numChar = '4';如果想打印带””的信息 He said “Java is fun “ System.out.println(“He said \\”Java is fun \\””); String表示一个字符序列，注意字符串是String类实现的，是引用类型字符串的字面值是由双引号界定的零个或多个字符。 \"Welcom to java!\" \"\"连接运算：+, +=加号用于连接两个字符串。如果其中一个不是字符串，则先将该操作数转换成字符串，再执行连接操作。String message = \"Welcome \" + \"to \" + \"java\"; // Welcome to JavaString s = “Chapter” + 2; // Chapter2：不能都是数值String s1 += \"Supplement\" + 'B'; // SupplementB message += \" and Java is fun\"; // Welcome to Java and Java is funint i = 1;int j = 2;System.out.println(\"i + j = \" + i + j); // i+j=12System.out.println(\"i + j = \" + (i + j)); // i+j = 3","categories":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://shyshy903.github.io/tags/java/"}]},{"title":"Linux","slug":"Linux/linux","date":"2019-10-31T16:00:00.000Z","updated":"2020-02-11T08:48:55.540Z","comments":true,"path":"2019/11/01/Linux/linux/","link":"","permalink":"https://shyshy903.github.io/2019/11/01/Linux/linux/","excerpt":"","text":"1. linux 的基础系统命令 在linux中系统命令通常是如下的格式：命令名称 【命令参数】 【命令对象】 获取登录信息123# w# who # who am i 查看自己使用的shellps1# ps 查看命令的说明whatis12# what is ps# what is python 查看命令的位置which、whereis1234# where is ps# where is python# which ps# which python 查看帮助文档man、help、apropos12# man ps# info ps 切换用户su1# su hellokitty 以管理员身份执行命令sudo12# ls /root(fail)# sudo ls /root(success) 登入和登出相关logout、exit、adduser、userdel、passwd、ssh1234# adduser hellokitty# passwd hellokitty (change password)# ssh@hellokitty@1.2.3.4# logout 查看系统和主机名unname、hostname12# unname# hostname 重启和关机rebot、init 6、shutdown、init 01# rebot 查看历史命令history1# history 2. linux常用的实用命令 创建和删除目录mkdir、rmdir123# mkdir abc# mkdir -p abc# rmdir abc 创建和删除文件touch、rm123456789101112131415161718192021222324# touch readme.md# rm readme.md# rm -rf xyz ```bash&gt; `touch`命令用于创建空白文件或者修改文件时间。在Linux系统中有三种文件时间：- 更改文件内容的时间：`mtime`- 更改权限的时间: `ctime`- 最后访问时间: `atime`&gt; rm有几个重要的参数如下：- `-i` :交互式删除，每个删除项都要询问- `-r` :删除目录并且递归删除目录及文件- `-f` :强制删除，忽略存在的文件，没有任何提示3. 切换和查看当前的工作目录`cd`、`pwd`4. 查看目录内容`ls`* `-l` :以长格式查看文件和目录* `-a` ：显示以点开头的文件和目录* `-R` ：遇到目录，递归展开* `-d` : 只列出目录，不列出内容* `-s -t` : 按照大小和时间进行排序5. 查看文件内容`cat`、`head`、`tail`、`more`、`less````bash# cat readme.md# head -10 sohu.html 拷贝和移动文件cp、mv1234# cp sohu.html backup/# cd backup# mv sohu.html sohu_index.html 查找文件和查找内容find、grep grep 在搜索字符串时可以直接使用正则表达式，如果需要使用正则表达式，则可以用grep -E 链接ln 链接可以分为硬链接和软链接，硬链接可以认为是一个指向文件数据的指针。我们平常删除数据时，并没有删除硬盘上的文件，我们删除的是一个指针。而软链接类似于windows里面的快捷方式。 压缩\\解压缩\\归档\\解归档gzip、gunzip、xz、tar12# gunzip redis-4.0.10.tar.gz# tar -xvf redis-4.0.10.tar 其它工具sort、uniq、diff、tr、cut、paste、file、wc 管道和重定向 管道的使用：| 123# find ./ | wc -1 % 查找当前目录下的文件个数 # ls | cat - n % 列出当前路径下的文件加，给每一项加一个编号# cat record.log | grep AAA | grep - v BBB | wc - 1 % 查找record.log 中的AAA，但不包含BBB的个数 输出重定向和错误重定向：- &gt; / &gt;&gt; / 2 &gt;1234567891011121314151617[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat readme.txtbananaapplegrapeapplegrapewatermelonpearpitaya[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat readme.txt | sort | uniq &gt; result.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat result.txtapplebananagrapepearpitayawatermelon 输入重定向：- &lt;1234567891011[root@iZwz97tbgo9lkabnat2lo8Z ~]# echo 'hello, world!' &gt; hello.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]# wall &lt; hello.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]#Broadcast message from root@iZwz97tbgo9lkabnat2lo8Z (Wed Jun 20 19:43:05 2018):hello, world![root@iZwz97tbgo9lkabnat2lo8Z ~]# echo 'I will show you some code.' &gt;&gt; hello.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]# wall &lt; hello.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]#Broadcast message from root@iZwz97tbgo9lkabnat2lo8Z (Wed Jun 20 19:43:55 2018):hello, world!I will show you some code. 别名 alias1234567[root@iZwz97tbgo9lkabnat2lo8Z ~]# alias ll='ls -l'[root@iZwz97tbgo9lkabnat2lo8Z ~]# alias frm='rm -rf'[root@iZwz97tbgo9lkabnat2lo8Z ~]# ll...drwxr-xr-x 2 root root 4096 Jun 20 12:52 abc...bash[root@iZwz97tbgo9lkabnat2lo8Z ~]# frm abc unlias123[root@iZwz97tbgo9lkabnat2lo8Z ~]# unalias frm[root@iZwz97tbgo9lkabnat2lo8Z ~]# frm sohu.html-bash: frm: command not found 其它程序 时间和日期 date / cal123456789101112131415161718[root@iZwz97tbgo9lkabnat2lo8Z ~]# dateWed Jun 20 12:53:19 CST 2018[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal June 2018Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 30[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal 5 2017 May 2017Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 17 18 19 2021 22 23 24 25 26 2728 29 30 31 录制操作脚本1script 给用户发送消息1mesg / write / wall / mail 3. 文件系统文件和路径 命名规则：文件名的最大长度与文件系统类型有关，一般情况下，文件名不应该超过255个字符，虽然绝大多数的字符都可以用于文件名，但是最好使用英文大小写字母、数字、下划线、点这样的符号。文件名中虽然可以使用空格，但应该尽可能避免使用空格，否则在输入文件名时需要用将文件名放在双引号中或者通过\\对空格进行转义。 扩展名：在Linux系统下文件的扩展名是可选的，但是使用扩展名有助于对文件内容的理解。有些应用程序要通过扩展名来识别文件，但是更多的应用程序并不依赖文件的扩展名，就像file命令在识别文件时并不是依据扩展名来判定文件的类型 隐藏文件：以点开头的文件在Linux系统中是隐藏文件（不可见文件）。目录结构123456789101112131415161718191. */bin* - 基本命令的二进制文件。2. */boot* - 引导加载程序的静态文件。3. */dev* - 设备文件。4. */etc* - 配置文件。5. */home* - 普通用户主目录的父目录。6. */lib* - 共享库文件。7. */lib64* - 共享64位库文件。8. */lost+found* - 存放未链接文件。9. */media* - 自动识别设备的挂载目录。10. */mnt* - 临时挂载文件系统的挂载点。11. */opt* - 可选插件软件包安装位置。12. */proc* - 内核和进程信息。13. */root* - 超级管理员用户主目录。14. */run* - 存放系统运行时需要的东西。15. */sbin* - 超级用户的二进制文件。16. */sys* - 设备的伪文件系统。17. */tmp* - 临时文件夹。18. */usr* - 用户应用目录。19. */var* - 变量数据目录。 访问权限 chmod 改变文件模式比特12345678910111213141516[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html...[root@iZwz97tbgo9lkabnat2lo8Z ~]# chmod g+w,o+w sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l...-rw-rw-rw- 1 root root 211878 Jun 19 16:06 sohu.html...[root@iZwz97tbgo9lkabnat2lo8Z ~]# chmod 644 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html说明：通过上面的例子可以看出，用chmod改变文件模式比特有两种方式：一种是字符设定法，另一种是数字设定法。 除了chmod之外，可以通过umask来设定哪些权限将在新文件的默认权限中被删除。... chown - 改变文件所有者。12345678[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l...-rw-r--r-- 1 root root 54 Jun 20 10:06 readme.txt...[root@iZwz97tbgo9lkabnat2lo8Z ~]# chown hellokitty readme.txt[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l...bash-rw-r--r-- 1 hellokitty root 54 Jun 20 10:06 readme.txt 磁盘管理 列出文件系统的磁盘使用状况: df12345678[root@iZwz97tbgo9lkabnat2lo8Z ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 5.0G 33G 14% /devtmpfs 486M 0 486M 0% /devtmpfs 497M 0 497M 0% /dev/shmtmpfs 497M 356K 496M 1% /runtmpfs 497M 0 497M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/0 磁盘分区操作: fdisk12345678910111213[root@iZwz97tbgo9lkabnat2lo8Z ~]# fdisk -lDisk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a42f4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 83884031 41940992 83 LinuxDisk /dev/vdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 格式化文件系统 mkfs 文件系统检查 fsck 挂载/卸载： mount / umount 4 编辑器 vim 启动vim。可以通过vi或vim命令来启动vim，启动时可以指定文件名来打开一个文件，如果没有指定文件名，也可以在保存的时候指定文件名。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# vim guess.py 命令模式、编辑模式和末行模式：启动vim进入的是命令模式（也称为Normal模式），在命令模式下输入英文字母i会进入编辑模式（Insert模式），屏幕下方出现– INSERT –提示；在编辑模式下按下Esc会回到命令模式，此时如果输入英文:会进入末行模式，在末行模式下输入q!可以在不保存当前工作的情况下强行退出vim；在命令模式下输入v会进入可视模式（Visual模式），可以用光标选择一个区域再完成对应的操作。 保存和退出vim：在命令模式下输入:进入末行模式，输入wq可以实现保存退出；如果想放弃编辑的内容输入q!强行退出，这一点刚才已经提到过了；在命令模式下也可以直接输入ZZ实现保存退出。如果只想保存文件不退出，那么可以在末行模式下输入w；可以在w后面输入空格再指定要保存的文件名。 光标操作： 在命令模式下可以通过h、j、k、l来控制光标向左、下、上、右的方向移动，可以在字母前输入数字来表示移动的距离，例如：10h表示向左移动10个字符。 在命令模式下可以通过Ctrl+y和Ctrl+e来实现向上、向下滚动一行文本的操作，可以通过Ctrl+f和Ctrl+b来实现向前和向后翻页的操作。 在命令模式下可以通过输入英文字母G将光标移到文件的末尾，可以通过gg将光标移到文件的开始，也可以通过在G前输入数字来将光标移动到指定的行。 文本操作 删除：在命令模式下可以用dd来删除整行；可以在dd前加数字来指定删除的行数；可以用d$来实现删除从光标处删到行尾的操作，也可以通过d0来实现从光标处删到行首的操作；如果想删除一个单词，可以使用dw；如果要删除全文，可以在输入:%d（其中:用来从命令模式进入末行模式）。 复制和粘贴：在命令模式下可以用yy来复制整行；可以在yy前加数字来指定复制的行数；可以通过p将复制的内容粘贴到光标所在的地方。 撤销和恢复：在命令模式下输入u可以撤销之前的操作；通过Ctrl+r可以恢复被撤销的操作。 对内容进行排序：在命令模式下输入%!sort。 查找和替换 查找操作需要输入/进入末行模式并提供正则表达式来匹配与之对应的内容，例如：/doc.*\\.，输入n来向前搜索，也可以输入N来向后搜索。 替换操作需要输入:进入末行模式并指定搜索的范围、正则表达式以及替换后的内容和匹配选项，例如：:1,$s/doc.*/hello/gice，其中： g - global：全局匹配。 i - ignore case：忽略大小写匹配。 c - confirm：替换时需要确认。 e - error：忽略错误。 参数设定在输入:进入末行模式后可以对vim进行设定。 设置Tab键的空格数：set ts=4 置显示/不显示行号：set nu / set nonu 设置启用/关闭高亮语法：syntax on / syntax off 设置显示标尺（光标所在的行和列）： set ruler 设置启用/关闭搜索结果高亮：set hls / set nohls 说明：如果希望上面的这些设定在每次启动vim时都能生效，需要将这些设定写到用户主目录下的.vimrc文件中。 高级技巧 比较多个文件1[root@iZwz97tbgo9lkabnat2lo8Z ~]# vim -d foo.txt bar.txt 打开多个文件1[root@iZwz97tbgo9lkabnat2lo8Z ~]# vim foo.txt bar.txt hello.txt 启动vim后只有一个窗口显示的是foo.txt，可以在末行模式中输入ls查看到打开的三个文件，也可以在末行模式中输入b &lt;num&gt;来显示另一个文件，例如可以用:b 2将bar.txt显示出来，可以用:b 3将hello.txt显示出来。 拆分和切换窗口：可以在末行模式中输入sp或vs来实现对窗口的水平或垂直拆分，这样我们就可以同时打开多个编辑窗口，通过按两次Ctrl+w就可以实现编辑窗口的切换，在一个窗口中执行退出操作只会关闭对应的窗口，其他的窗口继续保留。 映射快捷键：在vim下可以将一些常用操作映射为快捷键来提升工作效率。 例子1：在命令模式下输入F4执行从第一行开始删除10000行代码的操作。 1:map &lt;F4&gt; gg10000dd。 例子2：在编辑模式下输入__main直接补全为: 12if __name__ == '__main__'::inoremap __main if __name__ == '__main__': 说明：上面例子2的inoremap中的i表示映射的键在编辑模式使用，nore表示不要递归，这一点非常重要，否则如果键对应的内容中又出现键本身，就会引发递归（相当于进入了死循环）。如果希望映射的快捷键每次启动vim时都能生效，需要将映射写到用户主目录下的.vimrc文件中。 录制宏： 在命令模式下输入qa开始录制宏（其中a是寄存器的名字，也可以是其他英文字母或0-9的数字）。 执行你的操作（光标操作、编辑操作等），这些操作都会被录制下来。 如果录制的操作已经完成了，按q结束录制。 通过@a（a是刚才使用的寄存器的名字）播放宏，如果要多次执行宏可以在前面加数字，例如100@a表示将宏播放100次。 可以试一试下面的例子来体验录制宏的操作，该例子来源于Harttle Land网站，该网站上提供了很多关于vim的使用技巧，有兴趣的可以去了解一下。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://shyshy903.github.io/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://shyshy903.github.io/tags/linux/"}]},{"title":"理论计算基础","slug":"量子计算/理论催化计算(一)","date":"2019-10-19T16:00:00.000Z","updated":"2019-10-19T16:00:00.000Z","comments":true,"path":"2019/10/20/量子计算/理论催化计算(一)/","link":"","permalink":"https://shyshy903.github.io/2019/10/20/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/%E7%90%86%E8%AE%BA%E5%82%AC%E5%8C%96%E8%AE%A1%E7%AE%97(%E4%B8%80)/","excerpt":"","text":"（一）准备工作1 系统与软件部分 Linux与windows系统 编辑器：windows用notepad++编辑器，linux用vim编辑器 相关程序： Materials studio (用来建模) Vesta(用来进行可视化与文件转换) VASP与CP2k:用来做第一性原理计算的软件,CP2k是从头算分析动力学模拟，是表面催化计算的大杀器，资料少，学习困难 p4vasp:vasp计算结果的后处理程序 VMD:分子动力学的可视化程序，作图的玩着，自由度很高，使用复杂 2 理论知识部分2.1 催化化学与量子化学这里推荐两本书， 2.2 密度泛函理论这里也推荐本书： （二）催化模型构建3 晶体结构数据库的使用3.1 相关说明第一篇单原子催化文章：Nat. Chem., 2011, 3,634-641研究晶体结构的文献都会给出结构的详细参数：比如，J. Am. Chem. Soc. 136, 20, 7221-7224注意：不要用MS里build–crystals–build crystals对着文献输入参数。费了半天劲还容易搞错，在晶体数据库里可以直接找到cif结构文件。 3.2 晶体结构与数据库 问题：找晶体结构到底在找什么？答：CIF文件，后缀名为.cif，内部含有结构信息 常用的晶体结构数据库： ICSD – the Inorganic Crystal Structure Database 无机晶体数据库。http://www2.fiz-karlsruhe.de/icsd_home.html CCDC – The Cambridge Crystallographic Data Centrehttps://www.ccdc.cam.ac.uk/ Materials studio自带晶体数据库 Materials Project（强烈推荐）：https://materialsproject.org/特色，不但有实验结构参数，还有理论计算数据，比如磁矩，形成能，密度，带隙，空间群，点群，晶系，能带结构，弹性张量， 压电张量等数据。截至2018年9月11日，收录83989种无机化合物， 52179个能带结构 AMCSD – American Mineralogist Crystal Structure Database：http://rruff.geo.arizona.edu/AMS/amcsd.php google search （ex： Al2O3 filetype:cif ）3.3 CCDC实战训练（一）-从文章中找到晶体结构 如何从文章找到晶体结构？答： 直接到文章末尾去找CCDC编码，但有时晶体结构也会出现在文章中或者SI里面。 如何从晶体数据库中获得结构文件？ 登录数据库查找CCDC编码 下载CIF文件 3.4 ISDC 实战训练（二）-得到各种AL2O3模型Materials studio 只有一种Al2O3 模型，是a型的trigonal晶系，也称作corundum刚玉。如果想要得到不同晶型的Al2O3就要去晶体数据库上找。 总结：科研中碰到一个晶体，应该怎么找对应的结构文件。 依次尝试下列方法： 在materials project中直接输入对应元素和原子个数。如果搞不清该化合物的晶型，提前Google该晶体所属晶系，点群和空间群。 在ICSD chemistry中输入对应元素和原子个数。（ICSD是最全的无机晶体数据库，如果这都找不到结构，应该回头看那里搞错了） 在文献中找结构，然后去ICSD搜索该文献。 直接google，例如， black phosphorus CIF想偷懒可以在MS里， File-input-structures里找结构， MS里只有非常少数的常见结构。 最后非常感谢研之成理和清华化学系刘锦程博士，微信搜索研之成理就可以get一个非常非常优质的公众号了！！","categories":[{"name":"量子计算","slug":"量子计算","permalink":"https://shyshy903.github.io/categories/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"量子计算","slug":"量子计算","permalink":"https://shyshy903.github.io/tags/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/"}]},{"title":"Hexo + GithubPages 搭建个人博客与网站！","slug":"配置/Hexo-GithubPages 搭建个人网站","date":"2019-10-15T01:00:00.000Z","updated":"2020-02-11T08:48:55.623Z","comments":true,"path":"2019/10/15/配置/Hexo-GithubPages 搭建个人网站/","link":"","permalink":"https://shyshy903.github.io/2019/10/15/%E9%85%8D%E7%BD%AE/Hexo-GithubPages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/","excerpt":"","text":"Hexo + GithubPages 搭建个人博客与网站！博客搭建准备环境 node.js 下载，并安装。 Git 下载，并安装。 安装Hexo，在命令行（即Git Bash）运行以下命令: 1$ npm install -g hexo-cli 初始化hexo,在命令行依次运行以下命令: 123$ hexo init folder$ cd folder$ npm install hexo server 新建完成后，会在路径下产生下列文件和文件夹： 1234567├── _config.yml % 站点配置文件├── package.json├── scaffolds├── source| ├── _drafts| └── _posts % 文章发布文件夹 themes % 主题配置文件夹，可folk别人的模板 启动服务，在命令行输入: 1$ hexo server 浏览器访问网址,至此，hexo博客已经搭建到本地了 Github实时搭建 在github官网创建账号 创建仓库：&lt;账号名称&gt;github.io 将本地博客推送到githubPages。 安装hexo-deplyer-git插件。bash命令行运行。 1$ npm install hexo-deployer-git --save 添加github的SSH-KEY,创建一个SSH-KEY,在命令行输入: 1$ ssh-keygen -t rsa -C \"邮箱地址\" C:\\Users\\Administrator\\.ssh\\id_rsa.pub 修改_config.yml,文件末尾修改为:1234deploy: type: git % 注意冒号后面有一个空格 repo: git@github.com:shyshy903/shyshy903.github.io branch: master 推送到githubPages中。12$ hexo g$ hexo d 之后在浏览器输入：http://shyshy903.github.io 就可以访问个人博客了 添加域名，创建个人网站 到万网或者阿里云买一个域名，进行DNS解析 DNS解析：类型选择为 CNAME;机记录即域名前缀，填写为www;录值填写为&lt;Github账号名称&gt;.github.io;解析线路，TTL 默认即可 仓库设置 打开博客仓库设置：https://github.com/&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io/settings 在Custom domain下，填写自定义域名，点击save； 在站点目录的source文件夹下，创建并打开CNAME.txt，写入你的域名（如www.simon96.online），保存，并重命名为CNAME 完成以上步骤，就可以通过域名www.shyshy903.top来访问个人网站和博客了。 安装必要的插件1npm i -S hexo-generator-search hexo-generator-json-content hexo-renderer-less 站点配置-hexo根目录下config.yml文件 多语言支持12345language:- zh-CN- en- zh-HK- zh-TW 搜索框的配置1npm install hexo-generator-searchdb --save 站点文件添加下面的代码块：12345search: path: search.xml field: post format: html limit: 10000 主题优化之自定样式 cdn的使用cdn + github的博客可以参考别人的文章1234567891011############################### 基本信息 ###############################info: name: Material X docs: https://xaoxuu.com/wiki/material-x/ cdn: # 把对应的那一行注释掉就使用本地的文件 css: # style: https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9.9/css/style.css js: app: https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9/js/app.js search: https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.9/js/search.js volantis: https://cdn.jsdelivr.net/gh/xaoxuu/volantis@1.0.5/js/volantis.min.js 主题优化之评论系统 valine的使用 站点配置123leancloud: app_id: 你的appId # 从leancloud官网获得：https://www.avoscloud.com/dashboard/ app_key: 你的appKey 主题配置123456789101112131415valine: enable: true # 如果你想用Valine评论系统，请设置enable为true volantis: true # 是否启用volantis版本（禁止匿名，增加若干贴吧、QQ表情） # 还需要在根目录配置文件中添加下面这三行内容 # leancloud: # app_id: 你的appId # app_key: 你的appKey guest_info: nick,mail,link #valine comment header info placeholder: 快来评论吧~ # valine comment input placeholder(like: Please leave your footprints ) avatar: mp # gravatar style https://valine.js.org/avatar pageSize: 20 # comment list page size verify: false # valine verify code (true/false) notify: false # valine mail notify (true/false) lang: zh-cn highlight: false 主题优化之加入萌萌哒表情 这里需要安装一个插件哦: 1$ npm install hexo-helper-live2d --save 复制你喜欢的名字，如z16。 在hexo文件夹中建立一个文件夹live2d_models。 1 在live2d_models中建立文件夹z16。 2 在文件夹啊z16中创建json文件：z16.model.json。 将以下代码添加到主题配置文件中去： 12345678910111213141516live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false log: false model: use: live2d-widget-model-z16 display: position: right width: 150 height: 300 mobile: show: true 安装模型: 1$ npm install liv2d_models-widget-z16 --save 在命令行中运行命令，既可以在&quot;http://localhost:4000&quot;中预览自己的博客网页 1$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 如果预览的博客符合自己的要求，就可以进行更新,大公告成啦： 1$ hexo d -g 如果需要调整插入模型的透明度，可以在第4步中的代码中插入： 12345678910display: position: right width: 300 height: 600 opacity: 0.4 mobile: show: true react: opacity: 0.4 opacityOnHover: 0.7","categories":[{"name":"hexo","slug":"hexo","permalink":"https://shyshy903.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://shyshy903.github.io/tags/hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-10-14T16:00:00.000Z","updated":"2019-10-14T16:00:00.000Z","comments":true,"path":"2019/10/15/hello-world/","link":"","permalink":"https://shyshy903.github.io/2019/10/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}